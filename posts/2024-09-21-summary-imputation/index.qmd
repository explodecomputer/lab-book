---
title: "Summary stats and imputation"
author: Gibran Hemani
date: "2024-09-21"
categories: []
---

## Background

Summary imputation is very slow. This very fast approximation is based on simulating summary statistics for a region given knowledge of the causal variants and an LD matrix.

```{r}
library(dplyr)
library(data.table)
library(MASS)
library(ggplot2)

# Alternatives to matrix inversion (ignore just use MASS::ginv)
solve2 <- function(A) {
    eig <- eigen(A)
    eig$values <- 1 / eig$values
    return(eig$vectors %*% diag(eig$values) %*% t(eig$vectors))
}

solve3 <- function(A, lambda=1e-6) {
    solve(A + diag(nrow(A)) * lambda)
}
```

Get some data for testing etc

```{r, eval=FALSE}
system("plink2 --bfile ~/repo/opengwas-api-internal/opengwas-api/app/ld_files/EUR --chr 1 --from-mb 10 --to-mb 10.4 --recode A --out temp")
G <- fread("temp.raw", header = TRUE)
G <- G[,-c(1:6)] %>% as.matrix
X <- G
nsnp <- ncol(X)
for(i in 1:nsnp) {
    X[,i] <- X[,i] - mean(X[,i])
}
af <- colMeans(G) / 2
save(X, af, file="1kg_region.rdata")
```

Load the data

```{r}
load(url("https://github.com/explodecomputer/lab-book/raw/refs/heads/main/posts/2024-09-18-conditional-summary-stats/1kg_region.rdata"))
```


```{r}
#' Basic imputation function
#' 
#' @param R The correlation matrix - must be complete for the set of SNPs that need to be imputed
#' @param ss A data frame with columns betahat2 = vector of effect estimates in the same order as R and with NAs for variants that need to be imputed; se = as with betahat2 but for available standard errors, af = allele frequencies (no missing values allowed, so use reference panel if there are missing values)
#' @param index The positions of the SNPs that are causal and will be used to generate the simulated summary statistics. This can just be the top hit.
#' 
#' @return A list with the following elements:
#' - ss: The input data frame with the imputed values added
#' - b_adj: The adjustment factor for the effect sizes
#' - se_adj: The adjustment factor for the standard errors
#' - b_cor: The correlation between the true and imputed effect sizes - this is critical for evaluation of the performance of the imputation, it should be close to 1 e.g > 0.7 would be a reasonable threshold
#' - se_cor: The correlation between the true and imputed standard errors
imp <- function(R, ss, index) {
    b <- ss$betahat2
    se <- ss$se2
    af <- ss$af
    nsnp <- length(b)
    stopifnot(ncol(R) == nsnp)
    stopifnot(nrow(R) == nsnp)
    stopifnot(length(af) == nsnp)
    stopifnot(length(se) == nsnp)
    stopifnot(all(index) %in% 1:nsnp)
    stopifnot(length(index) < nsnp)
    stopifnot(all(af > 0 & af < 1))
    stopifnot(all(!is.na(af)))
    stopifnot(all(se > 0, na.rm=TRUE))
    if(all(!is.na(b))) {
        message("No missing values in b, imputation not required")
        b_cor=1
        se_cor=1
        mod1=1
        mod2=1
    } else {
        # Calculate the diagonal matrix of variances and the inverse
        D <- diag(sqrt(2 * af * (1 - af)))
        Di <- diag(1 / diag(D))

        # Get the conditional estimates of the index SNP effects
        if(length(index) == 1) {
            bhat2 <- b[index]
        } else {
            bhat2 <- D[index,index] %*% MASS::ginv(R[index,index]) %*% Di[index,index] %*% b[index]
        }
        b2 <- rep(0, nsnp)
        b2[index] <- bhat2

        # Get the simulated effect sizes
        betahat_sim <- as.numeric(Di %*% R %*% D %*% b2)

        # Initialise the SE - this doesn't account for var(y) or sample size, but those are constants that can be obtained from regression re-scaling
        sehat <- sqrt(diag(Di))

        # Re-scale effect sizes and standard errors
        # vb <- var(b, na.rm=TRUE)
        # vse <- var(se, na.rm=TRUE)
        # mod1 <- cov(b, betahat_sim, use="pair") / vb
        mod1 <- lm(betahat_sim ~ b)$coef[2]
        # mod2 <- cov(se, sehat, use="pair") / vse
        mod2 <- lm(sehat ~ se)$coef[2]

        # Performance metrics
        # b_cor = mod1 * sqrt(vb) / sd(betahat_sim, na.rm=TRUE)
        b_cor <- cor(b, betahat_sim, use="pair")
        # se_cor = mod2 * sqrt(vse) / sd(sehat, na.rm=TRUE)
        se_cor <- cor(se, sehat, use="pair")

        # Re-scale
        betahat_sim <- betahat_sim / mod1
        sehat <- sehat / mod2

        # Fill in missing values
        b[is.na(b)] <- betahat_sim[is.na(b)]
        se[is.na(se)] <- sehat[is.na(se)]

        stopifnot(all(!is.na(b)))
        stopifnot(all(!is.na(se)))
    }

    ss$betahatimp <- b
    ss$seimp <- se
    ss$zimp <- b / se
    ss$pimp <- 2 * pnorm(-abs(ss$zimp))

    # Output
    out <- list(
        ss = ss,
        b_adj = mod1,
        se_adj = mod2,
        b_cor = b_cor,
        se_cor = se_cor,
        n_ind = length(index)
    )
    return(out)
}
```

Run some simulations to test the performance across different scenarios

```{r}
simulate_ss <- function(X, af, ncause, sigmag, seed=1234) {
    set.seed(seed)
    nsnp <- length(af)
    nid <- nrow(X)
    b <- rep(0, nsnp)
    b[sample(1:nsnp, ncause)] <- rnorm(ncause, sd=sigmag)

    e <- rnorm(nid)
    y <- X %*% b + e 

    betahat <- sapply(1:nsnp, \(i) {cov(X[,i], y) / var(X[,i])})
    se <- sapply(1:nsnp, \(i) {sqrt(var(y) / (var(X[,i] * sqrt(nid))))})
    zhat <- betahat/se
    pval <- 2 * pnorm(-abs(zhat))

    return(tibble(betahat, b, se, zhat, pval, af))
}

generate_missing <- function(ss, frac) {
    ss <- ss %>% mutate(
        betahat2 = ifelse(runif(n()) < frac, NA, betahat),
        se2 = ifelse(is.na(betahat2), NA, se),
        zhat2 = ifelse(is.na(betahat2), NA, zhat))
    return(ss)
}

clump <- function(z, R, zthresh = qnorm(1e-5, low=F), rthresh = 0.2) {
    z <- abs(z)
    z[z < zthresh] <- NA
    k <- c()
    while(!all(is.na(z))) {
        i <- which.max(z)
        k <- c(k, i)
        z[i] <- NA
        z[which(R[i,]^2 > rthresh)] <- NA
    }
    return(k)
}
```

One simulation example where there are 3 causal variants and they are known and 10% of the data is masked for imputation

```{r}
R <- cor(X)
ss <- simulate_ss(X, af, 3, 20)
ss <- generate_missing(ss, 0.1)
ss1 <- imp(R, ss, which(ss$b != 0))
ss1
```

Show the performance of the imputation at the missing values

```{r}
ggplot(ss1$ss, aes(x=betahatimp, y=betahat)) + geom_point(aes(colour=is.na(betahat2)))
```


Now we can run a simulation to test the performance of the imputation across different scenarios
- 

```{r, eval=FALSE}
sim_all <- function(X, R, frac_missing, ncause, sigmag, zthresh, rthresh, seed=1234) {
    ss <- simulate_ss(X, af, ncause, sigmag, seed)
    ss <- generate_missing(ss, frac_missing)
    if(zthresh == -1) {
        index <- which(ss$b != 0)
    } else if(zthresh == -2) {
        index <- which.max(abs(ss$zhat))
    } else {
        index <- clump(ss$zhat2, R, qnorm(zthresh, low=FALSE), rthresh)
    }
    ss <- imp(R, ss, index)
    return(ss)
}

params <- expand.grid(
    frac_missing = c(0.1, 0.3),
    ncause = c(1, 2, 3),
    sigmag = c(10, 20),
    zthresh = c(-1, -2, 1e-5, 1e-8),
    rthresh = c(0.01),
    sim = 1:20
)
dim(params)

res <- lapply(1:nrow(params), \(i) {
    message(i)
    p <- params[i,]
    r <- tryCatch(sim_all(X, R, p$frac_missing, p$ncause, p$sigmag, p$zthresh, p$rthresh, seed=i), error=function(e) {return(NULL)})
    tibble(
        frac_missing = p$frac_missing,
        ncause = p$ncause,
        sigmag = p$sigmag,
        zthresh = p$zthresh,
        rthresh = p$rthresh,
        sim = p$sim,
        b_cor = r$b_cor,
        se_cor = r$se_cor,
        b_adj = r$b_adj,
        se_adj = r$se_adj
    )
}) %>% bind_rows()
save(res, file="simres.rdata")
```

Look at simulation results

```{r}
load("simres.rdata")
res %>% mutate(
    zthresh = case_when(zthresh == -1 ~ "Known causal variants",
                        zthresh == -2 ~ "Top hit",
                        TRUE ~ paste("Clump at", zthresh))
) %>%
    ggplot(aes(x=as.factor(frac_missing), y=b_cor)) + 
        geom_boxplot(aes(fill=as.factor(zthresh))) + 
        facet_grid(ncause ~ sigmag, labeller=label_both) + 
        labs(y="Correlation between known and imputed effect sizes", x="Fraction of missing values", fill="Index variant method")
```


## Summary

- Using a single tophit to generate the sumstats seems to be fine even when there are multiple causal variants
- Clumping with strict rsq threshold and relaxed p-value threshold to obtain index SNPs seems to be most effective
- The performance doesn't change drastically based on fraction of missing SNPs
- Previous iterations showed that if doing clumping, using rsq thresh 0.2 led to major problems, so having index SNPs be in relative linkage equilibrium seems important

---

```{r}
sessionInfo()
```
