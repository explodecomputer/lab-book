{
  "hash": "2d487bde3beddc74c73c98b083830635",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Summary stats and imputation\"\nauthor: Gibran Hemani\ndate: \"2024-09-21\"\ncategories: []\n---\n\n\n## Background\n\nSummary imputation is very slow. This very fast approximation is based on simulating summary statistics for a region given knowledge of the causal variants and an LD matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(data.table)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'data.table'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(MASS)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'MASS'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Alternatives to matrix inversion (ignore just use MASS::ginv)\nsolve2 <- function(A) {\n    eig <- eigen(A)\n    eig$values <- 1 / eig$values\n    return(eig$vectors %*% diag(eig$values) %*% t(eig$vectors))\n}\n\nsolve3 <- function(A, lambda=1e-6) {\n    solve(A + diag(nrow(A)) * lambda)\n}\n```\n:::\n\n\nGet some data for testing etc\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem(\"plink2 --bfile ~/repo/opengwas-api-internal/opengwas-api/app/ld_files/EUR --chr 1 --from-mb 10 --to-mb 10.4 --recode A --out temp\")\nG <- fread(\"temp.raw\", header = TRUE)\nG <- G[,-c(1:6)] %>% as.matrix\nX <- G\nnsnp <- ncol(X)\nfor(i in 1:nsnp) {\n    X[,i] <- X[,i] - mean(X[,i])\n}\naf <- colMeans(G) / 2\nsave(X, af, file=\"1kg_region.rdata\")\n```\n:::\n\n\nLoad the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(url(\"https://github.com/explodecomputer/lab-book/raw/refs/heads/main/posts/2024-09-18-conditional-summary-stats/1kg_region.rdata\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Basic imputation function\n#' \n#' @param R The correlation matrix - must be complete for the set of SNPs that need to be imputed\n#' @param ss A data frame with columns betahat2 = vector of effect estimates in the same order as R and with NAs for variants that need to be imputed; se = as with betahat2 but for available standard errors, af = allele frequencies (no missing values allowed, so use reference panel if there are missing values)\n#' @param index The positions of the SNPs that are causal and will be used to generate the simulated summary statistics. This can just be the top hit.\n#' \n#' @return A list with the following elements:\n#' - ss: The input data frame with the imputed values added\n#' - b_adj: The adjustment factor for the effect sizes\n#' - se_adj: The adjustment factor for the standard errors\n#' - b_cor: The correlation between the true and imputed effect sizes - this is critical for evaluation of the performance of the imputation, it should be close to 1 e.g > 0.7 would be a reasonable threshold\n#' - se_cor: The correlation between the true and imputed standard errors\nimp <- function(R, ss, index) {\n    b <- ss$betahat2\n    se <- ss$se2\n    af <- ss$af\n    nsnp <- length(b)\n    stopifnot(ncol(R) == nsnp)\n    stopifnot(nrow(R) == nsnp)\n    stopifnot(length(af) == nsnp)\n    stopifnot(length(se) == nsnp)\n    stopifnot(all(index) %in% 1:nsnp)\n    stopifnot(length(index) < nsnp)\n    stopifnot(all(af > 0 & af < 1))\n    stopifnot(all(!is.na(af)))\n    stopifnot(all(se > 0, na.rm=TRUE))\n    if(all(!is.na(b))) {\n        message(\"No missing values in b, imputation not required\")\n        b_cor=1\n        se_cor=1\n        mod1=1\n        mod2=1\n    } else {\n        # Calculate the diagonal matrix of variances and the inverse\n        D <- diag(sqrt(2 * af * (1 - af)))\n        Di <- diag(1 / diag(D))\n\n        # Get the conditional estimates of the index SNP effects\n        if(length(index) == 1) {\n            bhat2 <- b[index]\n        } else {\n            bhat2 <- D[index,index] %*% MASS::ginv(R[index,index]) %*% Di[index,index] %*% b[index]\n        }\n        b2 <- rep(0, nsnp)\n        b2[index] <- bhat2\n\n        # Get the simulated effect sizes\n        betahat_sim <- as.numeric(Di %*% R %*% D %*% b2)\n\n        # Initialise the SE - this doesn't account for var(y) or sample size, but those are constants that can be obtained from regression re-scaling\n        sehat <- sqrt(diag(Di))\n\n        # Re-scale effect sizes and standard errors\n        # vb <- var(b, na.rm=TRUE)\n        # vse <- var(se, na.rm=TRUE)\n        # mod1 <- cov(b, betahat_sim, use=\"pair\") / vb\n        mod1 <- lm(betahat_sim ~ b)$coef[2]\n        # mod2 <- cov(se, sehat, use=\"pair\") / vse\n        mod2 <- lm(sehat ~ se)$coef[2]\n\n        # Performance metrics\n        # b_cor = mod1 * sqrt(vb) / sd(betahat_sim, na.rm=TRUE)\n        b_cor <- cor(b, betahat_sim, use=\"pair\")\n        # se_cor = mod2 * sqrt(vse) / sd(sehat, na.rm=TRUE)\n        se_cor <- cor(se, sehat, use=\"pair\")\n\n        # Re-scale\n        betahat_sim <- betahat_sim / mod1\n        sehat <- sehat / mod2\n\n        # Fill in missing values\n        b[is.na(b)] <- betahat_sim[is.na(b)]\n        se[is.na(se)] <- sehat[is.na(se)]\n\n        stopifnot(all(!is.na(b)))\n        stopifnot(all(!is.na(se)))\n    }\n\n    ss$betahatimp <- b\n    ss$seimp <- se\n    ss$zimp <- b / se\n    ss$pimp <- 2 * pnorm(-abs(ss$zimp))\n\n    # Output\n    out <- list(\n        ss = ss,\n        b_adj = mod1,\n        se_adj = mod2,\n        b_cor = b_cor,\n        se_cor = se_cor,\n        n_ind = length(index)\n    )\n    return(out)\n}\n```\n:::\n\n\nRun some simulations to test the performance across different scenarios\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_ss <- function(X, af, ncause, sigmag, seed=1234) {\n    set.seed(seed)\n    nsnp <- length(af)\n    nid <- nrow(X)\n    b <- rep(0, nsnp)\n    b[sample(1:nsnp, ncause)] <- rnorm(ncause, sd=sigmag)\n\n    e <- rnorm(nid)\n    y <- X %*% b + e \n\n    betahat <- sapply(1:nsnp, \\(i) {cov(X[,i], y) / var(X[,i])})\n    se <- sapply(1:nsnp, \\(i) {sqrt(var(y) / (var(X[,i] * sqrt(nid))))})\n    zhat <- betahat/se\n    pval <- 2 * pnorm(-abs(zhat))\n\n    return(tibble(betahat, b, se, zhat, pval, af))\n}\n\ngenerate_missing <- function(ss, frac) {\n    ss <- ss %>% mutate(\n        betahat2 = ifelse(runif(n()) < frac, NA, betahat),\n        se2 = ifelse(is.na(betahat2), NA, se),\n        zhat2 = ifelse(is.na(betahat2), NA, zhat))\n    return(ss)\n}\n\nclump <- function(z, R, zthresh = qnorm(1e-5, low=F), rthresh = 0.2) {\n    z <- abs(z)\n    z[z < zthresh] <- NA\n    k <- c()\n    while(!all(is.na(z))) {\n        i <- which.max(z)\n        k <- c(k, i)\n        z[i] <- NA\n        z[which(R[i,]^2 > rthresh)] <- NA\n    }\n    return(k)\n}\n```\n:::\n\n\nOne simulation example where there are 3 causal variants and they are known and 10% of the data is masked for imputation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR <- cor(X)\nss <- simulate_ss(X, af, 3, 20)\nss <- generate_missing(ss, 0.1)\nss1 <- imp(R, ss, which(ss$b != 0))\nss1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$ss\n# A tibble: 1,075 × 13\n   betahat     b    se   zhat     pval    af betahat2   se2  zhat2 betahatimp\n     <dbl> <dbl> <dbl>  <dbl>    <dbl> <dbl>    <dbl> <dbl>  <dbl>      <dbl>\n 1   3.17      0 0.841  3.77  1.62e- 4 0.155    3.17  0.841  3.77       3.17 \n 2   0.844     0 2.53   0.333 7.39e- 1 0.989    0.844 2.53   0.333      0.844\n 3  -0.185     0 1.30  -0.142 8.87e- 1 0.939   -0.185 1.30  -0.142     -0.185\n 4   4.21      0 2.84   1.48  1.38e- 1 0.988    4.21  2.84   1.48       4.21 \n 5  -8.30      0 2.40  -3.46  5.36e- 4 0.983   -8.30  2.40  -3.46      -8.30 \n 6   4.15      0 0.699  5.94  2.93e- 9 0.241    4.15  0.699  5.94       4.15 \n 7 -12.6       0 1.49  -8.46  2.72e-17 0.960  -12.6   1.49  -8.46     -12.6  \n 8  -1.71      0 1.99  -0.859 3.90e- 1 0.977   -1.71  1.99  -0.859     -1.71 \n 9   4.21      0 2.84   1.48  1.38e- 1 0.988    4.21  2.84   1.48       4.21 \n10   0.370     0 1.29   0.286 7.75e- 1 0.938    0.370 1.29   0.286      0.370\n# ℹ 1,065 more rows\n# ℹ 3 more variables: seimp <dbl>, zimp <dbl>, pimp <dbl>\n\n$b_adj\n        b \n0.9762786 \n\n$se_adj\n       se \n0.6521467 \n\n$b_cor\n[1] 0.9925273\n\n$se_cor\n[1] 0.9928014\n\n$n_ind\n[1] 3\n```\n\n\n:::\n:::\n\n\nShow the performance of the imputation at the missing values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ss1$ss, aes(x=betahatimp, y=betahat)) + geom_point(aes(colour=is.na(betahat2)))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nNow we can run a simulation to test the performance of the imputation across different scenarios\n- \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_all <- function(X, R, frac_missing, ncause, sigmag, zthresh, rthresh, seed=1234) {\n    ss <- simulate_ss(X, af, ncause, sigmag, seed)\n    ss <- generate_missing(ss, frac_missing)\n    if(zthresh == -1) {\n        index <- which(ss$b != 0)\n    } else if(zthresh == -2) {\n        index <- which.max(abs(ss$zhat))\n    } else {\n        index <- clump(ss$zhat2, R, qnorm(zthresh, low=FALSE), rthresh)\n    }\n    ss <- imp(R, ss, index)\n    return(ss)\n}\n\nparams <- expand.grid(\n    frac_missing = c(0.1, 0.3),\n    ncause = c(1, 2, 3),\n    sigmag = c(10, 20),\n    zthresh = c(-1, -2, 1e-5, 1e-8),\n    rthresh = c(0.01),\n    sim = 1:20\n)\ndim(params)\n\nres <- lapply(1:nrow(params), \\(i) {\n    message(i)\n    p <- params[i,]\n    r <- tryCatch(sim_all(X, R, p$frac_missing, p$ncause, p$sigmag, p$zthresh, p$rthresh, seed=i), error=function(e) {return(NULL)})\n    tibble(\n        frac_missing = p$frac_missing,\n        ncause = p$ncause,\n        sigmag = p$sigmag,\n        zthresh = p$zthresh,\n        rthresh = p$rthresh,\n        sim = p$sim,\n        b_cor = r$b_cor,\n        se_cor = r$se_cor,\n        b_adj = r$b_adj,\n        se_adj = r$se_adj\n    )\n}) %>% bind_rows()\nsave(res, file=\"simres.rdata\")\n```\n:::\n\n\nLook at simulation results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"simres.rdata\")\nres %>% mutate(\n    zthresh = case_when(zthresh == -1 ~ \"Known causal variants\",\n                        zthresh == -2 ~ \"Top hit\",\n                        TRUE ~ paste(\"Clump at\", zthresh))\n) %>%\n    ggplot(aes(x=as.factor(frac_missing), y=b_cor)) + \n        geom_boxplot(aes(fill=as.factor(zthresh))) + \n        facet_grid(ncause ~ sigmag, labeller=label_both) + \n        labs(y=\"Correlation between known and imputed effect sizes\", x=\"Fraction of missing values\", fill=\"Index variant method\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 136 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n## Summary\n\n- Using a single tophit to generate the sumstats seems to be fine even when there are multiple causal variants\n- Clumping with strict rsq threshold and relaxed p-value threshold to obtain index SNPs seems to be most effective\n- The performance doesn't change drastically based on fraction of missing SNPs\n- Previous iterations showed that if doing clumping, using rsq thresh 0.2 led to major problems, so having index SNPs be in relative linkage equilibrium seems important\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.5.1     MASS_7.3-60.2     data.table_1.15.4 dplyr_1.1.4      \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.5       cli_3.6.2         knitr_1.47        rlang_1.1.3      \n [5] xfun_0.44         generics_0.1.3    jsonlite_1.8.8    labeling_0.4.3   \n [9] glue_1.7.0        colorspace_2.1-0  htmltools_0.5.8.1 scales_1.3.0     \n[13] fansi_1.0.6       rmarkdown_2.27    grid_4.4.1        munsell_0.5.1    \n[17] evaluate_0.23     tibble_3.2.1      fastmap_1.2.0     yaml_2.3.8       \n[21] lifecycle_1.0.4   compiler_4.4.1    htmlwidgets_1.6.4 pkgconfig_2.0.3  \n[25] farver_2.1.2      digest_0.6.35     R6_2.5.1          tidyselect_1.2.1 \n[29] utf8_1.2.4        pillar_1.9.0      magrittr_2.0.3    withr_3.0.0      \n[33] gtable_0.3.5      tools_4.4.1      \n```\n\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}