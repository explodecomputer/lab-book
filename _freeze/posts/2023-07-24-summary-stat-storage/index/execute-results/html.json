{
  "hash": "5a3f13c4c7503a57fdd1dbae3fcbf4a1",
  "result": {
    "markdown": "---\ntitle: \"Summary stat storage\"\nauthor: Gibran Hemani\ndate: \"2023-07-24\"\ncategories: []\n---\n\n\n## Background\n\nSummary statistics typically have a lot of redundant information due to LD. Can we improve storage space by converting to a sparse format using an external LD reference panel?\n\n### Overall strategy\n\nTwo types of compression - lossless and lossy. Lossless preserves the inforation perfectly. Lossy allows some loss of information to achieve computational / storage improvement.\n\n- Lossless compression:\n    - 1mb regions with suggestive association e.g. p-value < 1e-5\n    - cis regions of molecular traits\n- Lossy compression\n    - all other regions\n\n### Strategies for lossy compression\n\n- Use external LD reference panel, chunk up genome into areas of distinct LD, decompose summary statistics into eigenvectors and retain only the first X eigenvectors that explain sufficient variation\n- Retain only the sign of the effect estimate\n- Remove entirely and assume beta = 0\n\n### Determining the effectiveness of compression\n\n- data storage saving\n- time cost\n- loss of precision\n- bias?\n- sensitivity to ld reference panel\n\nImpact on different types of analyses e.g.\n\n- colocalisation\n- mr\n- ld score regression\n- bias of effects\n\n\n## Example of lossy LD compression of betas\n\n- Download some LD reference data e.g. from here: http://fileserve.mrcieu.ac.uk/ld/1kg.v3.tgz\n- Download some GWAS summary statistics: https://gwas.mrcieu.ac.uk/files/ukb-b-19953/ukb-b-19953.vcf.gz\n- Have bcftools on path\n- Have plink on path\n\n```\n# Download example summary statistics (UKBB GWAS of BMI)\nwget https://gwas.mrcieu.ac.uk/files/ukb-b-19953/ukb-b-19953.vcf.gz\nwget https://gwas.mrcieu.ac.uk/files/ukb-b-19953/ukb-b-19953.vcf.gz.tbi\n\n# Convert vcf to txt file, just keep chr 22\nbcftools query \\\n-r 22 \\\n-e 'ID == \".\"' \\\n-f '%ID\\t[%LP]\\t%CHROM\\t%POS\\t%ALT\\t%REF\\t%AF\\t[%ES\\t%SE]\\n' \\\nukb-b-19953.vcf.gz | \\\nawk 'BEGIN {print \"variant_id\\tp_value\\tchromosome\\tbase_pair_location\\teffect_allele\\tother_allele\\teffect_allele_frequency\\tbeta\\tstandard_error\"}; {OFS=\"\\t\"; if ($2==0) $2=1; else if ($2==999) $2=0; else $2=10^-$2; print}' > gwas.tsv\n\n# Download and extract the LD reference panel - 1000 genomes\nwget http://fileserve.mrcieu.ac.uk/ld/1kg.v3.tgz\ntar xvf 1kg.v3.tgz\n\n# Get allele frequencies\nplink --bfile EUR --freq --out EUR --chr 22\n/Users/gh13047/Downloads/plink_mac_20230116/plink --bfile /Users/gh13047/repo/opengwas-api-internal/opengwas-api/app/ld_files/EUR --freq --out EUR --chr 22\n```\n\nRead in GWAS sum stats\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ieugwasr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nAPI: public: http://gwas-api.mrcieu.ac.uk/\n```\n:::\n\n```{.r .cell-code}\nlibrary(data.table)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyr)\nlibrary(glue)\ngwas <- fread(\"gwas.tsv\")\n```\n:::\n\n\nJust keep 1 Mb and get LD matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwas <- subset(gwas, base_pair_location < (min(base_pair_location)+1000000))\nld <- ld_matrix(gwas$variant_id, bfile=\"/Users/gh13047/repo/opengwas-api-internal/opengwas-api/app/ld_files/EUR\", plink_bin=\"/Users/gh13047/Downloads/plink_mac_20230116/plink\")\ndim(ld)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 275 275\n```\n:::\n:::\n\n\nHarmonise gwas and ld\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstandardise <- function(d, ea=\"ea\", oa=\"oa\", beta=\"beta\", chr=\"chr\", pos=\"pos\") {\n    toflip <- d[[ea]] > d[[oa]]\n    d[[beta]][toflip] <- d[[beta]][toflip] * -1\n    temp <- d[[oa]][toflip]\n    d[[oa]][toflip] <- d[[ea]][toflip]\n    d[[ea]][toflip] <- temp\n    d[[\"snpid\"]] <- paste0(d[[chr]], \":\", d[[pos]], \"_\", toupper(d[[ea]]), \"_\", toupper(d[[oa]]))\n    d\n}\n\ngreedy_remove <- function(r, maxr=0.99) {\n    diag(r) <- 0\n    flag <- 1\n    rem <- c()\n    nom <- colnames(r)\n    while(flag == 1)\n    {\n        message(\"iteration\")\n        count <- apply(r, 2, function(x) sum(x >= maxr))\n        if(any(count > 0))\n        {\n            worst <- which.max(count)[1]\n            rem <- c(rem, names(worst))\n            r <- r[-worst,-worst]\n        } else {\n            flag <- 0\n        }\n    }\n    return(which(nom %in% rem))\n}\n\nmap <- gwas %>% dplyr::select(rsid=variant_id, chr=chromosome, pos=base_pair_location) %>% filter(!duplicated(rsid))\nldmap <- tibble(vid=rownames(ld), beta=1) %>%\n    tidyr::separate(vid, sep=\"_\", into=c(\"rsid\", \"ea\", \"oa\"), remove=FALSE) %>%\n    left_join(., map, by=\"rsid\") %>%\n    standardise()\ngwas <- subset(gwas, variant_id %in% ldmap$rsid) %>%\n    standardise(ea=\"effect_allele\", oa=\"other_allele\", chr=\"chromosome\", pos=\"base_pair_location\")\ngwas <- subset(gwas, snpid %in% ldmap$snpid)\nldmap <- subset(ldmap, snpid %in% gwas$snpid)\nstopifnot(all(gwas$snpid == ldmap$snpid))\nstopifnot(all(ldmap$vid == rownames(ld)))\n\n# Flip LD based on harmonisation with gwas\nm <- ldmap$beta %*% t(ldmap$beta)\nldh <- ld * m\n```\n:::\n\n\nGet allele frequency, need the standard deviation of each SNP (xvar)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrq <- fread(\"EUR.frq\") %>%\n    inner_join(., map, by=c(\"SNP\"=\"rsid\")) %>%\n    mutate(beta=1) %>%\n    standardise(., ea=\"A1\", oa=\"A2\")\nstopifnot(all(frq$snpid == gwas$snpid))\nxvar <- sqrt(2 * frq$MAF * (1-frq$MAF))\n```\n:::\n\n\n\n### Compression\n\nInverting LD matrices is sometimes difficult because of singularity issues. If at least one of the variants is a linear combination of the other variants in the region then the matrix will be singular. If sample sizes are very large then this is less likely to happen but matrices will still be near singular with occasional hugely inflated values.\n\nTo avoid these issues we could just estimate PCs on the LD matrix, and then perform a lossy compression by selecting the PCs that explain the top x% variation, and then only storing the $\\beta \\Lambda$ matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get principal components of the LD matrix\nldpc <- princomp(ldh)\n# This is the sd of each PC\nplot(ldpc$sdev)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nMost variation explained by rather few SNPs - how many needed to explain 80% of LD info?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nldpc <- princomp(ldh)\ni <- which(cumsum(ldpc$sdev) / sum(ldpc$sdev) >= 0.8)[1]\ni\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nComp.13 \n     13 \n```\n:::\n:::\n\n\nSo 5% of the original data size required to capture 80% of the variation?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compress using only 80% of PC variation\ncomp <- (gwas$beta) %*% ldpc$loadings[,1:i]\n# Uncompress back to betas\nuncomp <- comp %*% t(ldpc$loadings[,1:i])\n```\n:::\n\n\nPlot compressed betas against original betas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(uncomp, gwas$beta)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nHow much info is lost?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(drop(uncomp), gwas$beta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7708481\n```\n:::\n:::\n\n\nIs there bias? e.g. is coefficient different from 1?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(gwas$beta ~ drop(uncomp)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = gwas$beta ~ drop(uncomp))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0130259 -0.0009433  0.0001189  0.0010631  0.0110847 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.928e-05  1.465e-04  -0.473    0.637    \ndrop(uncomp)  9.973e-01  4.988e-02  19.994   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.002414 on 273 degrees of freedom\nMultiple R-squared:  0.5942,\tAdjusted R-squared:  0.5927 \nF-statistic: 399.8 on 1 and 273 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nDoesn't seem too bad. How consistent is the sign in the compressed vs original?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(sign(gwas$beta) == sign(uncomp))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFALSE  TRUE \n   40   235 \n```\n:::\n:::\n\n\nOverall 5% of the data seems to capture a reasonable amount of information.\n\n### Standard errors\n\nThe above does it for betas, but standard errors are a bit more complicated. Could just use the approx marginal $se = 1/2pq$, but this will ignore correlation structure\n\n\n\n## Unfinished (ignore)\n\nSE for each SNP will be\n\n$$\ns \\rho s\n$$\n\nwhere s is diagonal matrix of standard errors. Represent $\\rho = QA^{-1}Q^T$ where A is diagonal matrix of eigenvalues and Q is matrix of eigenvectors / loadings\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp <- as.matrix(ldpc$loadings) %*% diag(1/ldpc$sdev) %*% t(as.matrix(ldpc$loadings))\ntemp[1:10,1:10]\ncor(temp, ldh)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- prcomp(ldh)\nnames(x)\nstr(x)\nstr(ldpc)\n\ntemp <- x$vectors %*% diag(1/x$values) %*% t(x$vectors)\nfor(i in 1:nrow(temp)) { temp[,i] <- temp[,i] / temp[i,i]}\ntemp[1:10,1:10]\nplot(c(ldh), c(temp))\n```\n:::\n\n\n\ngwas stats = `gwas`\nld matrix = `ldh`\n\nhttps://explodecomputer.github.io/simulateGP/articles/gwas_summary_data_ld.html\n\nTry to solve ld\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntry(solve(ldh))\n```\n:::\n\n\nThis doesn't work - the matrix is singular. How to avoid? e.g. remove SNPs in high LD\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- greedy_remove(ldh, 0.99)\ngwas <- gwas[-g,]\nldh <- ldh[-g, -g]\nldmap <- ldmap[-g,]\nxvar <- xvar[-g]\nstopifnot(all(gwas$snpid == ldmap$snpid))\ntry(solve(ldh))\n```\n:::\n\n\n\nOk this is a problem. How to select SNPs to include that will involve a non-singular matrix?\n\n- Make sparse\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconv <- function(b, se, ld, xvar) {\n    # make sparse\n    bs <- (b %*% diag(xvar) %*% solve(ld) %*% diag(1/xvar)) %>% drop()\n    # make dense again\n    bhat <- (diag(1/xvar) %*% ld %*% diag(xvar) %*% bs) %>% drop()\n    # create sparse version of bs\n    tibble(b, bs, bhat)\n}\n\n#o <- conv(gwas$beta, gwas$standard_error, ldh, xvar)\n```\n:::\n\n\n\n\n- Make dense again\n- Compare sparse and dense \n\n\nSimulations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mvtnorm)\nlibrary(simulateGP)\n\n# Provide matrix of SNPs, phenotype y, true effects of SNPs on y\ncalcs <- function(x, y, b) {\n    xpx <- t(x) %*% x\n    D <- matrix(0, ncol(x), ncol(x))\n    diag(D) <- diag(xpx)\n    # Estimate effects (these will have LD influence)\n    betahat <- gwas(y, x)$bhat\n    # Convert back to marginal effects - this is approx, doesn't use AF\n    bhat <- drop(solve(xpx) %*% D %*% betahat)\n    # Determine betas with LD\n    betahatc <- b %*% xpx %*% solve(D) %>% drop\n    rho <- cor(x)\n    xvar <- apply(x, 2, sd)\n    # Another way to determine betas with LD using just sum stats\n    betahatrho <- (diag(1/xvar) %*% rho %*% diag(xvar) %*% b) %>% drop\n    # Go back to true betas\n    betaback <- (betahatrho %*% diag(xvar) %*% solve(rho) %*% diag(1/xvar)) %>% drop()\n    tibble(b, bhat, betahat, betahatc, betahatrho, betaback)\n}\n\nn <- 10000\nnsnp <- 20\nsigma <- matrix(0.7, nsnp, nsnp)\ndiag(sigma) <- 1\nx <- rmvnorm(n, rep(0, nsnp), sigma)\n\nb <- rnorm(nsnp) * 100\ny <- x %*% b + rnorm(n)\nres <- calcs(x, y, b)\nres\n```\n:::\n\n\n\n\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.8\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] glue_1.6.2        tidyr_1.3.0       dplyr_1.1.2       data.table_1.14.8\n[5] ieugwasr_0.1.5   \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       cli_3.6.1         knitr_1.43        rlang_1.1.1      \n [5] xfun_0.39         purrr_1.0.1       generics_0.1.3    jsonlite_1.8.5   \n [9] htmltools_0.5.5   fansi_1.0.4       rmarkdown_2.22    evaluate_0.21    \n[13] tibble_3.2.1      fastmap_1.1.1     yaml_2.3.7        lifecycle_1.0.3  \n[17] compiler_4.3.0    htmlwidgets_1.6.2 pkgconfig_2.0.3   rstudioapi_0.14  \n[21] digest_0.6.31     R6_2.5.1          tidyselect_1.2.0  utf8_1.2.3       \n[25] pillar_1.9.0      magrittr_2.0.3    withr_2.5.0       tools_4.3.0      \n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}