[
  {
    "objectID": "posts/2022-01-09-did-models-confounding/index.html",
    "href": "posts/2022-01-09-did-models-confounding/index.html",
    "title": "Difference in difference models in observational data are still potentially confounded",
    "section": "",
    "text": "Motivated by discussion with Tim Cadman relating to this Renzi et al (2019). Long-Term PM10 Exposure and Cause-Specific Mortality in the Latium Region (Italy): A Difference-in-Differences Approach.\nFor argument’s sake suppose that air pollution (PM10) isn’t causal for deaths. Simulate a situation where wealth varies by region, and each region has a different wealth trajectory over time. Some info:\n\nWealth, PM10 and deaths are measured for 10 years in 300 regions.\nWealth causes PM10 and deaths.\nThere is a global confounder for the start point of wealth and deaths\nTime has an additional effect on both deaths and wealth - i.e. it’s also a global confounder\nThe causal effect of wealth on death = 1. We want our model to get that right.\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nnregion <- 300\nregion <- 1:300\nwealth_int <- rnorm(nregion)\nwealth_slope <- rnorm(nregion)\nglobal_confounder <- rnorm(nregion, sd=10)\nnyear <- 10\n\ndat <- lapply(1:nregion, function(i)\n  {\n    tibble(\n      region=i,\n      year=1:nyear,\n      # wealth goes up due to global confounder, year, random error\n      wealth = global_confounder[i] + wealth_int[i] + year * wealth_slope[i] + rnorm(nyear),\n      # PM10 only related to wealth and random error\n      pm10 = wealth + rnorm(nyear, sd=5),\n      # deaths go up due to global confounders, wealth, year and random term\n      deaths = global_confounder[i] + wealth + rnorm(nyear) + year\n    )\n}) %>%\n  bind_rows()\n\n\ndat\n\n# A tibble: 3,000 × 5\n   region  year wealth   pm10 deaths\n    <int> <int>  <dbl>  <dbl>  <dbl>\n 1      1     1   2.25  2.28    5.18\n 2      1     2   1.81  0.393   5.79\n 3      1     3   2.25  4.05    9.20\n 4      1     4   2.89 12.0    11.6 \n 5      1     5   4.02  7.55   12.7 \n 6      1     6   3.15  3.08   12.7 \n 7      1     7   1.67  1.51   11.3 \n 8      1     8   2.83  4.92   13.4 \n 9      1     9   5.08  8.42   16.2 \n10      1    10   4.96 10.1    18.4 \n# … with 2,990 more rows\n\n\nPlot it showing change in deaths over time by region\n\nggplot(dat, aes(x=year, y=deaths)) +\n  geom_point(aes(group=as.factor(region))) +\n  geom_line(aes(group=as.factor(region)))\n\n\n\n\nTry again but just regression lines per region\n\nggplot(dat, aes(x=year, y=deaths)) +\n  geom_point(aes(group=as.factor(region))) +\n  geom_smooth(method=\"lm\", aes(group=as.factor(region)), se=FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nShow that PM10 is closely coupled with wealth\n\nplot(pm10 ~ wealth, dat)\n\n\n\n\nUse regression to test for influence of wealth on deaths - this gives a very confounded result because of the global confounder.\n\nsummary(lm(deaths ~ wealth, dat))\n\n\nCall:\nlm(formula = deaths ~ wealth, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.6518  -4.4416  -0.7553   3.7309  25.9024 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 5.760562   0.115301   49.96   <2e-16 ***\nwealth      1.715797   0.009365  183.21   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.315 on 2998 degrees of freedom\nMultiple R-squared:  0.918, Adjusted R-squared:  0.918 \nF-statistic: 3.357e+04 on 1 and 2998 DF,  p-value: < 2.2e-16\n\n\nSame will be true for PM10\n\nsummary(lm(deaths ~ pm10, dat))\n\n\nCall:\nlm(formula = deaths ~ pm10, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.668  -6.799  -0.196   6.836  38.319 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.88951    0.18634    31.6   <2e-16 ***\npm10         1.46754    0.01399   104.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.21 on 2998 degrees of freedom\nMultiple R-squared:  0.7858,    Adjusted R-squared:  0.7857 \nF-statistic: 1.1e+04 on 1 and 2998 DF,  p-value: < 2.2e-16\n\n\nSo PM10 is confounded at two levels - the between-region (global) level and the within-region level.\nNow do a DiD model for wealth - it should give us an unbiased estimate of 1. Note that this long form of DiD is sometimes called a fixed-effects model.\n\nsummary(lm(deaths ~ wealth + as.factor(region) + as.factor(year), dat)) %>%\n  coefficients %>% as_tibble %>% slice(n=2)\n\n# A tibble: 1 × 4\n  Estimate `Std. Error` `t value` `Pr(>|t|)`\n     <dbl>        <dbl>     <dbl>      <dbl>\n1    0.993      0.00590      168.          0\n\n\nIt does. What about DiD model for PM10? This should be less confounded because it eliminates the global confounder, but still confounded by the structural confounding that happens at all areas\n\nsummary(lm(deaths ~ pm10 + as.factor(region) + as.factor(year), dat)) %>%\n  coefficients %>% as_tibble %>% slice(n=2)\n\n# A tibble: 1 × 4\n  Estimate `Std. Error` `t value` `Pr(>|t|)`\n     <dbl>        <dbl>     <dbl>      <dbl>\n1    0.293      0.00931      31.4  4.94e-185\n\n\nIf we control for wealth, the effect of PM10 will be unbiased because the within-region bias has been removed, and the global bias acted via wealth anyway so that’s been removed also\n\nsummary(lm(deaths ~ pm10 + wealth, dat)) %>%\n  coefficients %>% as_tibble %>% slice(n=2)\n\n# A tibble: 1 × 4\n  Estimate `Std. Error` `t value` `Pr(>|t|)`\n     <dbl>        <dbl>     <dbl>      <dbl>\n1  -0.0224       0.0231    -0.969      0.333\n\n\nWe could try to simplify by doing the more explicit difference in difference estimate. Compare the change in deaths with the change in wealth (or PM10) over the 10 year period.\n\ndat2 <- group_by(dat, region) %>%\n  summarise(\n    delta_deaths = deaths[nyear] - deaths[1],\n    delta_wealth = wealth[nyear] - wealth[1],\n    delta_pm10 = pm10[nyear] - pm10[1]\n  )\ndat2\n\n# A tibble: 300 × 4\n   region delta_deaths delta_wealth delta_pm10\n    <int>        <dbl>        <dbl>      <dbl>\n 1      1        13.2         2.71        7.80\n 2      2        19.6        12.9        18.6 \n 3      3        20.6        10.6         6.65\n 4      4         4.24       -3.10      -10.7 \n 5      5         9.08       -0.285      -9.31\n 6      6        17.7         9.94        8.22\n 7      7        10.7         2.15       10.4 \n 8      8        16.7         6.61       -3.01\n 9      9         6.55       -2.51       13.1 \n10     10        17.9         9.83       10.8 \n# … with 290 more rows\n\n\nDo the DiD estimates using these - should recapitulate what we got above\n\nsummary(lm(delta_deaths ~ delta_wealth, dat2))\n\n\nCall:\nlm(formula = delta_deaths ~ delta_wealth, data = dat2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6788 -0.9511  0.1077  1.0252  3.6020 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.063688   0.082771   109.5   <2e-16 ***\ndelta_wealth 0.990565   0.008896   111.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.434 on 298 degrees of freedom\nMultiple R-squared:  0.9765,    Adjusted R-squared:  0.9764 \nF-statistic: 1.24e+04 on 1 and 298 DF,  p-value: < 2.2e-16\n\n\n\nsummary(lm(delta_deaths ~ delta_pm10, dat2))\n\n\nCall:\nlm(formula = delta_deaths ~ delta_pm10, data = dat2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.0715  -3.9080  -0.3154   3.7670  16.6385 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.09954    0.33647   27.04   <2e-16 ***\ndelta_pm10   0.61677    0.02844   21.69   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.827 on 298 degrees of freedom\nMultiple R-squared:  0.6122,    Adjusted R-squared:  0.6109 \nF-statistic: 470.5 on 1 and 298 DF,  p-value: < 2.2e-16\n\n\nWeirdly, it works for wealth but we do get a slightly difference answer for PM10. There is probably a lot of literature on what makes a fixed effects model (the first version of the DiD we did above) different from an explicit version like this one.\nTo summarise - the DiD model is useful to account for unmeasured global confounders (including time), but it might not be too surprising that it doesn’t control for all confounders - you really do need some sort of experiment / randomisation that specifically mimics the exact intervention you want to make to get to completely unconfounded effects.\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.0 dplyr_1.0.10 \n\nloaded via a namespace (and not attached):\n [1] pillar_1.8.1      compiler_4.2.1    tools_4.2.1       digest_0.6.31    \n [5] lattice_0.20-45   nlme_3.1-158      jsonlite_1.8.4    evaluate_0.19    \n [9] lifecycle_1.0.3   tibble_3.1.8      gtable_0.3.1      mgcv_1.8-40      \n[13] pkgconfig_2.0.3   rlang_1.0.6       Matrix_1.4-1      DBI_1.1.3        \n[17] cli_3.5.0         yaml_2.3.6        xfun_0.36         fastmap_1.1.0    \n[21] withr_2.5.0       stringr_1.5.0     knitr_1.41        generics_0.1.3   \n[25] vctrs_0.5.1       htmlwidgets_1.5.4 grid_4.2.1        tidyselect_1.2.0 \n[29] glue_1.6.2        R6_2.5.1          fansi_1.0.3       rmarkdown_2.16   \n[33] farver_2.1.1      magrittr_2.0.3    scales_1.2.1      htmltools_0.5.4  \n[37] splines_4.2.1     assertthat_0.2.1  colorspace_2.0-3  labeling_0.4.2   \n[41] utf8_1.2.2        stringi_1.7.8     munsell_0.5.0"
  },
  {
    "objectID": "posts/2022-11-01-mvnorm-max-variable/index.html",
    "href": "posts/2022-11-01-mvnorm-max-variable/index.html",
    "title": "Probability of a random variable being larger than all other random variables in a multivariate normal vector",
    "section": "",
    "text": "I have 1k SNPs in a region. I know the causal variant and the LD matrix. The effect size at each SNP will be related to the allele frequency and the LD at all other variants. The SE across the SNPs will be correlated in relation to the LD. I can generate the expected effect size and the variance covariance matrix of the effects. Once I have that, I can generate beta values from a multivariate normal distribution, and determine how often each of the SNPs is the top SNP.\nIs there a faster way to do this by getting the probability from a multivariate normal distribution?\nRelated to this question: https://stats.stackexchange.com/a/4181\n\nlibrary(MCMCpack)\n\nLoading required package: coda\n\n\nLoading required package: MASS\n\n\n##\n## Markov Chain Monte Carlo Package (MCMCpack)\n\n\n## Copyright (C) 2003-2022 Andrew D. Martin, Kevin M. Quinn, and Jong Hee Park\n\n\n##\n## Support provided by the U.S. National Science Foundation\n\n\n## (Grants SES-0350646 and SES-0350613)\n##\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(simulateGP)\nlibrary(MASS)\nlibrary(mvtnorm)\n\nEmpirical simulation for probabilities, case of 3 variables\n\nn <- 3\nmu <- rnorm(n)\nS <- rwish(n, diag(n))\nemp <- mvrnorm(1000, mu, S)\nres <- apply(emp, 1, function(x) which.max(x)) %>% table() %>% prop.table()\nres\n\n.\n    1     2     3 \n0.666 0.146 0.188 \n\n\n\nA <- matrix(c(1,-1,0, 1,0,-1), nrow = 2, byrow = TRUE)\nnewMu <- as.vector(A %*% mu)\nnewS <- A %*% S %*% t(A)\npmvnorm(lower=c(0,0), mean = newMu, sigma = newS)\n\n[1] 0.6660382\nattr(,\"error\")\n[1] 1e-15\nattr(,\"msg\")\n[1] \"Normal Completion\"\n\nA <- matrix(c(1,-1,0, 1,0,-1), nrow = 2, byrow = TRUE)\nA <- A[,c(2,1,3)]\nnewMu <- as.vector(A %*% mu)\nnewS <- A %*% S %*% t(A)\npmvnorm(lower=c(0,0), mean = newMu, sigma = newS)\n\n[1] 0.1487365\nattr(,\"error\")\n[1] 1e-15\nattr(,\"msg\")\n[1] \"Normal Completion\"\n\nA <- matrix(c(1,-1,0, 1,0,-1), nrow = 2, byrow = TRUE)\nA <- A[,c(2,3,1)]\nnewMu <- as.vector(A %*% mu)\nnewS <- A %*% S %*% t(A)\npmvnorm(lower=c(0,0), mean = newMu, sigma = newS)\n\n[1] 0.1852253\nattr(,\"error\")\n[1] 1e-15\nattr(,\"msg\")\n[1] \"Normal Completion\"\n\n\nIncrease to arbitrary variables\nUse wishart distribution to generate random vcov matrix\n\nn <- 100\nmu <- rnorm(n)\nS <- rwish(n, diag(n))\n\nEmpirically generate correlated variables and count how often each one is the largest\n\nsamp <- mvrnorm(10000, mu, S)\nres <- apply(samp, 1, function(x) which.max(x)) %>% table() %>% prop.table()\nres\n\n.\n     1      2      3      4      5      6      7      8      9     10     11 \n0.0232 0.0085 0.0211 0.0072 0.0152 0.0118 0.0038 0.0161 0.0058 0.0094 0.0080 \n    12     13     14     15     16     17     18     19     20     21     22 \n0.0083 0.0152 0.0250 0.0103 0.0131 0.0115 0.0112 0.0093 0.0105 0.0084 0.0066 \n    23     24     25     26     27     28     29     30     31     32     33 \n0.0055 0.0088 0.0150 0.0098 0.0182 0.0171 0.0065 0.0090 0.0074 0.0134 0.0057 \n    34     35     36     37     38     39     40     41     42     43     44 \n0.0125 0.0084 0.0129 0.0088 0.0110 0.0222 0.0154 0.0079 0.0046 0.0111 0.0045 \n    45     46     47     48     49     50     51     52     53     54     55 \n0.0045 0.0071 0.0034 0.0045 0.0133 0.0053 0.0096 0.0013 0.0078 0.0138 0.0053 \n    56     57     58     59     60     61     62     63     64     65     66 \n0.0138 0.0039 0.0025 0.0101 0.0100 0.0059 0.0050 0.0206 0.0043 0.0157 0.0068 \n    67     68     69     70     71     72     73     74     75     76     77 \n0.0068 0.0119 0.0097 0.0117 0.0030 0.0162 0.0082 0.0040 0.0077 0.0156 0.0037 \n    78     79     80     81     82     83     84     85     86     87     88 \n0.0135 0.0082 0.0168 0.0033 0.0095 0.0072 0.0062 0.0124 0.0062 0.0156 0.0088 \n    89     90     91     92     93     94     95     96     97     98     99 \n0.0038 0.0141 0.0086 0.0089 0.0184 0.0117 0.0069 0.0180 0.0098 0.0110 0.0058 \n   100 \n0.0071 \n\n\nUse probability density function instead, evaluating for each variable the probability that it’s larger than all the other variables\n\n# Create design matrix\nswap_1 <- function(n, i)\n{\n  ind <- 1:n\n  if(i == 1) return(ind)\n  ind[i] <- 1\n  ind[1:(i-1)] <- 2:i\n  return(ind)\n}\n#sapply(1:7, function(i) swap_1(7, i))\nA <- cbind(\n  rep(1, n-1),\n  diag(rep(-1, n-1))\n) %>% as.matrix()\n\nemp <- sapply(1:n, function(i)\n{\n  A <- A[,swap_1(n,i)]\n  newMu <- as.vector(A %*% mu)\n  newS <- A %*% S %*% t(A)\n  pmvnorm(lower=rep(0,n-1), mean = newMu, sigma = newS)\n})\nplot(emp ~ as.numeric(res))\n\n\n\n\nTheoretical result works fine but is slower than empirical sampling."
  },
  {
    "objectID": "posts/2022-12-14-wf-rare-variants/index.html",
    "href": "posts/2022-12-14-wf-rare-variants/index.html",
    "title": "Sibling replication of Backman rare variants",
    "section": "",
    "text": "Backman et al 2022 (https://www.nature.com/articles/s41586-021-04103-z) performed exome GWAS of 4k traits, and found 564 rare-variant - trait pairs. They corrected for 20 PCs from common variants and 20 PCs from rare variants.\nQuestion: Is uncontrolled recent population stratification biasing these associations or even driving false positives?\nWe could use siblings to estimate the direct effects for these discovery associations. There would be two major hits to power, first only 20k of the 500k UK Biobank will be used. Second, power to detect direct effects using siblings is reduced. So we can’t conclude that these are false positives if they simply don’t replicate in the sibling analysis.\nInstead, we can ask to what extent are the replication associations consistent with the original estimates, given the reduction in precision. i.e. What fraction are expected to replicate at some nominal threshold, and what fraction are observed to replicate. If there is a significant difference between the observed and expected replication rate, then that indicates uncorrected inflation in the discovery exome GWAS.\nThis analysis: Power calculation - how much shrinkage would there need to be before there is 80% power to detect a difference between the sibling estimate and the population estimate?\nRead in Backman results\nThere were 564 rare-variant - trait pairs in Backman et al 2022. Of these 484 were for quantitative traits, restricting to those here for simplicity.\n\nlibrary(here)\n\nhere() starts at /Users/gh13047/repo/lab-book\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nset.seed(12345)\ndat <- read.csv(\"Book3.csv\", stringsAsFactors=FALSE, header=TRUE) %>% \n  as_tibble() %>%\n  mutate(se = (uci - beta)/1.96)\ndat %>% glimpse()\n\nRows: 484\nColumns: 14\n$ Effect..95..CI.                               <chr> \"-0.15 (-0.19, -0.11)\", …\n$ P.value                                       <dbl> 9.25e-12, 1.24e-24, 1.03…\n$ Effect.direction                              <chr> \"-\", \"-\", \"-\", \"+\", \"-\",…\n$ N.cases.with.0.1.2.copies.of.effect.allele    <chr> \"88038|1684|13\", \"418277…\n$ N.controls.with.0.1.2.copies.of.effect.allele <chr> \"NA|NA|NA\", \"NA|NA|NA\", …\n$ Effect.allele.frequency                       <dbl> 9.5e-03, 2.1e-04, 4.9e-0…\n$ AA                                            <int> 88038, 418277, 407938, 3…\n$ AB                                            <int> 1684, 177, 4027, 4076, 1…\n$ BB                                            <int> 13, 0, 11, 1, 8, 0, 1, 0…\n$ N                                             <int> 89735, 418454, 411976, 3…\n$ beta                                          <dbl> -0.150, -0.720, -0.100, …\n$ lci                                           <chr> \"-0.19\", \"-0.86\", \"-0.13…\n$ uci                                           <dbl> -0.110, -0.580, -0.079, …\n$ se                                            <dbl> 0.020408163, 0.071428571…\n\nplot(-log10(dat$P.value), abs(dat$beta)/dat$se)\n\n\n\n\nFunctions from Alex for relative sample size between designs:\n\nunrel_vs_sib = function(h2){\n  return(1+h2*(1-h2)/(4-h2))\n}\n\nunrel_vs_sib_inv = function(h2){\n  return((1+h2*(1-h2)/(4-h2))^(-1))\n}\n\nunrel_vs_trio = function(h2){\n  return(1+h2*(1-h2)/(3-h2*(1+h2/2)))\n}\n\nunrel_vs_trio_inv = function(h2){\n  return((1+h2*(1-h2)/(3-h2*(1+h2/2)))^(-1))\n}\n\nunrel_vs_direct_trio = function(h2){return(0.5/3)}\nunrel_vs_direct_sibdiff = function(h2){return((2*(2-h2))^(-1))}\nunrel_vs_direct_sibimp = function(h2){return((2+h2/2)/(6*(1-(h2/2)^2)))}\n\nFunction to estimate expected replication rate\n\nprop_overlap <- function(b_disc, b_rep, se_disc, se_rep, alpha)\n{\n  p_sign <- pnorm(-abs(b_disc) / se_disc) * pnorm(-abs(b_disc) / se_rep) + ((1 - pnorm(-abs(b_disc) / se_disc)) * (1 - pnorm(-abs(b_disc) / se_rep)))\n  p_sig <- pnorm(-abs(b_disc) / se_rep + qnorm(alpha / 2)) + (1 - pnorm(-abs(b_disc) / se_rep - qnorm(alpha / 2)))\n  p_rep <- pnorm(abs(b_rep)/se_rep, lower.tail=FALSE)\n  res <- tibble::tibble(\n    nsnp=length(b_disc),\n    metric=c(\"Sign\", \"Sign\", \"P-value\", \"P-value\"),\n    datum=c(\"Expected\", \"Observed\", \"Expected\", \"Observed\"),\n    value=c(sum(p_sign, na.rm=TRUE), sum(sign(b_disc) == sign(b_rep)), sum(p_sig, na.rm=TRUE), sum(p_rep < alpha, na.rm=TRUE))\n  )\n  pdif <- list(\n    Sign = binom.test(res$value[2], res$nsnp[1], res$value[1] / res$nsnp[1])$p.value,\n    `P-value` = binom.test(res$value[4], res$nsnp[1], res$value[3] / res$nsnp[1])$p.value\n  ) %>% as_tibble()\n  return(list(res=res, pdif=pdif, variants=dplyr::tibble(sig=p_sig, sign=p_sign)))\n}\n\nFunction to simulate assocs in siblings\n\nexpected_se <- function (beta, af, n, vy) \n{\n    sqrt(c(vy) - beta^2 * 2 * af * (1 - af))/sqrt(n) * (1/sqrt(2 * \n        af * (1 - af)))\n}\n\nexpected_vy <- function(beta, af, n, se)\n{\n  se^2 * 2 * af * (1 - af)*n + beta^2 * 2 * af * (1 - af)\n}\n\n# check algebra!\n# expected_se(0.2, 0.4, 1000, 10)\n# expected_vy(0.2, 0.4, 1000, expected_se(0.2, 0.4, 1000, 10))\n\nsimgwas_sib <- function(beta_disc, se_disc, af_disc, n_disc, prop_sibs, h2, alpha, shrinkage)\n{\n  d <- tibble(\n    beta_disc = beta_disc,\n    beta_true = beta_disc * shrinkage,\n    vy = expected_vy(beta_disc, af_disc, n_disc, se_disc),\n    n_rep = n_disc * prop_sibs * unrel_vs_direct_sibimp(h2),\n    se_rep = expected_se(beta_disc, af_disc, n_rep, vy),\n    beta_rep = rnorm(length(beta_disc), beta_true, se_rep)\n  ) \n  d %>%\n    {prop_overlap(.$beta_disc, .$beta_rep, se_disc, .$se_rep, alpha)}\n}\n\nPower analysis\n\nparams <- expand.grid(sim=1:500, shrinkage=seq(0,1,0.05))\nres <- lapply(1:nrow(params), function(i) \n  simgwas_sib(dat$beta, dat$se, dat$Effect.allele.frequency, dat$N, 20000/500000,0.5, 0.05/nrow(dat), params$shrinkage[i])\n  ) %>%\n  lapply(., function(x) x$pdif) %>% bind_rows() %>% bind_cols(., params)\nres\n\n# A tibble: 10,500 × 4\n        Sign `P-value`   sim shrinkage\n       <dbl>     <dbl> <int>     <dbl>\n 1 1.04e- 91  2.33e-11     1         0\n 2 3.19e- 95  2.33e-11     2         0\n 3 7.72e- 91  2.33e-11     3         0\n 4 4.13e- 89  2.33e-11     4         0\n 5 3.52e-105  2.33e-11     5         0\n 6 6.78e- 98  2.33e-11     6         0\n 7 1.85e- 93  2.33e-11     7         0\n 8 4.14e- 96  2.33e-11     8         0\n 9 1.76e-118  9.03e-10     9         0\n10 9.63e-112  2.33e-11    10         0\n# … with 10,490 more rows\n\n\n\nres %>% \n  group_by(shrinkage) %>%\n  summarise(Sign=sum(Sign < 0.05)/n(), `P-value`=sum(`P-value` < 0.05)/n()) %>%\n  tidyr::gather(key=\"key\", value=\"value\", c(`Sign`, `P-value`)) %>%\n  ggplot(., aes(x=shrinkage, y=value)) +\n    geom_point(aes(colour=key)) +\n    geom_line(aes(colour=key)) +\n    labs(x=\"Shrinkage of effect size, or 1-FDR\", y=\"Power to detect difference from expectation\")"
  },
  {
    "objectID": "posts/2022-12-14-wf-rare-variants/index.html#summary",
    "href": "posts/2022-12-14-wf-rare-variants/index.html#summary",
    "title": "Sibling replication of Backman rare variants",
    "section": "Summary",
    "text": "Summary\nPower is quite high to detect a difference!\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.0 dplyr_1.0.10  here_1.0.1   \n\nloaded via a namespace (and not attached):\n [1] pillar_1.8.1      compiler_4.2.1    tools_4.2.1       digest_0.6.31    \n [5] jsonlite_1.8.4    evaluate_0.19     lifecycle_1.0.3   tibble_3.1.8     \n [9] gtable_0.3.1      pkgconfig_2.0.3   rlang_1.0.6       cli_3.5.0        \n[13] DBI_1.1.3         yaml_2.3.6        xfun_0.36         fastmap_1.1.0    \n[17] withr_2.5.0       stringr_1.5.0     knitr_1.41        generics_0.1.3   \n[21] vctrs_0.5.1       htmlwidgets_1.5.4 rprojroot_2.0.3   grid_4.2.1       \n[25] tidyselect_1.2.0  glue_1.6.2        R6_2.5.1          fansi_1.0.3      \n[29] rmarkdown_2.16    farver_2.1.1      tidyr_1.2.1       purrr_1.0.0      \n[33] magrittr_2.0.3    ellipsis_0.3.2    scales_1.2.1      htmltools_0.5.4  \n[37] assertthat_0.2.1  colorspace_2.0-3  labeling_0.4.2    utf8_1.2.2       \n[41] stringi_1.7.8     munsell_0.5.0"
  },
  {
    "objectID": "posts/2022-08-12-correlated-snps/index.html",
    "href": "posts/2022-08-12-correlated-snps/index.html",
    "title": "Correlated SNPs",
    "section": "",
    "text": "One instrument for X and X has no influence on Y\n\nlibrary(TwoSampleMR)\n\nTwoSampleMR version 0.5.6 \n[>] New: Option to use non-European LD reference panels for clumping etc\n[>] Some studies temporarily quarantined to verify effect allele\n[>] See news(package='TwoSampleMR') and https://gwas.mrcieu.ac.uk for further details\n\nlibrary(simulateGP)\n\n\nAttaching package: 'simulateGP'\n\n\nThe following objects are masked from 'package:TwoSampleMR':\n\n    allele_frequency, contingency, get_population_allele_frequency\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nset.seed(12345)\nmap <- tibble(snp=1, af=0.5)\nparams_x <- generate_gwas_params(map=map, h2=0.01, S=-0.4, Pi=1)\nparams_y <- generate_gwas_params(map=map, h2=0.0, S=-0.4, Pi=1)\nnid <- 100000\nss <- summary_set(\n    beta_gx=params_x$beta,\n    beta_gy=params_y$beta,\n    af=params_x$af,\n    n_gx=10000,\n    n_gy=10000,\n    n_overlap=0,\n    cor_xy=0.5\n)\n\nPerform MR with single causal variant\n\nmr(ss) %>% glimpse()\n\nAnalysing 'X' on 'Y'\n\n\nRows: 1\nColumns: 9\n$ id.exposure <chr> \"X\"\n$ id.outcome  <chr> \"Y\"\n$ outcome     <chr> \"Y\"\n$ exposure    <chr> \"X\"\n$ method      <chr> \"Wald ratio\"\n$ nsnp        <dbl> 1\n$ b           <dbl> -0.08648138\n$ se          <dbl> 0.0892847\n$ pval        <dbl> 0.3327436\n\n\nPerform MR with causal variant + 100 correlated tag SNPs\n\nss2 <- ss[rep(1,100),] %>% mutate(SNP=1:100)\nmr(ss2, method_list=\"mr_ivw\") %>% glimpse()\n\nAnalysing 'X' on 'Y'\n\n\nRows: 1\nColumns: 9\n$ id.exposure <chr> \"X\"\n$ id.outcome  <chr> \"Y\"\n$ outcome     <chr> \"Y\"\n$ exposure    <chr> \"X\"\n$ method      <chr> \"Inverse variance weighted\"\n$ nsnp        <int> 100\n$ b           <dbl> -0.08648138\n$ se          <dbl> 0.00892847\n$ pval        <dbl> 3.457243e-22\n\n\nVery small p-value - inflated type 1 error"
  },
  {
    "objectID": "posts/2022-08-12-correlated-snps/index.html#grs-correlation-performance",
    "href": "posts/2022-08-12-correlated-snps/index.html#grs-correlation-performance",
    "title": "Correlated SNPs",
    "section": "GRS correlation performance",
    "text": "GRS correlation performance\nTag SNPs are perfectly correlated with causal variant\n\nsim <- function(nid=10000, nsnp=10)\n{\n  g <- matrix(0, nid, nsnp)\n  g[,1] <- rnorm(nid)\n  for(i in 2:nsnp)\n  {\n    g[,i] <- g[,1]\n  }\n  y <- g[,1] + rnorm(nid)\n  summary(lm(y ~ g[,1]))\n  grs <- rowSums(g)\n  return(c(cor(y, g[,1])^2, cor(y, grs)^2))\n}\nsapply(1:10, function(i) sim()) %>% rowMeans() %>% tibble(method=c(\"Causal variant only\", \"GRS\"), rsq=.)\n\n# A tibble: 2 × 2\n  method                rsq\n  <chr>               <dbl>\n1 Causal variant only 0.500\n2 GRS                 0.500\n\n\nGRS and single causal variant work the same as Jack showed.\nTag SNPs are imperfectly correlated with causal variant\n\nsim <- function(nid=10000, nsnp=10)\n{\n  g <- matrix(0, nid, nsnp)\n  g[,1] <- rnorm(nid)\n  for(i in 2:nsnp)\n  {\n    g[,i] <- g[,i] + rnorm(nid, sd=0.5)\n  }\n  y <- g[,1] + rnorm(nid)\n  summary(lm(y ~ g[,1]))\n  grs <- rowSums(g)\n  return(c(cor(y, g[,1])^2, cor(y, grs)^2))\n}\nsapply(1:10, function(i) sim()) %>% rowMeans() %>% tibble(method=c(\"Causal variant only\", \"GRS\"), rsq=.)\n\n# A tibble: 2 × 2\n  method                rsq\n  <chr>               <dbl>\n1 Causal variant only 0.500\n2 GRS                 0.156\n\n\nNow the GRS doesn’t work well because it includes the variance of the SNP + noise that isn’t causally related to the trait.\n\\[\nr^2 = \\frac{cov(grs, y)^2}{var(grs) var(y)}\n\\]\ni.e. cov(grs,y) isn’t increasing, but (var(y)) is."
  },
  {
    "objectID": "posts/2022-08-12-correlated-snps/index.html#checking",
    "href": "posts/2022-08-12-correlated-snps/index.html#checking",
    "title": "Correlated SNPs",
    "section": "Checking",
    "text": "Checking\n\nn <- 10000\nnsnp <- 10\ng <- matrix(0, n, nsnp)\ng[,1] <- rnorm(n)\nfor(i in 2:nsnp)\n{\n  g[,i] <- g[,i] + rnorm(n, sd=0.5)\n}\ny <- g[,1] + rnorm(n)\ngrs <- rowSums(g)\ncov(y, grs)\n\n[1] 1.003715\n\ncov(y, g[,1])\n\n[1] 0.9788621\n\nsd(grs)\n\n[1] 1.805151\n\nsd(g[,1])\n\n[1] 0.996039\n\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dplyr_1.0.10      simulateGP_0.1.2  TwoSampleMR_0.5.6\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9        plyr_1.8.7        compiler_4.2.1    pillar_1.8.1     \n [5] iterators_1.0.14  tools_4.2.1       mr.raps_0.2       digest_0.6.29    \n [9] jsonlite_1.8.0    evaluate_0.16     lifecycle_1.0.3   tibble_3.1.8     \n[13] lattice_0.20-45   pkgconfig_2.0.3   rlang_1.0.6       Matrix_1.4-1     \n[17] foreach_1.5.2     DBI_1.1.3         cli_3.4.1         yaml_2.3.5       \n[21] xfun_0.33         fastmap_1.1.0     stringr_1.4.1     knitr_1.40       \n[25] generics_0.1.3    htmlwidgets_1.5.4 vctrs_0.5.1       tidyselect_1.1.2 \n[29] glmnet_4.1-4      grid_4.2.1        nortest_1.0-4     glue_1.6.2       \n[33] R6_2.5.1          fansi_1.0.3       survival_3.4-0    rmarkdown_2.16   \n[37] purrr_0.3.4       magrittr_2.0.3    ellipsis_0.3.2    codetools_0.2-18 \n[41] htmltools_0.5.3   splines_4.2.1     assertthat_0.2.1  shape_1.4.6      \n[45] utf8_1.2.2        stringi_1.7.8"
  },
  {
    "objectID": "posts/2022-12-18-bmi-wf-replication/index.html",
    "href": "posts/2022-12-18-bmi-wf-replication/index.html",
    "title": "BMI instrument replication",
    "section": "",
    "text": "Liza’s analysis of BMI instruments clusters them by PheWAS and finds that cluster 4 relates to SES, and drives BMI-EDU biased effect. Are the instruments in cluster 4 solely due to dynastic confounding? If so they should fail to replicate in the sibling analysis."
  },
  {
    "objectID": "posts/2022-12-18-bmi-wf-replication/index.html#instruments",
    "href": "posts/2022-12-18-bmi-wf-replication/index.html#instruments",
    "title": "BMI instrument replication",
    "section": "Instruments",
    "text": "Instruments\n\nlibrary(ieugwasr)\n\nAPI: public: http://gwas-api.mrcieu.ac.uk/\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nbmi_inst <- list(\n  c(\"rs1097327\",\"rs2186120\",\"rs2166172\",\"rs75641275\",\"rs12037698\",\"rs1446585\",\"rs16846140\",\"rs13062093\",\"rs2051559\",\"rs6861649\",\"rs2281819\",\"rs12662900\",\"rs9388681\",\"rs17132130\",\"rs215634\",\"rs79682948\",\"rs2192649\",\"rs13294945\",\"rs7357754\",\"rs4749937\",\"rs1465900\",\"rs1799992\",\"rs55938344\",\"rs7987928\",\"rs7331420\",\"rs9522279\",\"rs55689274\",\"rs4777541\",\"rs7189149\",\"rs11079849\",\"rs113230003\",\"rs150998792\"),\n  c(\"rs3737992\",\"rs1167311\",\"rs12140153\",\"rs34361149\",\"rs12049202\",\"rs2181375\",\"rs17024393\",\"rs61813324\",\"rs815163\",\"rs2678204\",\"rs17014332\",\"rs563738408\",\"rs6751993\",\"rs4671328\",\"rs6545714\",\"rs4482463\",\"rs815715\",\"rs6769617\",\"rs1225004\",\"rs355777\",\"rs6776471\",\"rs869400\",\"rs4261944\",\"rs1296328\",\"rs6867471\",\"rs2307111\",\"rs28404639\",\"rs7442885\",\"rs55838622\",\"rs13174863\",\"rs146696797\",\"rs9368828\",\"rs34045288\",\"rs72892910\",\"rs2482398\",\"rs2253310\",\"rs765874\",\"rs9688977\",\"rs6950388\",\"rs4722398\",\"rs1470749\",\"rs147678035\",\"rs39330\",\"rs1805123\",\"rs6601527\",\"rs791405\",\"rs12679106\",\"rs201519328\",\"rs9297524\",\"rs17770336\",\"rs2440589\",\"rs3931548\",\"rs3861879\",\"rs2356376\",\"rs17399739\",\"rs6265\",\"rs491711\",\"rs7942037\",\"rs10898330\",\"rs12364470\",\"rs55726687\",\"rs1458156\",\"rs7138383\",\"rs147730268\",\"rs9507895\",\"rs10507483\",\"rs11148421\",\"rs1949204\",\"rs1441264\",\"rs7990098\",\"rs8015400\",\"rs7141420\",\"rs1286138\",\"rs6575340\",\"rs12881629\",\"rs3803286\",\"rs7183417\",\"rs4776970\",\"rs2870111\",\"rs2046002\",\"rs56803094\",\"rs12926311\",\"rs879620\",\"rs7193783\",\"rs4402589\",\"rs11642015\",\"rs117342986\",\"rs11150461\",\"rs4790841\",\"rs56161855\",\"rs11150745\",\"rs9319615\",\"rs57636386\",\"rs111640872\",\"rs3810291\",\"rs6050446\",\"rs67844506\",\"rs6001870\"),\n  c(\"rs12024554\",\"rs35722922\",\"rs593010\",\"rs10803762\",\"rs6772763\",\"rs80082351\",\"rs1471740\",\"rs2606228\",\"rs13107325\",\"rs13176429\",\"rs1919243\",\"rs286818\",\"rs3844598\",\"rs7755574\",\"rs3843540\",\"rs17716502\",\"rs1411432\",\"rs2267958\",\"rs2450447\",\"rs11826177\",\"rs7124681\",\"rs317687\",\"rs3897102\",\"rs9506311\",\"rs35193668\",\"rs3759584\",\"rs862320\",\"rs7774\",\"rs8078135\",\"rs11653258\",\"rs56212061\",\"rs11084554\",\"rs2153740\",\"rs8134638\",\"rs2837398\"),\n  c(\"rs115866895\",\"rs6687953\",\"rs935166\",\"rs13002946\",\"rs72820274\",\"rs12619626\",\"rs13427822\",\"rs72967047\",\"rs9843653\",\"rs1454687\",\"rs34811474\",\"rs17085463\",\"rs1383723\",\"rs35853157\",\"rs1503526\",\"rs1477290\",\"rs9463511\",\"rs9277992\",\"rs236660\",\"rs2045293\",\"rs10954772\",\"rs77883185\",\"rs35529153\",\"rs11782074\",\"rs2398861\",\"rs7030732\",\"rs61845249\",\"rs4595495\",\"rs61903695\",\"rs1048932\",\"rs2292238\",\"rs116394958\",\"rs2933223\",\"rs217672\",\"rs2333012\",\"rs11855853\",\"rs62037365\",\"rs34966008\",\"rs1788808\",\"rs784257\",\"rs11666480\"),\n  c(\"rs4648450\",\"rs12031634\",\"rs1778830\",\"rs61826867\",\"rs170553\",\"rs10185199\",\"rs6545144\",\"rs3806572\",\"rs2861685\",\"rs7557796\",\"rs72844755\",\"rs115584509\",\"rs35882248\",\"rs34373881\",\"rs4377469\",\"rs75557510\",\"rs2035936\",\"rs66679256\",\"rs73213484\",\"rs6831020\",\"rs11099020\",\"rs750090\",\"rs6536575\",\"rs7701777\",\"rs9291822\",\"rs10059453\",\"rs2118793\",\"rs4921301\",\"rs4467770\",\"rs7453694\",\"rs9342196\",\"rs9489620\",\"rs13210756\",\"rs3807652\",\"rs7810870\",\"rs7461253\",\"rs2616192\",\"rs12681792\",\"rs55781253\",\"rs13289199\",\"rs2254331\",\"rs1327808\",\"rs6597653\",\"rs2439823\",\"rs6591\",\"rs75936055\",\"rs34292685\",\"rs7940866\",\"rs329651\",\"rs12422552\",\"rs4761401\",\"rs1901241\",\"rs11613680\",\"rs9579775\",\"rs7995015\",\"rs9527895\",\"rs9522183\",\"rs145946602\",\"rs8027969\",\"rs117632017\",\"rs113182412\",\"rs756717\",\"rs11656076\",\"rs2332306\",\"rs7237783\",\"rs1942826\",\"rs45486197\",\"rs76040172\",\"rs28489620\"),\n  c(\"rs61743745\",\"rs10921760\",\"rs2141004\",\"rs754481\",\"rs80330591\",\"rs199750218\",\"rs12479357\",\"rs4485556\",\"rs2569993\",\"rs9847186\",\"rs13076052\",\"rs762705\",\"rs550669262\",\"rs10865612\",\"rs6780459\",\"rs73169730\",\"rs10938397\",\"rs35851183\",\"rs190301182\",\"rs1428120\",\"rs245775\",\"rs1775255\",\"rs2749929\",\"rs6973656\",\"rs12537134\",\"rs1425717\",\"rs11012732\",\"rs12260817\",\"rs112921972\",\"rs61871615\",\"rs10749233\",\"rs845084\",\"rs67609008\",\"rs2035806\",\"rs12575252\",\"rs555754158\",\"rs2234458\",\"rs704061\",\"rs13353100\",\"rs7498044\",\"rs2342892\",\"rs55931203\",\"rs60764613\",\"rs2155869\",\"rs1389067\",\"rs2247593\",\"rs273505\",\"rs10404726\",\"rs112693590\")\n)\n\nExtract instruments from the population and sibling GWASs\n\nwfest <- ieugwasr::associations(unlist(bmi_inst), \"ieu-b-4815\")\npopest <- ieugwasr::associations(unlist(bmi_inst), \"ieu-b-4816\")\ngiantest <- ieugwasr::associations(unlist(bmi_inst), \"ieu-b-40\")\nukbest <- ieugwasr::associations(unlist(bmi_inst), \"ukb-b-19953\")\n\nReplication rates function\n\nexp_rep <- function(b_disc, b_rep, se_disc, se_rep, alpha)\n{\n  p_sign <- pnorm(-abs(b_disc) / se_disc) * pnorm(-abs(b_disc) / se_rep) + ((1 - pnorm(-abs(b_disc) / se_disc)) * (1 - pnorm(-abs(b_disc) / se_rep)))\n  p_sig <- pnorm(-abs(b_disc) / se_rep + qnorm(alpha / 2)) + (1 - pnorm(-abs(b_disc) / se_rep - qnorm(alpha / 2)))\n  p_rep <- pnorm(abs(b_rep)/se_rep, lower.tail=FALSE)\n  res <- tibble::tibble(\n    nsnp=length(b_disc),\n    metric=c(\"Sign\", \"Sign\", \"P-value\", \"P-value\"),\n    datum=c(\"Expected\", \"Observed\", \"Expected\", \"Observed\"),\n    value=c(sum(p_sign, na.rm=TRUE), sum(sign(b_disc) == sign(b_rep)), sum(p_sig, na.rm=TRUE), sum(p_rep < alpha, na.rm=TRUE))\n  )\n  return(list(res=res, variants=dplyr::tibble(sig=p_sig, sign=p_sign)))\n}\n\nAnalysis\n\nests <- bind_rows(giantest, wfest, popest, ukbest) %>%\n  mutate(cluster=NA)\nfor(i in 1:length(bmi_inst))\n{\n  ests$cluster[ests$rsid %in% bmi_inst[[i]]] <- i\n}\n\nests %>%\n  group_by(id, cluster) %>%\n  summarise(n=n(), psig = sum(p < 5e-3)/n) %>%\n  ggplot(., aes(x=as.factor(cluster), y=psig)) +\n  geom_bar(position=\"dodge\", stat=\"identity\", aes(fill=id))\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\n\n\n\n\nExpected vs observed replication rates\n\no <- lapply(1:length(bmi_inst), function(i)\n{\n  x <- bmi_inst[[i]]\n  dat <- inner_join(\n    subset(popest, rsid %in% x),\n    subset(wfest, rsid %in% x),\n    by=\"rsid\"\n  )\n  exp_rep(dat$beta.x, dat$beta.y, dat$se.x, dat$se.y, 1e-3)[[1]] %>%\n    mutate(cluster=i)\n})\no %>% bind_rows() %>%\n  ggplot(aes(x=as.factor(cluster), y=value/nsnp)) +\n  geom_point(aes(colour=datum)) +\n  facet_grid(. ~ metric) +\n  labs(x=\"Cluster\", y=\"Fraction of sig. instruments\", colour=\"\")"
  },
  {
    "objectID": "posts/2022-12-18-bmi-wf-replication/index.html#summary",
    "href": "posts/2022-12-18-bmi-wf-replication/index.html#summary",
    "title": "BMI instrument replication",
    "section": "Summary",
    "text": "Summary\nAll clusters appear to replicate as expected in the within-family GWAS, which is consistent with there being almost no shrinkage of the effect sizes."
  },
  {
    "objectID": "posts/2022-12-18-bmi-wf-replication/index.html#childhood-vs-adulthood-relationship-to-ses",
    "href": "posts/2022-12-18-bmi-wf-replication/index.html#childhood-vs-adulthood-relationship-to-ses",
    "title": "BMI instrument replication",
    "section": "Childhood vs adulthood relationship to SES",
    "text": "Childhood vs adulthood relationship to SES\n\nlibrary(TwoSampleMR)\n\nTwoSampleMR version 0.5.6 \n[>] New: Option to use non-European LD reference panels for clumping etc\n[>] Some studies temporarily quarantined to verify effect allele\n[>] See news(package='TwoSampleMR') and https://gwas.mrcieu.ac.uk for further details\n\n\n\nAttaching package: 'TwoSampleMR'\n\n\nThe following object is masked from 'package:ieugwasr':\n\n    ld_matrix\n\nchild <- make_dat(\"ukb-b-10011\", \"ebi-a-GCST90002409\")\n\nExtracting data for 18 SNP(s) from 1 GWAS(s)\n\n\nHarmonising Townsend deprivation index at recruitment || id:ukb-b-10011 (ukb-b-10011) and Childhood body mass index || id:ebi-a-GCST90002409 (ebi-a-GCST90002409)\n\nmr(child) %>% select(nsnp, b, se, pval)\n\nAnalysing 'ukb-b-10011' on 'ebi-a-GCST90002409'\n\n\n  nsnp           b        se      pval\n1   18  0.00605344 1.4172450 0.9966448\n2   18 -0.07468209 0.2308366 0.7462954\n3   18 -0.05801819 0.2184155 0.7905224\n4   18 -0.11644263 0.4977415 0.8178241\n5   18 -0.11644263 0.4702948 0.8074135\n\n\n\nadult <- make_dat(\"ukb-b-10011\", \"ukb-b-19953\")\n\nExtracting data for 18 SNP(s) from 1 GWAS(s)\n\n\nHarmonising Townsend deprivation index at recruitment || id:ukb-b-10011 (ukb-b-10011) and Body mass index (BMI) || id:ukb-b-19953 (ukb-b-19953)\n\nmr(adult) %>% select(nsnp, b, se, pval)\n\nAnalysing 'ukb-b-10011' on 'ukb-b-19953'\n\n\n  nsnp          b         se         pval\n1   18 -0.2812238 0.80612571 7.317440e-01\n2   18  0.3450191 0.07873753 1.176549e-05\n3   18  0.4835539 0.13327136 2.852487e-04\n4   18  0.3588983 0.15546210 3.380204e-02\n5   18  0.3128024 0.12623146 2.400304e-02\n\n\nReplication of clustered instruments from adult to child\n\nukbest <- ieugwasr::associations(unlist(bmi_inst), \"ukb-b-19953\")\nchildest <- ieugwasr::associations(unlist(bmi_inst), \"ebi-a-GCST90002409\")\n\n\no1 <- lapply(1:length(bmi_inst), function(i)\n{\n  x <- bmi_inst[[i]]\n  dat <- inner_join(\n    subset(ukbest, rsid %in% x),\n    subset(childest, rsid %in% x),\n    by=\"rsid\"\n  )\n  exp_rep(dat$beta.x, dat$beta.y, dat$se.x, dat$se.y, 1e-3)[[1]] %>%\n    mutate(cluster=i)\n})\no1 %>% bind_rows() %>%\n  ggplot(aes(x=as.factor(cluster), y=value/nsnp)) +\n  geom_point(aes(colour=datum)) +\n  facet_grid(. ~ metric) +\n  labs(x=\"Cluster\", y=\"Fraction of sig. instruments\", colour=\"\")\n\n\n\n\n\nSummary\n\nSES has an influence on BMI in adulthood but not childhood\nThe replication rate amongst clusters appears to be relatively consistent, except cluster 2 replicates particularly well"
  },
  {
    "objectID": "posts/2022-07-24-case-control-power/index.html",
    "href": "posts/2022-07-24-case-control-power/index.html",
    "title": "Power of GWAS in ascertained case control datasets",
    "section": "",
    "text": "Case control studies ascertain a fixed number of cases and controls. This changes the distribution of genetic liability in the selected sample - e.g. if the prevalence is low then the liability will be a truncated distribution for cases ascertained for the tail of the distribution, and truncated for the controls ascertained for a depletion of values in the tail (e.g. see here for illustrations https://pubmed.ncbi.nlm.nih.gov/21376301/).\nThe more rare the disease, the larger the variance of the liability when cases and controls are matched. This should improve statistical power because the cases and controls are ascertained to be more genetically distinct from each other.\nHowever, the Genetic Power Calculator concludes the opposite, as prevalence gets lower the power goes down (https://zzz.bwh.harvard.edu/gpc/cc2.html). e.g. for OR=1.1, ncase=1000, ncontrol=1000, af=0.5, for 80% power:\n\nprev = 0.001, power = 4e-5\nprev = 0.4, power = 0.71\n\nQuick simulation to investigate:\n\nlibrary(simulateGP)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nGenerate a function that will\n\ncreate a population with some genetic liability\nstochastically assign disease status based on heritability and prevalence\nascertain cases and controls\nidentify how many significant associations in the case/control sample\n\n\nsims <- function(ncase, ncontrol, nsnp, prev, hsq=0.5, thresh=5e-8)\n{\n  # Determine minimum sample size required to ascertain required number of cases and controls\n  n_req <- round(max(ncase / prev, ncontrol / prev) + 10000)\n  \n  # Generate matrix of genotype values\n  g <- make_geno(n_req, nsnp, 0.5)\n  \n  # Effect sizes for each SNP\n  b <- rnorm(nsnp)\n\n  dat <- tibble(\n    id = 1:n_req,\n    l = scale(g %*% b),               # genetic liability\n    p = gx_to_gp(l, hsq, prev),       # convert to disease probability\n    d = rbinom(n_req, 1, p)           # sample disease status from probability\n  )\n  \n  # Ascertain cases and controls \n  dat <- rbind(\n    subset(dat, d == 0)[1:ncase,],\n    subset(dat, d == 1)[1:ncontrol,]\n  )\n\n  # Perform GWAS\n  res <- gwas(dat$d, g[dat$id,], logistic=TRUE)\n  \n  # Count number of significant assocs\n  return(sum(res$pval < thresh))\n}\n\nRun a bunch of simulations\n\nparams <- expand.grid(\n  ncase = 1000,\n  ncontrol = 1000, \n  nsnp = c(2, 10, 100),\n  repeats = 1:10,\n  prev = seq(0.01, 0.3, by=0.01),\n  hsq=0.5,\n  thresh=5e-8\n) %>% select(-repeats)\nparams$nsig <- sapply(1:nrow(params), function(i) do.call(sims, params[i,]))\n\nPlot\n\nggplot(params, aes(x=prev, y=nsig/nsnp)) +\n  geom_point() +\n  facet_grid(nsnp ~ ., labeller=label_both, scale=\"free_y\") +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nSo power increases when prevalence is lower. Maybe this is related to polygenicity.\nSo what’s GPC doing differently to get the opposite result (lower prev = lower power)?"
  },
  {
    "objectID": "posts/2023-01-26-metabolite-power-calculation/index.html",
    "href": "posts/2023-01-26-metabolite-power-calculation/index.html",
    "title": "Metabolite power calculation",
    "section": "",
    "text": "Sample of individuals who have had heart surgery, followed up and some number go on to have kidney disease. What is the predictive rsq of a metabolite on kidney disease outcome, that has 80% power to be detected after multiple testing correction?"
  },
  {
    "objectID": "posts/2023-01-26-metabolite-power-calculation/index.html#simulation",
    "href": "posts/2023-01-26-metabolite-power-calculation/index.html#simulation",
    "title": "Metabolite power calculation",
    "section": "Simulation",
    "text": "Simulation\nCall libraries\n\nlibrary(fmsb)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nDefine model\n\nsim <- function(ncase, ncontrol, b)\n{\n  met <- rnorm(ncase+ncontrol)\n  y <- rbinom(ncase+ncontrol, 1, plogis(log(ncase/(ncase+ncontrol)) + met*b + rnorm(ncase+ncontrol)))\n  table(y)\n  mod <- glm(y ~ met)\n  rsq <- NagelkerkeR2(mod)$R2\n  pval <- summary(mod)$coef[2,4]\n  return(tibble(rsq, pval, ncase, ncontrol, b))\n}\n\nSet parameters\n\n# Parameters\nntest <- 1500\nncase <- 70\nncontrol <- 100\n\nRun\n\nparam <- expand.grid(\n  b = seq(0, 1.5, by=0.01),\n  sim = 1:100\n)\nres <- lapply(1:nrow(param), function(i) {\n  sim(ncase=ncase, ncontrol=ncontrol, b=param$b[i])\n}) %>% bind_rows()\n\nVisualise\n\nres %>% group_by(b) %>%\n  summarise(rsq=mean(rsq), psig = sum(pval < (0.05/ntest))/n()) %>%\n  ggplot(., aes(x=rsq, y=psig)) +\n  geom_point() +\n  geom_hline(yintercept=0.8) +\n  labs(x=\"Negelkerke R2\", y=\"Power\", title=paste0(ncase, \" cases, \", ncontrol, \" controls, \", ntest, \" independent tests\"))\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.0 dplyr_1.0.10  fmsb_0.7.5   \n\nloaded via a namespace (and not attached):\n [1] knitr_1.41        magrittr_2.0.3    munsell_0.5.0     tidyselect_1.2.0 \n [5] colorspace_2.0-3  R6_2.5.1          rlang_1.0.6       fastmap_1.1.0    \n [9] fansi_1.0.3       stringr_1.5.0     tools_4.2.1       grid_4.2.1       \n[13] gtable_0.3.1      xfun_0.36         utf8_1.2.2        DBI_1.1.3        \n[17] cli_3.5.0         withr_2.5.0       htmltools_0.5.4   assertthat_0.2.1 \n[21] yaml_2.3.6        digest_0.6.31     tibble_3.1.8      lifecycle_1.0.3  \n[25] farver_2.1.1      htmlwidgets_1.5.4 vctrs_0.5.1       glue_1.6.2       \n[29] evaluate_0.19     rmarkdown_2.16    labeling_0.4.2    stringi_1.7.8    \n[33] compiler_4.2.1    pillar_1.8.1      scales_1.2.1      generics_0.1.3   \n[37] jsonlite_1.8.4    pkgconfig_2.0.3"
  },
  {
    "objectID": "posts/2022-05-08-prs-vs-ivw/index.html",
    "href": "posts/2022-05-08-prs-vs-ivw/index.html",
    "title": "PRS vs IVW",
    "section": "",
    "text": "How does PRS compare to IVW fixed effects analysis"
  },
  {
    "objectID": "posts/2022-05-08-prs-vs-ivw/index.html#simulation-study",
    "href": "posts/2022-05-08-prs-vs-ivw/index.html#simulation-study",
    "title": "PRS vs IVW",
    "section": "Simulation study",
    "text": "Simulation study\n\nlibrary(simulateGP)\ngeno1 <- make_geno(10000, 500, 0.5)\nb <- choose_effects(500, 0.3)\nx1 <- make_phen(b, geno1)\ny1 <- make_phen(0.4, x1)\n\ngeno2 <- make_geno(1000, 500, 0.5)\nx2 <- make_phen(b, geno2)\ny2 <- make_phen(0.4, x2)\n\nbhat <- gwas(x1, geno1)\nb_unweighted <- sign(b)\n\n\nStandard unweighted PRS analysis\n\nprs_unweighted <- geno2 %*% b_unweighted\nsummary(lm(x2 ~ prs_unweighted))\n\n\nCall:\nlm(formula = x2 ~ prs_unweighted)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2356 -0.5547 -0.0330  0.6087  3.1932 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -0.321168   0.035444  -9.061   <2e-16 ***\nprs_unweighted  0.027292   0.001791  15.239   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9011 on 998 degrees of freedom\nMultiple R-squared:  0.1888,    Adjusted R-squared:  0.1879 \nF-statistic: 232.2 on 1 and 998 DF,  p-value: < 2.2e-16\n\n\n\n\nMeta analysing per-SNP PRS scores\n\nlibrary(meta)\n\nLoading 'meta' package (version 6.0-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\no <- sapply(1:ncol(geno2), function(i)\n{\n  prs_unweighted <- geno2[,i] * b_unweighted[i]\n  summary(lm(x2 ~ prs_unweighted))$coef[2,1:2]\n})\nmetafor::rma(yi=o[1,], sei=o[2,], method=\"EE\")\n\n\nEqual-Effects Model (k = 500)\n\nI^2 (total heterogeneity / total variability):   17.58%\nH^2 (total variability / sampling variability):  1.21\n\nTest for Heterogeneity:\nQ(df = 499) = 605.4622, p-val = 0.0007\n\nModel Results:\n\nestimate      se     zval    pval   ci.lb   ci.ub      \n  0.0277  0.0020  13.8615  <.0001  0.0238  0.0316  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nStandard errors with number of SNPs\n\nses_unweighted <- sapply(1:ncol(geno2), function(i)\n{\n  prs_unweighted <- geno2[,1:i, drop=FALSE] %*% b_unweighted[1:i]\n  summary(lm(x2 ~ prs_unweighted))$coef[2,2]\n})\nplot(ses_unweighted)"
  },
  {
    "objectID": "posts/2022-10-25-regression-non-iid/index.html",
    "href": "posts/2022-10-25-regression-non-iid/index.html",
    "title": "Regression with non i.i.d. samples",
    "section": "",
    "text": "If the individuals in my dataset are correlated with known correlation structure, how can I perform regression whilst accounting for that correlation structure?\n\\[\n\\beta = (X^T \\rho^{-1} X)^{-1} X^T\\rho^{-1}Y\n\\]\nThe variance of the estimate will be\n\\[\n\\begin{aligned}\nVar(\\beta) &= \\sigma^2(X^T \\rho^{-1} X) \\\\\n&= \\frac{(\\hat{e}^T \\rho^{-1} \\hat{e})(X^T \\rho^{-1} X)}{n-r}\n\\end{aligned}\n\\]\nAdding to the complication, in this case it is the h2 estimates of a set of traits being correlated against number of GWAS hits for a set of traits, where the traits are in some way correlated. So the estimates of e.g. h2 are estimated with sampling error, so to account for that perhaps need to parametric bootstrap.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(mvtnorm)\n\n#' Regression with samples that are not independent\n#'\n#' For our analysis we're regressing estimates against estimates,\n#' and so it's a bit more complicated because the estimates each have an SE.\n#' For now let's just ignore that but I think if we don't account for it we\n#' will get a bit of regression dilution bias.\n#'\n#' @param x Vector of x values\n#' @param y Vector of y values\n#' @param rho correlation matrix\n#' @param se_x SE of x evalues (Ignore)\n#' @param se_y SE of y values (Ignore)\n#' @param nboot Number of bootstraps to get standard error (Ignore)\n#'\n#' @return\n#' @export\nreg_nonind <- function(x, y, rho, se_x=NULL, se_y=NULL, nboot=NULL)\n{\n  X <- cbind(rep(1, length(x)), x)\n  rho_inv <- solve(rho)\n  #beta <- solve(t(X) %*% rho_inv %*% X) %*% t(X) %*% rho_inv %*% y\n  beta <- ginv(t(X)%*%rho_inv %*%X)%*%t(X)%*%rho_inv %*%y\n  yhat <- X %*% beta\n  yres <- as.numeric(y - yhat)\n  se <- as.numeric((t(yres) %*% rho_inv %*% yres) / (length(x)-qr(X)$rank)) * solve(t(X) %*% rho_inv %*% X)\n  se <- sqrt(diag(se))\n  return(tibble(\n    param=c(\"intercept\", \"slope\"), beta=beta, se=se, pval=pnorm(abs(beta)/se, lower.tail=FALSE)\n  ))\n  \n  # get standard error via parametric bootstrap\n  # betaboot <- matrix(0, nboot, 2)\n  # for(i in 1:nboot)\n  # {\n  #   X[,2] <- rnorm(length(x), mean=x, sd=se_x)\n  #   Y <- rnorm(length(y), mean=y, sd=se_y)\n  #   betaboot[i,] <- solve(t(X) %*% rho_inv %*% X) %*% t(X) %*% rho_inv %*% Y %>% as.numeric()\n  # }\n  # se_boot <- apply(betaboot, 2, sd)\n}\n\nx <- runif(300)\ny <- runif(300)\nrho <- diag(300)\nrho[lower.tri(rho)] <- rnorm(sum(lower.tri(rho)), sd=0.01)\nrho[upper.tri(rho)] <- t(rho)[upper.tri(rho)]\n\nreg_nonind(x, y, rho, 10)\n\n# A tibble: 2 × 4\n  param     beta[,1]     se pval[,1]\n  <chr>        <dbl>  <dbl>    <dbl>\n1 intercept   0.507  0.0332 8.42e-53\n2 slope      -0.0522 0.0579 1.84e- 1\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50453 -0.23397 -0.01499  0.22869  0.51654 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.51135    0.03319  15.407   <2e-16 ***\nx           -0.05653    0.05748  -0.983    0.326    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2824 on 298 degrees of freedom\nMultiple R-squared:  0.003235,  Adjusted R-squared:  -0.0001103 \nF-statistic: 0.967 on 1 and 298 DF,  p-value: 0.3262"
  },
  {
    "objectID": "posts/2023-01-26-convert-lbf-to-summary-stats/index.html",
    "href": "posts/2023-01-26-convert-lbf-to-summary-stats/index.html",
    "title": "Convert Bayes factors to beta and standard error",
    "section": "",
    "text": "Is it possible to convert BF to beta and standard error? According to Giambartolomei et al 2014 -\n\\[\nABF = \\sqrt{1-r} \\times exp(rZ^2/2)\n\\]\nso\n\\[\n|Z| = \\sqrt{\\frac{2 * log(ABF) - log(\\sqrt{1-r})}{r}}\n\\]\nhere \\(r = W / V\\) where V is the variance of the SNP effect estimate\n\\[\nV \\approx \\frac{1}{2np(1-p)}\n\\]\nwhere n is sample size and p is allele frequency (assumes small amount of variance explained in trait and sd of trait is 1).\nRun simulation\n\nUse regional LD matrix to generate summary statistics with a single causal variant\nUse SuSiE to perform fine mapping\nConvert SuSiE Bayes Factors into Z scores, betas, standard errors\nCompare converted Z, beta, se against original simulated Z, beta, SE"
  },
  {
    "objectID": "posts/2023-01-26-convert-lbf-to-summary-stats/index.html#simulation",
    "href": "posts/2023-01-26-convert-lbf-to-summary-stats/index.html#simulation",
    "title": "Convert Bayes factors to beta and standard error",
    "section": "Simulation",
    "text": "Simulation\nLibraries\n\nlibrary(simulateGP)\nlibrary(susieR)\nlibrary(here)\n\nhere() starts at /Users/gh13047/repo/lab-book\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nConversion function for logBF to z, beta, se\n\n#' Convert log Bayes Factor to summary stats\n#'\n#' @param lbf p-vector of log Bayes Factors for each SNP\n#' @param n Overall sample size\n#' @param af p-vector of allele frequencies for each SNP\n#' @param prior_v Variance of prior distribution. SuSiE uses 50\n#'\n#' @return tibble with lbf, af, beta, se, z\nlbf_to_z_cont <- function(lbf, n, af, prior_v=50)\n{\n  se = sqrt(1 / (2 * n * af * (1-af)))\n  r = prior_v / (prior_v + se^2)\n  z = sqrt((2 * lbf - log(sqrt(1-r)))/r)\n  beta <- z * se\n  return(tibble(lbf, af, z, beta, se))\n}\n\nRead in example LD matrix from simulateGP repository\n\nmap <- readRDS(url(\"https://github.com/explodecomputer/simulateGP/raw/master/data/ldobj_5_141345062_141478055.rds\", \"rb\"))\nglimpse(map)\n\nList of 3\n $ ld  : num [1:501, 1:501] 1 0.565 0.566 0.565 0.565 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:501] \"V2000\" \"V2001\" \"V2002\" \"V2003\" ...\n $ map : tibble [501 × 6] (S3: tbl_df/tbl/data.frame)\n  ..$ chr: int [1:501] 5 5 5 5 5 5 5 5 5 5 ...\n  ..$ snp: chr [1:501] \"rs252141\" \"rs252140\" \"rs252139\" \"rs187544\" ...\n  ..$ pos: int [1:501] 141345062 141345192 141345218 141345361 141345678 141345805 141346830 141347360 141347465 141347931 ...\n  ..$ alt: chr [1:501] \"T\" \"T\" \"C\" \"G\" ...\n  ..$ ref: chr [1:501] \"C\" \"C\" \"T\" \"T\" ...\n  ..$ af : num [1:501] 0.627 0.831 0.83 0.831 0.831 ...\n  ..- attr(*, \".internal.selfref\")=<externalptr> \n $ nref: num 503\n\n\nGenerate summary statistics for a single causal variant and\n\nset.seed(1234)\nss <- map$map %>%\n    generate_gwas_params(h2=0.003, Pi=1/nrow(.)) %>%\n    generate_gwas_ss(50000, ld=map$ld)\ntable(ss$beta == 0)\n\n\nFALSE  TRUE \n    1   500 \n\n\n\nplot(-log10(pval) ~ pos, ss)\n\n\n\n\nRun SuSiE\n\nsout <- susie_rss(ss$bhat / ss$se, R = map$ld, n = 50000, bhat = ss$bhat, var_y=1)\n\nWARNING: XtX is not symmetric; forcing XtX to be symmetric by replacing XtX with (XtX + t(XtX))/2\n\nsummary(sout)\n\n\nVariables in credible sets:\n\n variable variable_prob cs\n      286     0.1604616  1\n      306     0.1604616  1\n      291     0.1604616  1\n      300     0.1604616  1\n      274     0.1604616  1\n      284     0.1604616  1\n\nCredible sets summary:\n\n cs cs_log10bf cs_avg_r2 cs_min_r2                variable\n  1   30.37357         1         1 274,284,286,291,300,306\n\n\n\nglimpse(sout)\n\nList of 18\n $ alpha                 : num [1:10, 1:501] 6.9e-35 2.0e-03 2.0e-03 2.0e-03 2.0e-03 ...\n $ mu                    : num [1:10, 1:501] 0.000761 0 0 0 0 ...\n $ mu2                   : num [1:10, 1:501] 2.04e-05 0.00 0.00 0.00 0.00 ...\n $ KL                    : num [1:10] 6.75 -1.24e-14 -1.24e-14 -1.24e-14 -1.24e-14 ...\n $ lbf                   : num [1:10] 6.99e+01 1.24e-14 1.24e-14 1.24e-14 1.24e-14 ...\n $ lbf_variable          : num [1:10, 1:501] -2.51 0 0 0 0 ...\n $ sigma2                : num 1\n $ V                     : num [1:10] 0.00307 0 0 0 0 ...\n $ pi                    : num [1:501] 0.002 0.002 0.002 0.002 0.002 ...\n $ null_index            : num 0\n $ XtXr                  : num [1:501, 1] -0.328 70.558 72.085 70.558 70.558 ...\n $ converged             : logi TRUE\n $ elbo                  : num [1:2] -70876 -70876\n $ niter                 : int 2\n $ X_column_scale_factors: num [1:501] 1 1 1 1 1 1 1 1 1 1 ...\n $ intercept             : num NA\n $ sets                  :List of 5\n  ..$ cs                :List of 1\n  .. ..$ L1: int [1:6] 274 284 286 291 300 306\n  ..$ purity            :'data.frame':  1 obs. of  3 variables:\n  .. ..$ min.abs.corr   : num 1\n  .. ..$ mean.abs.corr  : num 1\n  .. ..$ median.abs.corr: num 1\n  ..$ cs_index          : int 1\n  ..$ coverage          : num 0.963\n  ..$ requested_coverage: num 0.95\n $ pip                   : num [1:501] 0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"class\")= chr \"susie\"\n\n\nGet Z scores from lbf\n\na <- lbf_to_z_cont(sout$lbf_variable[1,], 50000, ss$af, prior_v = 50)\na\n\n# A tibble: 501 × 5\n     lbf     af     z    beta      se\n   <dbl>  <dbl> <dbl>   <dbl>   <dbl>\n 1 -2.51 0.373   1.41 0.00919 0.00654\n 2 -2.43 0.169   1.37 0.0115  0.00844\n 3 -2.37 0.17    1.42 0.0119  0.00842\n 4 -2.43 0.169   1.37 0.0115  0.00844\n 5 -2.43 0.169   1.37 0.0115  0.00844\n 6 -2.52 0.191   1.32 0.0106  0.00805\n 7 -2.43 0.169   1.37 0.0115  0.00844\n 8 -2.44 0.17    1.36 0.0115  0.00842\n 9  2.23 0.0139  3.17 0.0855  0.0270 \n10 -2.43 0.169   1.37 0.0115  0.00844\n# … with 491 more rows\n\n\nRelationship between lbf and re-estimated z\n\nplot(z ~ lbf, a)\n\n\n\n\nNew Z vs original Z\n\nplot(a$z^2 ~ ss$fval)\n\n\n\n\n\nlm(a$z^2 ~ ss$fval)\n\n\nCall:\nlm(formula = a$z^2 ~ ss$fval)\n\nCoefficients:\n(Intercept)      ss$fval  \n     1.5141       0.9834  \n\n\nNew beta vs original beta\n\nplot(a$beta ~ ss$bhat)\n\n\n\n\nTwo causal variants\nSet two causal variants at either end of the region\n\nset.seed(12)\nparam <- map$map\nparam$beta <- 0\nparam$beta[c(10, 490)] <- 0.3\nss <- generate_gwas_ss(param, 50000, ld=map$ld)\nplot(-log10(pval) ~ pos, ss)\n\n\n\n\nFirst variant\n\nsout <- susie_rss(ss$bhat / ss$se, R = map$ld, n = 50000, bhat = ss$bhat, var_y=1)\n\nWARNING: XtX is not symmetric; forcing XtX to be symmetric by replacing XtX with (XtX + t(XtX))/2\n\na1 <- lbf_to_z_cont(sout$lbf_variable[1,], 50000, ss$af, prior_v = 50)\n\nWarning in sqrt((2 * lbf - log(sqrt(1 - r)))/r): NaNs produced\n\nplot(a1$beta ~ ss$bhat)\n\n\n\n\n\na2 <- lbf_to_z_cont(sout$lbf_variable[2,], 50000, ss$af, prior_v = 50)\nplot(a2$beta ~ ss$bhat)\n\n\n\n\nThis looks good - it’s setting different values to 0 in the two lbf vectors that correspond to two causal variants\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dplyr_1.0.10     here_1.0.1       susieR_0.12.27   simulateGP_0.1.2\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9         plyr_1.8.7         compiler_4.2.1     pillar_1.8.1      \n [5] tools_4.2.1        digest_0.6.31      jsonlite_1.8.4     evaluate_0.19     \n [9] lifecycle_1.0.3    tibble_3.1.8       gtable_0.3.1       lattice_0.20-45   \n[13] pkgconfig_2.0.3    rlang_1.0.6        Matrix_1.4-1       DBI_1.1.3         \n[17] cli_3.5.0          yaml_2.3.6         xfun_0.36          fastmap_1.1.0     \n[21] stringr_1.5.0      knitr_1.41         generics_0.1.3     vctrs_0.5.1       \n[25] htmlwidgets_1.5.4  rprojroot_2.0.3    tidyselect_1.2.0   grid_4.2.1        \n[29] reshape_0.8.9      glue_1.6.2         R6_2.5.1           fansi_1.0.3       \n[33] rmarkdown_2.16     mixsqp_0.3-48      irlba_2.3.5.1      ggplot2_3.4.0     \n[37] magrittr_2.0.3     MASS_7.3-58.1      matrixStats_0.63.0 scales_1.2.1      \n[41] htmltools_0.5.4    assertthat_0.2.1   colorspace_2.0-3   utf8_1.2.2        \n[45] stringi_1.7.8      munsell_0.5.0      crayon_1.5.2"
  },
  {
    "objectID": "posts/2022-12-16-vqtl/index.html",
    "href": "posts/2022-12-16-vqtl/index.html",
    "title": "Inflation of vQTLs",
    "section": "",
    "text": "This paper describes how incomplete linkage disequilibrium can lead to inflated test statistics for interactions - https://www.nature.com/articles/s41586-021-03765-z. Because interaction terms contribute to variance heterogeneity across genotype classes, this could also inflate vQTL detection methods.\nExample model\nSuppose a system with three variants and one trait. The trait $x$ is influenced by a single additive causal variant $y_1$. But there is another variant in LD with this causal variant $y_2$. Finally, a third variant is independent of all other variables (think of that as a trans SNP). So\n\\[\nx_i = y_{1,i} + e_i\n\\]\nBut we test for an interaction between y_2 and y_3.\nRun some simulations…\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nset.seed(12345)\n\ntest_drm <- function(g, y)\n{\n  y.i <- tapply(y, g, median, na.rm=T)  \n  z.ij <- abs(y - y.i[g+1])\n  summary(lm(z.ij ~ g))$coef %>%\n    as_tibble() %>%\n    slice(2) %>%\n    mutate(method=\"drm\")\n}\n\ncorrelated_binomial <- function (nid, p1, p2, rho, n = 2, round = TRUE, print = FALSE) \n{\n    p <- p1\n    q <- p2\n    a <- function(rho, p, q) {\n        rho * sqrt(p * q * (1 - p) * (1 - q)) + (1 - p) * (1 - \n            q)\n    }\n    a.0 <- a(rho, p, q)\n    prob <- c(`(0,0)` = a.0, `(1,0)` = 1 - q - a.0, `(0,1)` = 1 - \n        p - a.0, `(1,1)` = a.0 + p + q - 1)\n    if (min(prob) < 0) {\n        print(prob)\n        stop(\"Error: a probability is negative.\")\n    }\n    n.sim <- nid\n    u <- sample.int(4, n.sim * n, replace = TRUE, prob = prob)\n    y <- floor((u - 1)/2)\n    x <- 1 - u%%2\n    x <- colSums(matrix(x, nrow = n))\n    y <- colSums(matrix(y, nrow = n))\n    if (round) {\n        x <- round(x)\n        y <- round(y)\n    }\n    if (print) {\n        print(table(x, y))\n        print(stats::cor(x, y))\n    }\n    return(cbind(x, y))\n}\n\ngendatp <- function(n, p1, p2, p3, r1)\n{\n    dat <- correlated_binomial(n, p1, p2, r1) %>% as_tibble()\n    names(dat) <- c(\"y1\", \"y2\")\n    dat$y3 <- rbinom(n, 1, p3)\n    return(dat)\n}\n\nrun_simp <- function(param, i)\n{\n    set.seed(i*10)\n    dat <- gendatp(param$n[i], param$p1[i], param$p2[i], param$p3[i], param$r1[i])\n    x <- dat$y1 + rnorm(nrow(dat), sd=sd(dat$y1)/4)\n    mod1 <- lm(x ~ y2 + y3, dat)\n    mod2 <- lm(x ~ y2 + y3 + y2*y3, dat)\n    amod <- anova(mod1, mod2)\n    param$F[i] <- amod$P[2]\n    o1 <- test_drm(dat$y1, x)\n    o2 <- test_drm(dat$y2, x)\n    o3 <- test_drm(dat$y3, x)\n    param$drm1[i] <- o1$`Pr(>|t|)`\n    param$drm2[i] <- o2$`Pr(>|t|)`\n    param$drm3[i] <- o3$`Pr(>|t|)`\n    return(param[i,])\n}\n\nparam <- expand.grid(\n    p1=0.1,\n    p2=0.1,\n    p3=0.5,\n    p4=0.1,\n    n=1000,\n    r1=seq(0, 1, by=0.2),\n    sim=1:500,\n    r2=NA,\n    F=NA,\n    drm1=NA,\n    drm2=NA,\n    drm3=NA\n)\n\nresp <- lapply(1:nrow(param), function(x) run_simp(param, x)) %>% bind_rows()\nstr(resp)\n\n'data.frame':   3000 obs. of  12 variables:\n $ p1  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p2  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p3  : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n $ p4  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ n   : num  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ...\n $ r1  : num  0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 ...\n $ sim : int  1 1 1 1 1 1 2 2 2 2 ...\n $ r2  : logi  NA NA NA NA NA NA ...\n $ F   : num  0.4136 0.0371 0.0567 0.5604 0.0364 ...\n $ drm1: num  0.288 0.376 0.16 0.215 0.998 ...\n $ drm2: num  1.64e-02 3.88e-17 1.58e-25 2.64e-26 9.41e-19 ...\n $ drm3: num  0.9178 0.1475 0.0181 0.1416 0.7348 ...\n - attr(*, \"out.attrs\")=List of 2\n  ..$ dim     : Named int [1:12] 1 1 1 1 1 6 500 1 1 1 ...\n  .. ..- attr(*, \"names\")= chr [1:12] \"p1\" \"p2\" \"p3\" \"p4\" ...\n  ..$ dimnames:List of 12\n  .. ..$ p1  : chr \"p1=0.1\"\n  .. ..$ p2  : chr \"p2=0.1\"\n  .. ..$ p3  : chr \"p3=0.5\"\n  .. ..$ p4  : chr \"p4=0.1\"\n  .. ..$ n   : chr \"n=1000\"\n  .. ..$ r1  : chr [1:6] \"r1=0.0\" \"r1=0.2\" \"r1=0.4\" \"r1=0.6\" ...\n  .. ..$ sim : chr [1:500] \"sim=  1\" \"sim=  2\" \"sim=  3\" \"sim=  4\" ...\n  .. ..$ r2  : chr \"r2=NA\"\n  .. ..$ F   : chr \"F=NA\"\n  .. ..$ drm1: chr \"drm1=NA\"\n  .. ..$ drm2: chr \"drm2=NA\"\n  .. ..$ drm3: chr \"drm3=NA\""
  },
  {
    "objectID": "posts/2022-12-16-vqtl/index.html#type-1-error-of-vqtls",
    "href": "posts/2022-12-16-vqtl/index.html#type-1-error-of-vqtls",
    "title": "Inflation of vQTLs",
    "section": "Type 1 error of vQTLs",
    "text": "Type 1 error of vQTLs\nThis is what happens to the genetic interaction between y_2 and y_3 - remember that neither of these have a causal effect, and there is no interaction term, however y_2 is correlated with the causal variant y_1\n\nggplot(resp, aes(x=as.factor(r1), y=-log10(F))) +\ngeom_boxplot() +\ngeom_hline(yintercept=-log10(0.05/nrow(resp))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"Interaction -log10 p for y2xy3\", x=\"LD between tagging\\nvariant and causal variant\")\n\n\n\n\nSo you get some false positives even after bonferroni correction. However now look at what happens to the variance QTL estimate for y_2 (the SNP that has no interaction but is in incomplete LD with the additive SNP y_1). Here we’ll use the DRM method to test for vQTL effects at y_2\n\nggplot(resp, aes(x=r1, y=-log10(drm2))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\")\n\n\n\n\nThis is really extreme type 1 sensitivity to incomplete LD. There’s no problem at the actual causal locus (y_1)\n\nggplot(resp, aes(x=r1, y=-log10(drm1))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\")\n\n\n\n\nOr at the unlinked locus y_3\n\nggplot(resp, aes(x=r1, y=-log10(drm3))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\")\n\n\n\n\nImplications - performing an exhaustive search is going to give quite problematic results if the main effects aren’t controlled. So you’d really have to know what all the main effects are before performing the vQTL tests in order to control for them. Note that incomplete control of the main effects is inevitable and we should be anticipating elevated type 1 error rates for any SNPs that are in the region of any large main effects."
  },
  {
    "objectID": "posts/2022-12-16-vqtl/index.html#power-issues-when-controlling-for-main-effects",
    "href": "posts/2022-12-16-vqtl/index.html#power-issues-when-controlling-for-main-effects",
    "title": "Inflation of vQTLs",
    "section": "Power issues when controlling for main effects",
    "text": "Power issues when controlling for main effects\nThe other problem is actually controlling for main effects. Suppose that a probe has two distal SNPs that interact e.g.\n\nn <- 10000\ng1 <- rbinom(n, 2, 0.4)\ng2 <- rbinom(n, 2, 0.4)\ny <- g1 + g2 + g1 * g2 + rnorm(n, sd=1.5)\ntest_drm(g1, y) %>% str\n\ntibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n $ Estimate  : num 0.362\n $ Std. Error: num 0.0173\n $ t value   : num 20.9\n $ Pr(>|t|)  : num 3.16e-95\n $ method    : chr \"drm\"\n\n\nThe DRM method finds a big vQTL effect here because of the GxG interaction - so it’s detecting that g1 might be interacting with something.\nIf we adjust for the main effects of g1 and g2 now look at DRM\n\nyres <- residuals(lm(y ~ g1 + g2))\ntest_drm(g1, yres) %>% str\n\ntibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n $ Estimate  : num 0.0194\n $ Std. Error: num 0.0138\n $ t value   : num 1.4\n $ Pr(>|t|)  : num 0.161\n $ method    : chr \"drm\"\n\n\nThe test statistic for the interaction test has massively attenuated.\nWhere does this leave us?\n\nIf a SNP is a known additive causal variant then it is relatively safe from type 1 error\nIf a SNP is not a known additive causal variant, then it is susceptible to type 1 error due to incomplete LD with actual additive causal variants\nIf we adjust the probe for additive causal variants before testing the SNP, we risk drastically reducing the vQTL effect that arise due to GxG interactions\nNote that this applies to GxE for when adjusting for other covariates too - e.g. if we adjust probes for smoking, age, sex, cell type etc and we are trying to find interactions with those based on vQTLs then the power to identify those vQTL effects drastically reduces"
  },
  {
    "objectID": "posts/2022-12-16-vqtl/index.html#power-of-vqtl-vs-interaction",
    "href": "posts/2022-12-16-vqtl/index.html#power-of-vqtl-vs-interaction",
    "title": "Inflation of vQTLs",
    "section": "Power of vQTL vs interaction",
    "text": "Power of vQTL vs interaction\nSuppose we simulate a GxE interaction. We can try to detect it either using a vQTL method (e.g. DRM) or using a direct interaction test.\n\nsim_gxe <- function(n, p, bi)\n{\n  params <- environment() %>% as.list() %>% as_tibble()\n  g <- rbinom(n, 2, p)\n  e <- rnorm(n)\n  y <- g + bi * g*e + e + rnorm(n)\n  \n  bind_rows(\n    test_drm(g, y),\n    summary(lm(y ~ g*e))$coef %>%\n      as_tibble() %>%\n      slice(n=4) %>%\n      mutate(method=\"interaction\")\n  ) %>%\n    bind_cols(., params)\n}\n\nparam <- expand.grid(\n  n=1000,\n  bi=seq(0,1,by=0.01),\n  nsim=10,\n  p=0.5\n) %>% select(-nsim)\nres <- lapply(1:nrow(param), function(i) do.call(sim_gxe, param[i,])) %>% bind_rows()\nres %>%\n  ggplot(., aes(x=bi, y=-log10(`Pr(>|t|)`))) +\n  geom_point(aes(colour=method))\n\n\n\n\nThe direct interaction test seems much better powered to detect these associations."
  },
  {
    "objectID": "posts/2023-01-30-conditional-f-winners-curse/index.html",
    "href": "posts/2023-01-30-conditional-f-winners-curse/index.html",
    "title": "Can winner’s curse generate a high conditional F statistic",
    "section": "",
    "text": "Multivariable MR requires that exposure effects are heterogeneous, indicated by the conditional F-statistic. If the sample overlap between two exposures is 0, how much can winner’s curse drive apparent heterogeneity?\n\nlibrary(simulateGP)\nlibrary(dplyr)\nlibrary(MVMR)\nlibrary(ggplot2)\n\n# 1. Simulate architecture for nsnps\n# 2. Simualate two independent GWAS summary datasets each with sample size nid\n# 3. Identify instruments significant in each dataset\n# 4. Calculate conditional Fstat for these independent instruments\n# 5. Repeat for a random selection of instruments (no selection)\nsim <- function(nsnp, nid)\n{\n    map <- tibble(snp=1:nsnp, af=runif(nsnp, 0.01, 0.99)) %>%\n        generate_gwas_params(h2=0.4, S=-0.1, Pi=1)\n    map\n\n    ss1 <- generate_gwas_ss(map, nid)\n    ss2 <- generate_gwas_ss(map, nid)\n\n    table(ss1$pval < 5e-8, ss2$pval < 5e-8)\n\n    inst1 <- subset(ss1, pval < 5e-8)$snp\n    inst2 <- subset(ss2, pval < 5e-8)$snp\n    insts <- unique(c(inst1, inst2))\n\n    mvmrdat <- format_mvmr(\n        BXGs = cbind(ss1$bhat[insts], ss2$bhat[insts]), \n        BYG = runif(length(insts)), \n        seBXGs = cbind(ss1$se[insts], ss2$se[insts]),\n        seBYG = rep(0.1, length(insts)),\n        RSID = insts\n    )\n    selected <- strength_mvmr(mvmrdat, 0)\n\n    insts_random <- sample(map$snp, length(insts), replace=FALSE)\n    mvmrdat_random <- format_mvmr(\n        BXGs = cbind(ss1$bhat[insts_random], ss2$bhat[insts_random]), \n        BYG = runif(length(insts_random)), \n        seBXGs = cbind(ss1$se[insts_random], ss2$se[insts_random]),\n        seBYG = rep(0.1, length(insts_random)),\n        RSID = insts\n    )    \n    random <- strength_mvmr(mvmrdat_random, 0)\n    return(list(selected=selected, random=random, ninst=length(insts)))\n}\n\n# Simulation parameters\nparam <- expand.grid(\n    nsnp=seq(5000, 100000, by=5000), \n    nid=240000,\n    nsim=1:5\n)\n\n# Run simulations\no <- lapply(1:nrow(param), function(i)\n{\n    x <- param[i,]\n    o <- sim(x$nsnp, x$nid)\n    bind_cols(\n        x, \n        tibble(\n            what=c(\"selected\", \"random\"), \n            Fstat=c(o$selected$exposure1[1], o$random$exposure1[1]), \n            ninst=o$ninst\n        )\n    )\n}) %>% bind_rows()\n\n\n# Plot\nggplot(o, aes(x=as.factor(nsnp), y=Fstat)) +\ngeom_boxplot(aes(fill=what)) +\nlabs(x=\"Number of causal variants\", y=\"Conditional F-statistic\", fill=\"\") +\ntheme(axis.text.x=element_text(angle=90))\n\n\n\n\nSo, it looks like winner’s curse alone can generate quite large conditional F stat ~4 under some circumstances - that the number of causal variants is very high like 60k with n=240000 in each GWAS (which essentially means that most of the GWAS hits are hovering around the significance threshold, which is where winner’s curse is maximised). The empirical analysis in ukbb will be useful to get a more concrete answer on how much it’s realistically contributing\n\n\nsessionInfo()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes",
    "section": "",
    "text": "Cell-specific effects for mQTLs from bulk tissue\n\n\n\n\n\n\n\nDNA methylation\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\nCan winner's curse generate a high conditional F statistic\n\n\n\nstatistics\nwinner's curse\nmultivariable mr\n\n\n\n\n\n\n\nJan 30, 2023\nGibran Hemani\n\n\n\n\n\n\nMetabolite power calculation\n\n\n\n\n\n\n\npower calculation\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nGibran Hemani and Nic Timpson\n\n\n\n\n\n\n\n\nConvert Bayes factors to beta and standard error\n\n\n\n\n\n\n\nstatistics\n\n\nfine mapping\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\nBMI instrument replication\n\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\nInflation of vQTLs\n\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\nSibling replication of Backman rare variants\n\n\n\n\n\n\n\nrare variants\n\n\ngenetics\n\n\nfamily studies\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\nProbability of a random variable being larger than all other random variables in a multivariate normal vector\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\nRegression with non i.i.d. samples\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\nCorrelated SNPs\n\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\nPower of GWAS in ascertained case control datasets\n\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\nPRS vs IVW\n\n\n\n\n\n\n\nstatistics\n\n\nMendelian randomization\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\nDifference in difference models in observational data are still potentially confounded\n\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I use this place to keep track of miscellaneous code and dynamic documents. It’s always rough work, unpolished, potentially full of errors."
  },
  {
    "objectID": "posts/2023-02-07-mqtl-interaction/index.html",
    "href": "posts/2023-02-07-mqtl-interaction/index.html",
    "title": "Cell-specific effects for mQTLs from bulk tissue",
    "section": "",
    "text": "What is the data generating model for the mQTL x celltype interaction analysis?\nDoes this rescue the per-celltype mQTL effects?"
  },
  {
    "objectID": "posts/2023-02-07-mqtl-interaction/index.html#basic-simulation",
    "href": "posts/2023-02-07-mqtl-interaction/index.html#basic-simulation",
    "title": "Cell-specific effects for mQTLs from bulk tissue",
    "section": "Basic simulation",
    "text": "Basic simulation\n\nFive cell types\n10k individuals\nDifferent SNP effect on methylation in each cell type\nEach individual has a different cell type proportion\nBulk tissue is the weighted average of all the cell types (weighted by cell type proportion in the individual)\nCan we recapitulate the cell-type specific effect through the interaction term?\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nsim <- function(nc, n)\n{\n    g <- rbinom(n, 2, 0.4)\n    betas <- runif(nc, -2, 2)\n    m <- sapply(1:nc, function(i)\n    {\n        g * betas[i] + rnorm(n)\n    })\n    # for each individual sample cell type proportions\n    cellprop <- sapply(1:n, function(x) {a <- runif(nc); a/sum(a)}) %>% t()\n    # weighted sum\n    M <- (scale(m) * cellprop) %>% rowSums\n    res <- sapply(1:nc, function(i)\n    {\n      summary(lm(M ~ g * cellprop[,i]))$coef[4,1]\n    })\n    return(tibble(res, betas))\n}\n\no <- lapply(1:1000, function(i) sim(5, 10000) %>% mutate(sim=i)) %>% bind_rows()\no\n\n# A tibble: 5,000 × 3\n       res   betas   sim\n     <dbl>   <dbl> <int>\n 1 -0.820  -1.59       1\n 2  0.358  -0.0905     1\n 3  0.0370 -0.481      1\n 4 -0.841  -1.62       1\n 5  1.24    0.459      1\n 6  1.70    1.34       2\n 7 -0.870  -1.35       2\n 8 -0.902  -1.39       2\n 9  0.942   0.500      2\n10 -0.788  -1.45       2\n# … with 4,990 more rows\n\n\n\nggplot(o, aes(x=betas, y=res)) +\ngeom_point() +\ngeom_abline(colour=\"red\") +\ngeom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nGenerally seems to work but expect some shrinkage of large effects"
  },
  {
    "objectID": "posts/2023-02-07-mqtl-interaction/index.html#introduce-measurement-error-in-cell-type-proportions",
    "href": "posts/2023-02-07-mqtl-interaction/index.html#introduce-measurement-error-in-cell-type-proportions",
    "title": "Cell-specific effects for mQTLs from bulk tissue",
    "section": "Introduce measurement error in cell-type proportions",
    "text": "Introduce measurement error in cell-type proportions\n\ncellprop_noise <- function(cellprop, sigma)\n{\n    apply(cellprop, 1, function(x)\n    {\n        a <- rnorm(length(x), x, sigma)\n        a / sum(a)\n    }) %>% t()\n}\n\n\nsim2 <- function(nc, n, noise_sigma)\n{\n    g <- rbinom(n, 2, 0.4)\n    betas <- runif(nc, -2, 2)\n    m <- sapply(1:nc, function(i)\n    {\n        g * betas[i] + rnorm(n)\n    })\n    # for each individual sample cell type proportions\n    cellprop <- sapply(1:n, function(x) {a <- runif(nc); a/sum(a)}) %>% t()\n    cpn <- cellprop_noise(cellprop, noise_sigma)\n    # weighted sum\n    M <- (scale(m) * cellprop) %>% rowSums\n    res <- sapply(1:nc, function(i)\n    {\n      summary(lm(M ~ g * cpn[,i]))$coef[4,1]\n    })\n    return(tibble(res, betas))\n}\n\no2 <- lapply(1:1000, function(i) {\n    s <- sample(c(0, 0.05, 0.1), 1)\n    sim2(5, 10000, s) %>% mutate(sim=i, s=s)\n}) %>% bind_rows()\no2\n\n# A tibble: 5,000 × 4\n        res  betas   sim     s\n      <dbl>  <dbl> <int> <dbl>\n 1  1.32     0.524     1  0.05\n 2 -0.106   -0.890     1  0.05\n 3 -0.231   -0.982     1  0.05\n 4 -0.480   -1.77      1  0.05\n 5 -0.482   -1.48      1  0.05\n 6  0.102    0.141     2  0.05\n 7 -0.00435  0.107     2  0.05\n 8 -0.738   -0.825     2  0.05\n 9 -0.160   -0.188     2  0.05\n10  0.787    0.834     2  0.05\n# … with 4,990 more rows\n\n\n\nggplot(o2, aes(x=betas, y=res)) +\ngeom_point() +\ngeom_smooth() +\ngeom_abline(colour=\"red\") +\nfacet_wrap(~ s)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nNoisy estimates of cell type proportions will lead to attenuated effect estimates\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.0 dplyr_1.0.10 \n\nloaded via a namespace (and not attached):\n [1] pillar_1.8.1      compiler_4.2.1    tools_4.2.1       digest_0.6.31    \n [5] jsonlite_1.8.4    evaluate_0.19     lifecycle_1.0.3   tibble_3.1.8     \n [9] gtable_0.3.1      nlme_3.1-158      lattice_0.20-45   mgcv_1.8-40      \n[13] pkgconfig_2.0.3   rlang_1.0.6       Matrix_1.4-1      DBI_1.1.3        \n[17] cli_3.5.0         yaml_2.3.6        xfun_0.36         fastmap_1.1.0    \n[21] withr_2.5.0       stringr_1.5.0     knitr_1.41        generics_0.1.3   \n[25] vctrs_0.5.1       htmlwidgets_1.5.4 grid_4.2.1        tidyselect_1.2.0 \n[29] glue_1.6.2        R6_2.5.1          fansi_1.0.3       rmarkdown_2.16   \n[33] farver_2.1.1      magrittr_2.0.3    scales_1.2.1      htmltools_0.5.4  \n[37] splines_4.2.1     assertthat_0.2.1  colorspace_2.0-3  labeling_0.4.2   \n[41] utf8_1.2.2        stringi_1.7.8     munsell_0.5.0"
  }
]