[
  {
    "objectID": "posts/2022-11-01-mvnorm-max-variable/index.html",
    "href": "posts/2022-11-01-mvnorm-max-variable/index.html",
    "title": "Probability of a random variable being larger than all other random variables in a multivariate normal vector",
    "section": "",
    "text": "Is there a faster way to do this by getting the probability from a multivariate normal distribution?\nRelated to this question: https://stats.stackexchange.com/a/4181\n\nlibrary(MCMCpack)\n\nLoading required package: coda\n\n\nLoading required package: MASS\n\n\n##\n## Markov Chain Monte Carlo Package (MCMCpack)\n\n\n## Copyright (C) 2003-2022 Andrew D. Martin, Kevin M. Quinn, and Jong Hee Park\n\n\n##\n## Support provided by the U.S. National Science Foundation\n\n\n## (Grants SES-0350646 and SES-0350613)\n##\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(simulateGP)\nlibrary(MASS)\nlibrary(mvtnorm)\n\nEmpirical simulation for probabilities, case of 3 variables\n\nn <- 3\nmu <- rnorm(n)\nS <- rwish(n, diag(n))\nemp <- mvrnorm(1000, mu, S)\nres <- apply(emp, 1, function(x) which.max(x)) %>% table() %>% prop.table()\nres\n\n.\n    1     2     3 \n0.666 0.146 0.188 \n\n\n\nA <- matrix(c(1,-1,0, 1,0,-1), nrow = 2, byrow = TRUE)\nnewMu <- as.vector(A %*% mu)\nnewS <- A %*% S %*% t(A)\npmvnorm(lower=c(0,0), mean = newMu, sigma = newS)\n\n[1] 0.6660382\nattr(,\"error\")\n[1] 1e-15\nattr(,\"msg\")\n[1] \"Normal Completion\"\n\nA <- matrix(c(1,-1,0, 1,0,-1), nrow = 2, byrow = TRUE)\nA <- A[,c(2,1,3)]\nnewMu <- as.vector(A %*% mu)\nnewS <- A %*% S %*% t(A)\npmvnorm(lower=c(0,0), mean = newMu, sigma = newS)\n\n[1] 0.1487365\nattr(,\"error\")\n[1] 1e-15\nattr(,\"msg\")\n[1] \"Normal Completion\"\n\nA <- matrix(c(1,-1,0, 1,0,-1), nrow = 2, byrow = TRUE)\nA <- A[,c(2,3,1)]\nnewMu <- as.vector(A %*% mu)\nnewS <- A %*% S %*% t(A)\npmvnorm(lower=c(0,0), mean = newMu, sigma = newS)\n\n[1] 0.1852253\nattr(,\"error\")\n[1] 1e-15\nattr(,\"msg\")\n[1] \"Normal Completion\"\n\n\nIncrease to arbitrary variables\nUse wishart distribution to generate random vcov matrix\n\nn <- 100\nmu <- rnorm(n)\nS <- rwish(n, diag(n))\n\nEmpirically generate correlated variables and count how often each one is the largest\n\nsamp <- mvrnorm(10000, mu, S)\nres <- apply(samp, 1, function(x) which.max(x)) %>% table() %>% prop.table()\nres\n\n.\n     1      2      3      4      5      6      7      8      9     10     11 \n0.0232 0.0085 0.0211 0.0072 0.0152 0.0118 0.0038 0.0161 0.0058 0.0094 0.0080 \n    12     13     14     15     16     17     18     19     20     21     22 \n0.0083 0.0152 0.0250 0.0103 0.0131 0.0115 0.0112 0.0093 0.0105 0.0084 0.0066 \n    23     24     25     26     27     28     29     30     31     32     33 \n0.0055 0.0088 0.0150 0.0098 0.0182 0.0171 0.0065 0.0090 0.0074 0.0134 0.0057 \n    34     35     36     37     38     39     40     41     42     43     44 \n0.0125 0.0084 0.0129 0.0088 0.0110 0.0222 0.0154 0.0079 0.0046 0.0111 0.0045 \n    45     46     47     48     49     50     51     52     53     54     55 \n0.0045 0.0071 0.0034 0.0045 0.0133 0.0053 0.0096 0.0013 0.0078 0.0138 0.0053 \n    56     57     58     59     60     61     62     63     64     65     66 \n0.0138 0.0039 0.0025 0.0101 0.0100 0.0059 0.0050 0.0206 0.0043 0.0157 0.0068 \n    67     68     69     70     71     72     73     74     75     76     77 \n0.0068 0.0119 0.0097 0.0117 0.0030 0.0162 0.0082 0.0040 0.0077 0.0156 0.0037 \n    78     79     80     81     82     83     84     85     86     87     88 \n0.0135 0.0082 0.0168 0.0033 0.0095 0.0072 0.0062 0.0124 0.0062 0.0156 0.0088 \n    89     90     91     92     93     94     95     96     97     98     99 \n0.0038 0.0141 0.0086 0.0089 0.0184 0.0117 0.0069 0.0180 0.0098 0.0110 0.0058 \n   100 \n0.0071 \n\n\nUse probability density function instead, evaluating for each variable the probability that it’s larger than all the other variables\n\n# Create design matrix\nswap_1 <- function(n, i)\n{\n  ind <- 1:n\n  if(i == 1) return(ind)\n  ind[i] <- 1\n  ind[1:(i-1)] <- 2:i\n  return(ind)\n}\n#sapply(1:7, function(i) swap_1(7, i))\nA <- cbind(\n  rep(1, n-1),\n  diag(rep(-1, n-1))\n) %>% as.matrix()\n\nemp <- sapply(1:n, function(i)\n{\n  A <- A[,swap_1(n,i)]\n  newMu <- as.vector(A %*% mu)\n  newS <- A %*% S %*% t(A)\n  pmvnorm(lower=rep(0,n-1), mean = newMu, sigma = newS)\n})\nplot(emp ~ as.numeric(res))\n\n\n\n\nTheoretical result works fine but is slower than empirical sampling."
  },
  {
    "objectID": "posts/2022-10-25-regression-non-iid/index.html",
    "href": "posts/2022-10-25-regression-non-iid/index.html",
    "title": "Regression with non i.i.d. samples",
    "section": "",
    "text": "\\[\n\\beta = (X^T \\rho^{-1} X)^{-1} X^T\\rho^{-1}Y\n\\]\nThe variance of the estimate will be\n\\[\n\\begin{aligned}\nVar(\\beta) &= \\sigma^2(X^T \\rho^{-1} X) \\\\\n&= \\frac{(\\hat{e}^T \\rho^{-1} \\hat{e})(X^T \\rho^{-1} X)}{n-r}\n\\end{aligned}\n\\]\nAdding to the complication, in this case it is the h2 estimates of a set of traits being correlated against number of GWAS hits for a set of traits, where the traits are in some way correlated. So the estimates of e.g. h2 are estimated with sampling error, so to account for that perhaps need to parametric bootstrap.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(mvtnorm)\n\n#' Regression with samples that are not independent\n#'\n#' For our analysis we're regressing estimates against estimates,\n#' and so it's a bit more complicated because the estimates each have an SE.\n#' For now let's just ignore that but I think if we don't account for it we\n#' will get a bit of regression dilution bias.\n#'\n#' @param x Vector of x values\n#' @param y Vector of y values\n#' @param rho correlation matrix\n#' @param se_x SE of x evalues (Ignore)\n#' @param se_y SE of y values (Ignore)\n#' @param nboot Number of bootstraps to get standard error (Ignore)\n#'\n#' @return\n#' @export\nreg_nonind <- function(x, y, rho, se_x=NULL, se_y=NULL, nboot=NULL)\n{\n  X <- cbind(rep(1, length(x)), x)\n  rho_inv <- solve(rho)\n  #beta <- solve(t(X) %*% rho_inv %*% X) %*% t(X) %*% rho_inv %*% y\n  beta <- ginv(t(X)%*%rho_inv %*%X)%*%t(X)%*%rho_inv %*%y\n  yhat <- X %*% beta\n  yres <- as.numeric(y - yhat)\n  se <- as.numeric((t(yres) %*% rho_inv %*% yres) / (length(x)-qr(X)$rank)) * solve(t(X) %*% rho_inv %*% X)\n  se <- sqrt(diag(se))\n  return(tibble(\n    param=c(\"intercept\", \"slope\"), beta=beta, se=se, pval=pnorm(abs(beta)/se, lower.tail=FALSE)\n  ))\n  \n  # get standard error via parametric bootstrap\n  # betaboot <- matrix(0, nboot, 2)\n  # for(i in 1:nboot)\n  # {\n  #   X[,2] <- rnorm(length(x), mean=x, sd=se_x)\n  #   Y <- rnorm(length(y), mean=y, sd=se_y)\n  #   betaboot[i,] <- solve(t(X) %*% rho_inv %*% X) %*% t(X) %*% rho_inv %*% Y %>% as.numeric()\n  # }\n  # se_boot <- apply(betaboot, 2, sd)\n}\n\nx <- runif(300)\ny <- runif(300)\nrho <- diag(300)\nrho[lower.tri(rho)] <- rnorm(sum(lower.tri(rho)), sd=0.01)\nrho[upper.tri(rho)] <- t(rho)[upper.tri(rho)]\n\nreg_nonind(x, y, rho, 10)\n\n# A tibble: 2 × 4\n  param     beta[,1]     se pval[,1]\n  <chr>        <dbl>  <dbl>    <dbl>\n1 intercept   0.507  0.0332 8.42e-53\n2 slope      -0.0522 0.0579 1.84e- 1\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50453 -0.23397 -0.01499  0.22869  0.51654 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.51135    0.03319  15.407   <2e-16 ***\nx           -0.05653    0.05748  -0.983    0.326    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2824 on 298 degrees of freedom\nMultiple R-squared:  0.003235,  Adjusted R-squared:  -0.0001103 \nF-statistic: 0.967 on 1 and 298 DF,  p-value: 0.3262"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "explodecomputer-blog",
    "section": "",
    "text": "Dec 18, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\nMendelian randomization\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Need a place to put my dynamic document notes"
  },
  {
    "objectID": "pending/vqtl.html",
    "href": "pending/vqtl.html",
    "title": "Inflation of vQTLs",
    "section": "",
    "text": "This paper describes how incomplete linkage disequilibrium can lead to inflated test statistics for interactions - https://www.nature.com/articles/s41586-021-03765-z. Because interaction terms contribute to variance heterogeneity across genotype classes, this could also inflate vQTL detection methods.\nExample model\nSuppose a system with three variants and one trait. The trait $x$ is influenced by a single additive causal variant $y_1$. But there is another variant in LD with this causal variant $y_2$. Finally, a third variant is independent of all other variables (think of that as a trans SNP). So\n\\[\nx_i = y_{1,i} + e_i\n\\]\nBut we test for an interaction between y_2 and y_3.\nRun some simulations…\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nset.seed(12345)\n\ntest_drm <- function(g, y)\n{\n  y.i <- tapply(y, g, median, na.rm=T)  \n  z.ij <- abs(y - y.i[g+1])\n  summary(lm(z.ij ~ g))$coef %>%\n    as_tibble() %>%\n    slice(2) %>%\n    mutate(method=\"drm\")\n}\n\ncorrelated_binomial <- function (nid, p1, p2, rho, n = 2, round = TRUE, print = FALSE) \n{\n    p <- p1\n    q <- p2\n    a <- function(rho, p, q) {\n        rho * sqrt(p * q * (1 - p) * (1 - q)) + (1 - p) * (1 - \n            q)\n    }\n    a.0 <- a(rho, p, q)\n    prob <- c(`(0,0)` = a.0, `(1,0)` = 1 - q - a.0, `(0,1)` = 1 - \n        p - a.0, `(1,1)` = a.0 + p + q - 1)\n    if (min(prob) < 0) {\n        print(prob)\n        stop(\"Error: a probability is negative.\")\n    }\n    n.sim <- nid\n    u <- sample.int(4, n.sim * n, replace = TRUE, prob = prob)\n    y <- floor((u - 1)/2)\n    x <- 1 - u%%2\n    x <- colSums(matrix(x, nrow = n))\n    y <- colSums(matrix(y, nrow = n))\n    if (round) {\n        x <- round(x)\n        y <- round(y)\n    }\n    if (print) {\n        print(table(x, y))\n        print(stats::cor(x, y))\n    }\n    return(cbind(x, y))\n}\n\ngendatp <- function(n, p1, p2, p3, r1)\n{\n    dat <- correlated_binomial(n, p1, p2, r1) %>% as_tibble()\n    names(dat) <- c(\"y1\", \"y2\")\n    dat$y3 <- rbinom(n, 1, p3)\n    return(dat)\n}\n\nrun_simp <- function(param, i)\n{\n    set.seed(i*10)\n    dat <- gendatp(param$n[i], param$p1[i], param$p2[i], param$p3[i], param$r1[i])\n    x <- dat$y1 + rnorm(nrow(dat), sd=sd(dat$y1)/4)\n    mod1 <- lm(x ~ y2 + y3, dat)\n    mod2 <- lm(x ~ y2 + y3 + y2*y3, dat)\n    amod <- anova(mod1, mod2)\n    param$F[i] <- amod$P[2]\n    o1 <- test_drm(dat$y1, x)\n    o2 <- test_drm(dat$y2, x)\n    o3 <- test_drm(dat$y3, x)\n    param$drm1[i] <- o1$`Pr(>|t|)`\n    param$drm2[i] <- o2$`Pr(>|t|)`\n    param$drm3[i] <- o3$`Pr(>|t|)`\n    return(param[i,])\n}\n\nparam <- expand.grid(\n    p1=0.1,\n    p2=0.1,\n    p3=0.5,\n    p4=0.1,\n    n=1000,\n    r1=seq(0, 1, by=0.2),\n    sim=1:500,\n    r2=NA,\n    F=NA,\n    drm1=NA,\n    drm2=NA,\n    drm3=NA\n)\n\nresp <- lapply(1:nrow(param), function(x) run_simp(param, x)) %>% bind_rows()\nstr(resp)\n\n'data.frame':   3000 obs. of  12 variables:\n $ p1  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p2  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p3  : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n $ p4  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ n   : num  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ...\n $ r1  : num  0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 ...\n $ sim : int  1 1 1 1 1 1 2 2 2 2 ...\n $ r2  : logi  NA NA NA NA NA NA ...\n $ F   : num  0.4136 0.0371 0.0567 0.5604 0.0364 ...\n $ drm1: num  0.288 0.376 0.16 0.215 0.998 ...\n $ drm2: num  1.64e-02 3.88e-17 1.58e-25 2.64e-26 9.41e-19 ...\n $ drm3: num  0.9178 0.1475 0.0181 0.1416 0.7348 ...\n - attr(*, \"out.attrs\")=List of 2\n  ..$ dim     : Named int [1:12] 1 1 1 1 1 6 500 1 1 1 ...\n  .. ..- attr(*, \"names\")= chr [1:12] \"p1\" \"p2\" \"p3\" \"p4\" ...\n  ..$ dimnames:List of 12\n  .. ..$ p1  : chr \"p1=0.1\"\n  .. ..$ p2  : chr \"p2=0.1\"\n  .. ..$ p3  : chr \"p3=0.5\"\n  .. ..$ p4  : chr \"p4=0.1\"\n  .. ..$ n   : chr \"n=1000\"\n  .. ..$ r1  : chr [1:6] \"r1=0.0\" \"r1=0.2\" \"r1=0.4\" \"r1=0.6\" ...\n  .. ..$ sim : chr [1:500] \"sim=  1\" \"sim=  2\" \"sim=  3\" \"sim=  4\" ...\n  .. ..$ r2  : chr \"r2=NA\"\n  .. ..$ F   : chr \"F=NA\"\n  .. ..$ drm1: chr \"drm1=NA\"\n  .. ..$ drm2: chr \"drm2=NA\"\n  .. ..$ drm3: chr \"drm3=NA\""
  },
  {
    "objectID": "pending/vqtl.html#type-1-error-of-vqtls",
    "href": "pending/vqtl.html#type-1-error-of-vqtls",
    "title": "Inflation of vQTLs",
    "section": "Type 1 error of vQTLs",
    "text": "Type 1 error of vQTLs\nThis is what happens to the genetic interaction between y_2 and y_3 - remember that neither of these have a causal effect, and there is no interaction term, however y_2 is correlated with the causal variant y_1\n\nggplot(resp, aes(x=as.factor(r1), y=-log10(F))) +\ngeom_boxplot() +\ngeom_hline(yintercept=-log10(0.05/nrow(resp))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"Interaction -log10 p for y2xy3\", x=\"LD between tagging\\nvariant and causal variant\")\n\n\n\n\nSo you get some false positives even after bonferroni correction. However now look at what happens to the variance QTL estimate for y_2 (the SNP that has no interaction but is in incomplete LD with the additive SNP y_1). Here we’ll use the DRM method to test for vQTL effects at y_2\n\nggplot(resp, aes(x=r1, y=-log10(drm2))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\")\n\n\n\n\nThis is really extreme type 1 sensitivity to incomplete LD. There’s no problem at the actual causal locus (y_1)\n\nggplot(resp, aes(x=r1, y=-log10(drm1))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\")\n\n\n\n\nOr at the unlinked locus y_3\n\nggplot(resp, aes(x=r1, y=-log10(drm3))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\")\n\n\n\n\nImplications - performing an exhaustive search is going to give quite problematic results if the main effects aren’t controlled. So you’d really have to know what all the main effects are before performing the vQTL tests in order to control for them. Note that incomplete control of the main effects is inevitable and we should be anticipating elevated type 1 error rates for any SNPs that are in the region of any large main effects."
  },
  {
    "objectID": "pending/vqtl.html#power-issues-when-controlling-for-main-effects",
    "href": "pending/vqtl.html#power-issues-when-controlling-for-main-effects",
    "title": "Inflation of vQTLs",
    "section": "Power issues when controlling for main effects",
    "text": "Power issues when controlling for main effects\nThe other problem is actually controlling for main effects. Suppose that a probe has two distal SNPs that interact e.g.\n\nn <- 10000\ng1 <- rbinom(n, 2, 0.4)\ng2 <- rbinom(n, 2, 0.4)\ny <- g1 + g2 + g1 * g2 + rnorm(n, sd=1.5)\ntest_drm(g1, y) %>% str\n\ntibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n $ Estimate  : num 0.362\n $ Std. Error: num 0.0173\n $ t value   : num 20.9\n $ Pr(>|t|)  : num 3.16e-95\n $ method    : chr \"drm\"\n\n\nThe DRM method finds a big vQTL effect here because of the GxG interaction - so it’s detecting that g1 might be interacting with something.\nIf we adjust for the main effects of g1 and g2 now look at DRM\n\nyres <- residuals(lm(y ~ g1 + g2))\ntest_drm(g1, yres) %>% str\n\ntibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n $ Estimate  : num 0.0194\n $ Std. Error: num 0.0138\n $ t value   : num 1.4\n $ Pr(>|t|)  : num 0.161\n $ method    : chr \"drm\"\n\n\nThe test statistic for the interaction test has massively attenuated.\nWhere does this leave us?\n\nIf a SNP is a known additive causal variant then it is relatively safe from type 1 error\nIf a SNP is not a known additive causal variant, then it is susceptible to type 1 error due to incomplete LD with actual additive causal variants\nIf we adjust the probe for additive causal variants before testing the SNP, we risk drastically reducing the vQTL effect that arise due to GxG interactions\nNote that this applies to GxE for when adjusting for other covariates too - e.g. if we adjust probes for smoking, age, sex, cell type etc and we are trying to find interactions with those based on vQTLs then the power to identify those vQTL effects drastically reduces"
  },
  {
    "objectID": "pending/vqtl.html#power-of-vqtl-vs-interaction",
    "href": "pending/vqtl.html#power-of-vqtl-vs-interaction",
    "title": "Inflation of vQTLs",
    "section": "Power of vQTL vs interaction",
    "text": "Power of vQTL vs interaction\nSuppose we simulate a GxE interaction. We can try to detect it either using a vQTL method (e.g. DRM) or using a direct interaction test.\n\nsim_gxe <- function(n, p, bi)\n{\n  params <- environment() %>% as.list() %>% as_tibble()\n  g <- rbinom(n, 2, p)\n  e <- rnorm(n)\n  y <- g + bi * g*e + e + rnorm(n)\n  \n  bind_rows(\n    test_drm(g, y),\n    summary(lm(y ~ g*e))$coef %>%\n      as_tibble() %>%\n      slice(n=4) %>%\n      mutate(method=\"interaction\")\n  ) %>%\n    bind_cols(., params)\n}\n\nparam <- expand.grid(\n  n=1000,\n  bi=seq(0,1,by=0.01),\n  nsim=10,\n  p=0.5\n) %>% select(-nsim)\nres <- lapply(1:nrow(param), function(i) do.call(sim_gxe, param[i,])) %>% bind_rows()\nres %>%\n  ggplot(., aes(x=bi, y=-log10(`Pr(>|t|)`))) +\n  geom_point(aes(colour=method))\n\n\n\n\nThe direct interaction test seems much better powered to detect these associations."
  },
  {
    "objectID": "posts/2022-05-08-prs-vs-ivw/index.html",
    "href": "posts/2022-05-08-prs-vs-ivw/index.html",
    "title": "PRS vs IVW",
    "section": "",
    "text": "How does PRS compare to IVW fixed effects analysis"
  },
  {
    "objectID": "posts/2022-12-16-vqtl/index.html",
    "href": "posts/2022-12-16-vqtl/index.html",
    "title": "Inflation of vQTLs",
    "section": "",
    "text": "This paper describes how incomplete linkage disequilibrium can lead to inflated test statistics for interactions - https://www.nature.com/articles/s41586-021-03765-z. Because interaction terms contribute to variance heterogeneity across genotype classes, this could also inflate vQTL detection methods.\nExample model\nSuppose a system with three variants and one trait. The trait $x$ is influenced by a single additive causal variant $y_1$. But there is another variant in LD with this causal variant $y_2$. Finally, a third variant is independent of all other variables (think of that as a trans SNP). So\n\\[\nx_i = y_{1,i} + e_i\n\\]\nBut we test for an interaction between y_2 and y_3.\nRun some simulations…\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nset.seed(12345)\n\ntest_drm <- function(g, y)\n{\n  y.i <- tapply(y, g, median, na.rm=T)  \n  z.ij <- abs(y - y.i[g+1])\n  summary(lm(z.ij ~ g))$coef %>%\n    as_tibble() %>%\n    slice(2) %>%\n    mutate(method=\"drm\")\n}\n\ncorrelated_binomial <- function (nid, p1, p2, rho, n = 2, round = TRUE, print = FALSE) \n{\n    p <- p1\n    q <- p2\n    a <- function(rho, p, q) {\n        rho * sqrt(p * q * (1 - p) * (1 - q)) + (1 - p) * (1 - \n            q)\n    }\n    a.0 <- a(rho, p, q)\n    prob <- c(`(0,0)` = a.0, `(1,0)` = 1 - q - a.0, `(0,1)` = 1 - \n        p - a.0, `(1,1)` = a.0 + p + q - 1)\n    if (min(prob) < 0) {\n        print(prob)\n        stop(\"Error: a probability is negative.\")\n    }\n    n.sim <- nid\n    u <- sample.int(4, n.sim * n, replace = TRUE, prob = prob)\n    y <- floor((u - 1)/2)\n    x <- 1 - u%%2\n    x <- colSums(matrix(x, nrow = n))\n    y <- colSums(matrix(y, nrow = n))\n    if (round) {\n        x <- round(x)\n        y <- round(y)\n    }\n    if (print) {\n        print(table(x, y))\n        print(stats::cor(x, y))\n    }\n    return(cbind(x, y))\n}\n\ngendatp <- function(n, p1, p2, p3, r1)\n{\n    dat <- correlated_binomial(n, p1, p2, r1) %>% as_tibble()\n    names(dat) <- c(\"y1\", \"y2\")\n    dat$y3 <- rbinom(n, 1, p3)\n    return(dat)\n}\n\nrun_simp <- function(param, i)\n{\n    set.seed(i*10)\n    dat <- gendatp(param$n[i], param$p1[i], param$p2[i], param$p3[i], param$r1[i])\n    x <- dat$y1 + rnorm(nrow(dat), sd=sd(dat$y1)/4)\n    mod1 <- lm(x ~ y2 + y3, dat)\n    mod2 <- lm(x ~ y2 + y3 + y2*y3, dat)\n    amod <- anova(mod1, mod2)\n    param$F[i] <- amod$P[2]\n    o1 <- test_drm(dat$y1, x)\n    o2 <- test_drm(dat$y2, x)\n    o3 <- test_drm(dat$y3, x)\n    param$drm1[i] <- o1$`Pr(>|t|)`\n    param$drm2[i] <- o2$`Pr(>|t|)`\n    param$drm3[i] <- o3$`Pr(>|t|)`\n    return(param[i,])\n}\n\nparam <- expand.grid(\n    p1=0.1,\n    p2=0.1,\n    p3=0.5,\n    p4=0.1,\n    n=1000,\n    r1=seq(0, 1, by=0.2),\n    sim=1:500,\n    r2=NA,\n    F=NA,\n    drm1=NA,\n    drm2=NA,\n    drm3=NA\n)\n\nresp <- lapply(1:nrow(param), function(x) run_simp(param, x)) %>% bind_rows()\nstr(resp)\n\n'data.frame':   3000 obs. of  12 variables:\n $ p1  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p2  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p3  : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n $ p4  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ n   : num  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ...\n $ r1  : num  0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 ...\n $ sim : int  1 1 1 1 1 1 2 2 2 2 ...\n $ r2  : logi  NA NA NA NA NA NA ...\n $ F   : num  0.4136 0.0371 0.0567 0.5604 0.0364 ...\n $ drm1: num  0.288 0.376 0.16 0.215 0.998 ...\n $ drm2: num  1.64e-02 3.88e-17 1.58e-25 2.64e-26 9.41e-19 ...\n $ drm3: num  0.9178 0.1475 0.0181 0.1416 0.7348 ...\n - attr(*, \"out.attrs\")=List of 2\n  ..$ dim     : Named int [1:12] 1 1 1 1 1 6 500 1 1 1 ...\n  .. ..- attr(*, \"names\")= chr [1:12] \"p1\" \"p2\" \"p3\" \"p4\" ...\n  ..$ dimnames:List of 12\n  .. ..$ p1  : chr \"p1=0.1\"\n  .. ..$ p2  : chr \"p2=0.1\"\n  .. ..$ p3  : chr \"p3=0.5\"\n  .. ..$ p4  : chr \"p4=0.1\"\n  .. ..$ n   : chr \"n=1000\"\n  .. ..$ r1  : chr [1:6] \"r1=0.0\" \"r1=0.2\" \"r1=0.4\" \"r1=0.6\" ...\n  .. ..$ sim : chr [1:500] \"sim=  1\" \"sim=  2\" \"sim=  3\" \"sim=  4\" ...\n  .. ..$ r2  : chr \"r2=NA\"\n  .. ..$ F   : chr \"F=NA\"\n  .. ..$ drm1: chr \"drm1=NA\"\n  .. ..$ drm2: chr \"drm2=NA\"\n  .. ..$ drm3: chr \"drm3=NA\""
  },
  {
    "objectID": "posts/2022-12-16-vqtl/index.html#type-1-error-of-vqtls",
    "href": "posts/2022-12-16-vqtl/index.html#type-1-error-of-vqtls",
    "title": "Inflation of vQTLs",
    "section": "Type 1 error of vQTLs",
    "text": "Type 1 error of vQTLs\nThis is what happens to the genetic interaction between y_2 and y_3 - remember that neither of these have a causal effect, and there is no interaction term, however y_2 is correlated with the causal variant y_1\n\nggplot(resp, aes(x=as.factor(r1), y=-log10(F))) +\ngeom_boxplot() +\ngeom_hline(yintercept=-log10(0.05/nrow(resp))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"Interaction -log10 p for y2xy3\", x=\"LD between tagging\\nvariant and causal variant\")\n\n\n\n\nSo you get some false positives even after bonferroni correction. However now look at what happens to the variance QTL estimate for y_2 (the SNP that has no interaction but is in incomplete LD with the additive SNP y_1). Here we’ll use the DRM method to test for vQTL effects at y_2\n\nggplot(resp, aes(x=r1, y=-log10(drm2))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\")\n\n\n\n\nThis is really extreme type 1 sensitivity to incomplete LD. There’s no problem at the actual causal locus (y_1)\n\nggplot(resp, aes(x=r1, y=-log10(drm1))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\")\n\n\n\n\nOr at the unlinked locus y_3\n\nggplot(resp, aes(x=r1, y=-log10(drm3))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\")\n\n\n\n\nImplications - performing an exhaustive search is going to give quite problematic results if the main effects aren’t controlled. So you’d really have to know what all the main effects are before performing the vQTL tests in order to control for them. Note that incomplete control of the main effects is inevitable and we should be anticipating elevated type 1 error rates for any SNPs that are in the region of any large main effects."
  },
  {
    "objectID": "posts/2022-12-16-vqtl/index.html#power-issues-when-controlling-for-main-effects",
    "href": "posts/2022-12-16-vqtl/index.html#power-issues-when-controlling-for-main-effects",
    "title": "Inflation of vQTLs",
    "section": "Power issues when controlling for main effects",
    "text": "Power issues when controlling for main effects\nThe other problem is actually controlling for main effects. Suppose that a probe has two distal SNPs that interact e.g.\n\nn <- 10000\ng1 <- rbinom(n, 2, 0.4)\ng2 <- rbinom(n, 2, 0.4)\ny <- g1 + g2 + g1 * g2 + rnorm(n, sd=1.5)\ntest_drm(g1, y) %>% str\n\ntibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n $ Estimate  : num 0.362\n $ Std. Error: num 0.0173\n $ t value   : num 20.9\n $ Pr(>|t|)  : num 3.16e-95\n $ method    : chr \"drm\"\n\n\nThe DRM method finds a big vQTL effect here because of the GxG interaction - so it’s detecting that g1 might be interacting with something.\nIf we adjust for the main effects of g1 and g2 now look at DRM\n\nyres <- residuals(lm(y ~ g1 + g2))\ntest_drm(g1, yres) %>% str\n\ntibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n $ Estimate  : num 0.0194\n $ Std. Error: num 0.0138\n $ t value   : num 1.4\n $ Pr(>|t|)  : num 0.161\n $ method    : chr \"drm\"\n\n\nThe test statistic for the interaction test has massively attenuated.\nWhere does this leave us?\n\nIf a SNP is a known additive causal variant then it is relatively safe from type 1 error\nIf a SNP is not a known additive causal variant, then it is susceptible to type 1 error due to incomplete LD with actual additive causal variants\nIf we adjust the probe for additive causal variants before testing the SNP, we risk drastically reducing the vQTL effect that arise due to GxG interactions\nNote that this applies to GxE for when adjusting for other covariates too - e.g. if we adjust probes for smoking, age, sex, cell type etc and we are trying to find interactions with those based on vQTLs then the power to identify those vQTL effects drastically reduces"
  },
  {
    "objectID": "posts/2022-12-16-vqtl/index.html#power-of-vqtl-vs-interaction",
    "href": "posts/2022-12-16-vqtl/index.html#power-of-vqtl-vs-interaction",
    "title": "Inflation of vQTLs",
    "section": "Power of vQTL vs interaction",
    "text": "Power of vQTL vs interaction\nSuppose we simulate a GxE interaction. We can try to detect it either using a vQTL method (e.g. DRM) or using a direct interaction test.\n\nsim_gxe <- function(n, p, bi)\n{\n  params <- environment() %>% as.list() %>% as_tibble()\n  g <- rbinom(n, 2, p)\n  e <- rnorm(n)\n  y <- g + bi * g*e + e + rnorm(n)\n  \n  bind_rows(\n    test_drm(g, y),\n    summary(lm(y ~ g*e))$coef %>%\n      as_tibble() %>%\n      slice(n=4) %>%\n      mutate(method=\"interaction\")\n  ) %>%\n    bind_cols(., params)\n}\n\nparam <- expand.grid(\n  n=1000,\n  bi=seq(0,1,by=0.01),\n  nsim=10,\n  p=0.5\n) %>% select(-nsim)\nres <- lapply(1:nrow(param), function(i) do.call(sim_gxe, param[i,])) %>% bind_rows()\nres %>%\n  ggplot(., aes(x=bi, y=-log10(`Pr(>|t|)`))) +\n  geom_point(aes(colour=method))\n\n\n\n\nThe direct interaction test seems much better powered to detect these associations."
  },
  {
    "objectID": "posts/2022-07-24-case-control-power/index.html",
    "href": "posts/2022-07-24-case-control-power/index.html",
    "title": "Power of GWAS in ascertained case control datasets",
    "section": "",
    "text": "The more rare the disease, the larger the variance of the liability when cases and controls are matched. This should improve statistical power because the cases and controls are ascertained to be more genetically distinct from each other.\nHowever, the Genetic Power Calculator concludes the opposite, as prevalence gets lower the power goes down (https://zzz.bwh.harvard.edu/gpc/cc2.html). e.g. for OR=1.1, ncase=1000, ncontrol=1000, af=0.5, for 80% power:\n\nprev = 0.001, power = 4e-5\nprev = 0.4, power = 0.71\n\nQuick simulation to investigate:\n\nlibrary(simulateGP)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nGenerate a function that will\n\ncreate a population with some genetic liability\nstochastically assign disease status based on heritability and prevalence\nascertain cases and controls\nidentify how many significant associations in the case/control sample\n\n\nsims <- function(ncase, ncontrol, nsnp, prev, hsq=0.5, thresh=5e-8)\n{\n  # Determine minimum sample size required to ascertain required number of cases and controls\n  n_req <- round(max(ncase / prev, ncontrol / prev) + 10000)\n  \n  # Generate matrix of genotype values\n  g <- make_geno(n_req, nsnp, 0.5)\n  \n  # Effect sizes for each SNP\n  b <- rnorm(nsnp)\n\n  dat <- tibble(\n    id = 1:n_req,\n    l = scale(g %*% b),               # genetic liability\n    p = gx_to_gp(l, hsq, prev),       # convert to disease probability\n    d = rbinom(n_req, 1, p)           # sample disease status from probability\n  )\n  \n  # Ascertain cases and controls \n  dat <- rbind(\n    subset(dat, d == 0)[1:ncase,],\n    subset(dat, d == 1)[1:ncontrol,]\n  )\n\n  # Perform GWAS\n  res <- gwas(dat$d, g[dat$id,], logistic=TRUE)\n  \n  # Count number of significant assocs\n  return(sum(res$pval < thresh))\n}\n\nRun a bunch of simulations\n\nparams <- expand.grid(\n  ncase = 1000,\n  ncontrol = 1000, \n  nsnp = c(2, 10, 100),\n  repeats = 1:10,\n  prev = seq(0.01, 0.3, by=0.01),\n  hsq=0.5,\n  thresh=5e-8\n) %>% select(-repeats)\nparams$nsig <- sapply(1:nrow(params), function(i) do.call(sims, params[i,]))\n\nPlot\n\nggplot(params, aes(x=prev, y=nsig/nsnp)) +\n  geom_point() +\n  facet_grid(nsnp ~ ., labeller=label_both, scale=\"free_y\") +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nSo power increases when prevalence is lower. Maybe this is related to polygenicity.\nSo what’s GPC doing differently to get the opposite result (lower prev = lower power)?"
  },
  {
    "objectID": "posts/2022-08-12-correlated-snps/index.html",
    "href": "posts/2022-08-12-correlated-snps/index.html",
    "title": "Correlated SNPs",
    "section": "",
    "text": "One instrument for X and X has no influence on Y\n\nlibrary(TwoSampleMR)\n\nTwoSampleMR version 0.5.6 \n[>] New: Option to use non-European LD reference panels for clumping etc\n[>] Some studies temporarily quarantined to verify effect allele\n[>] See news(package='TwoSampleMR') and https://gwas.mrcieu.ac.uk for further details\n\nlibrary(simulateGP)\n\n\nAttaching package: 'simulateGP'\n\n\nThe following objects are masked from 'package:TwoSampleMR':\n\n    allele_frequency, contingency, get_population_allele_frequency\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nset.seed(12345)\nmap <- tibble(snp=1, af=0.5)\nparams_x <- generate_gwas_params(map=map, h2=0.01, S=-0.4, Pi=1)\nparams_y <- generate_gwas_params(map=map, h2=0.0, S=-0.4, Pi=1)\nnid <- 100000\nss <- summary_set(\n    beta_gx=params_x$beta,\n    beta_gy=params_y$beta,\n    af=params_x$af,\n    n_gx=10000,\n    n_gy=10000,\n    n_overlap=0,\n    cor_xy=0.5\n)\n\nPerform MR with single causal variant\n\nmr(ss) %>% glimpse()\n\nAnalysing 'X' on 'Y'\n\n\nRows: 1\nColumns: 9\n$ id.exposure <chr> \"X\"\n$ id.outcome  <chr> \"Y\"\n$ outcome     <chr> \"Y\"\n$ exposure    <chr> \"X\"\n$ method      <chr> \"Wald ratio\"\n$ nsnp        <dbl> 1\n$ b           <dbl> -0.08648138\n$ se          <dbl> 0.0892847\n$ pval        <dbl> 0.3327436\n\n\nPerform MR with causal variant + 100 correlated tag SNPs\n\nss2 <- ss[rep(1,100),] %>% mutate(SNP=1:100)\nmr(ss2, method_list=\"mr_ivw\") %>% glimpse()\n\nAnalysing 'X' on 'Y'\n\n\nRows: 1\nColumns: 9\n$ id.exposure <chr> \"X\"\n$ id.outcome  <chr> \"Y\"\n$ outcome     <chr> \"Y\"\n$ exposure    <chr> \"X\"\n$ method      <chr> \"Inverse variance weighted\"\n$ nsnp        <int> 100\n$ b           <dbl> -0.08648138\n$ se          <dbl> 0.00892847\n$ pval        <dbl> 3.457243e-22\n\n\nVery small p-value - inflated type 1 error"
  },
  {
    "objectID": "posts/2022-08-12-correlated-snps/index.html#grs-correlation-performance",
    "href": "posts/2022-08-12-correlated-snps/index.html#grs-correlation-performance",
    "title": "Correlated SNPs",
    "section": "GRS correlation performance",
    "text": "GRS correlation performance\nTag SNPs are perfectly correlated with causal variant\n\nsim <- function(nid=10000, nsnp=10)\n{\n  g <- matrix(0, nid, nsnp)\n  g[,1] <- rnorm(nid)\n  for(i in 2:nsnp)\n  {\n    g[,i] <- g[,1]\n  }\n  y <- g[,1] + rnorm(nid)\n  summary(lm(y ~ g[,1]))\n  grs <- rowSums(g)\n  return(c(cor(y, g[,1])^2, cor(y, grs)^2))\n}\nsapply(1:10, function(i) sim()) %>% rowMeans() %>% tibble(method=c(\"Causal variant only\", \"GRS\"), rsq=.)\n\n# A tibble: 2 × 2\n  method                rsq\n  <chr>               <dbl>\n1 Causal variant only 0.500\n2 GRS                 0.500\n\n\nGRS and single causal variant work the same as Jack showed.\nTag SNPs are imperfectly correlated with causal variant\n\nsim <- function(nid=10000, nsnp=10)\n{\n  g <- matrix(0, nid, nsnp)\n  g[,1] <- rnorm(nid)\n  for(i in 2:nsnp)\n  {\n    g[,i] <- g[,i] + rnorm(nid, sd=0.5)\n  }\n  y <- g[,1] + rnorm(nid)\n  summary(lm(y ~ g[,1]))\n  grs <- rowSums(g)\n  return(c(cor(y, g[,1])^2, cor(y, grs)^2))\n}\nsapply(1:10, function(i) sim()) %>% rowMeans() %>% tibble(method=c(\"Causal variant only\", \"GRS\"), rsq=.)\n\n# A tibble: 2 × 2\n  method                rsq\n  <chr>               <dbl>\n1 Causal variant only 0.500\n2 GRS                 0.156\n\n\nNow the GRS doesn’t work well because it includes the variance of the SNP + noise that isn’t causally related to the trait.\n\\[\nr^2 = \\frac{cov(grs, y)^2}{var(grs) var(y)}\n\\]\ni.e. cov(grs,y) isn’t increasing, but (var(y)) is."
  },
  {
    "objectID": "posts/2022-08-12-correlated-snps/index.html#checking",
    "href": "posts/2022-08-12-correlated-snps/index.html#checking",
    "title": "Correlated SNPs",
    "section": "Checking",
    "text": "Checking\n\nn <- 10000\nnsnp <- 10\ng <- matrix(0, n, nsnp)\ng[,1] <- rnorm(n)\nfor(i in 2:nsnp)\n{\n  g[,i] <- g[,i] + rnorm(n, sd=0.5)\n}\ny <- g[,1] + rnorm(n)\ngrs <- rowSums(g)\ncov(y, grs)\n\n[1] 1.003715\n\ncov(y, g[,1])\n\n[1] 0.9788621\n\nsd(grs)\n\n[1] 1.805151\n\nsd(g[,1])\n\n[1] 0.996039\n\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dplyr_1.0.10      simulateGP_0.1.2  TwoSampleMR_0.5.6\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9        plyr_1.8.7        compiler_4.2.1    pillar_1.8.1     \n [5] iterators_1.0.14  tools_4.2.1       mr.raps_0.2       digest_0.6.29    \n [9] jsonlite_1.8.0    evaluate_0.16     lifecycle_1.0.3   tibble_3.1.8     \n[13] lattice_0.20-45   pkgconfig_2.0.3   rlang_1.0.6       Matrix_1.4-1     \n[17] foreach_1.5.2     DBI_1.1.3         cli_3.4.1         yaml_2.3.5       \n[21] xfun_0.33         fastmap_1.1.0     stringr_1.4.1     knitr_1.40       \n[25] generics_0.1.3    htmlwidgets_1.5.4 vctrs_0.5.1       tidyselect_1.1.2 \n[29] glmnet_4.1-4      grid_4.2.1        nortest_1.0-4     glue_1.6.2       \n[33] R6_2.5.1          fansi_1.0.3       survival_3.4-0    rmarkdown_2.16   \n[37] purrr_0.3.4       magrittr_2.0.3    ellipsis_0.3.2    codetools_0.2-18 \n[41] htmltools_0.5.3   splines_4.2.1     assertthat_0.2.1  shape_1.4.6      \n[45] utf8_1.2.2        stringi_1.7.8"
  },
  {
    "objectID": "posts/2022-12-18-bmi-wf-replication/index.html",
    "href": "posts/2022-12-18-bmi-wf-replication/index.html",
    "title": "BMI instrument replication",
    "section": "",
    "text": "Liza’s analysis of BMI instruments clusters them by PheWAS and finds that cluster 4 relates to SES, and drives BMI-EDU biased effect. Are the instruments in cluster 4 solely due to dynastic confounding? If so they should fail to replicate in the sibling analysis."
  },
  {
    "objectID": "posts/2022-12-18-bmi-wf-replication/index.html#instruments",
    "href": "posts/2022-12-18-bmi-wf-replication/index.html#instruments",
    "title": "BMI instrument replication",
    "section": "Instruments",
    "text": "Instruments\n\nlibrary(ieugwasr)\n\nAPI: public: http://gwas-api.mrcieu.ac.uk/\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nbmi_inst <- list(\n  c(\"rs1097327\",\"rs2186120\",\"rs2166172\",\"rs75641275\",\"rs12037698\",\"rs1446585\",\"rs16846140\",\"rs13062093\",\"rs2051559\",\"rs6861649\",\"rs2281819\",\"rs12662900\",\"rs9388681\",\"rs17132130\",\"rs215634\",\"rs79682948\",\"rs2192649\",\"rs13294945\",\"rs7357754\",\"rs4749937\",\"rs1465900\",\"rs1799992\",\"rs55938344\",\"rs7987928\",\"rs7331420\",\"rs9522279\",\"rs55689274\",\"rs4777541\",\"rs7189149\",\"rs11079849\",\"rs113230003\",\"rs150998792\"),\n  c(\"rs3737992\",\"rs1167311\",\"rs12140153\",\"rs34361149\",\"rs12049202\",\"rs2181375\",\"rs17024393\",\"rs61813324\",\"rs815163\",\"rs2678204\",\"rs17014332\",\"rs563738408\",\"rs6751993\",\"rs4671328\",\"rs6545714\",\"rs4482463\",\"rs815715\",\"rs6769617\",\"rs1225004\",\"rs355777\",\"rs6776471\",\"rs869400\",\"rs4261944\",\"rs1296328\",\"rs6867471\",\"rs2307111\",\"rs28404639\",\"rs7442885\",\"rs55838622\",\"rs13174863\",\"rs146696797\",\"rs9368828\",\"rs34045288\",\"rs72892910\",\"rs2482398\",\"rs2253310\",\"rs765874\",\"rs9688977\",\"rs6950388\",\"rs4722398\",\"rs1470749\",\"rs147678035\",\"rs39330\",\"rs1805123\",\"rs6601527\",\"rs791405\",\"rs12679106\",\"rs201519328\",\"rs9297524\",\"rs17770336\",\"rs2440589\",\"rs3931548\",\"rs3861879\",\"rs2356376\",\"rs17399739\",\"rs6265\",\"rs491711\",\"rs7942037\",\"rs10898330\",\"rs12364470\",\"rs55726687\",\"rs1458156\",\"rs7138383\",\"rs147730268\",\"rs9507895\",\"rs10507483\",\"rs11148421\",\"rs1949204\",\"rs1441264\",\"rs7990098\",\"rs8015400\",\"rs7141420\",\"rs1286138\",\"rs6575340\",\"rs12881629\",\"rs3803286\",\"rs7183417\",\"rs4776970\",\"rs2870111\",\"rs2046002\",\"rs56803094\",\"rs12926311\",\"rs879620\",\"rs7193783\",\"rs4402589\",\"rs11642015\",\"rs117342986\",\"rs11150461\",\"rs4790841\",\"rs56161855\",\"rs11150745\",\"rs9319615\",\"rs57636386\",\"rs111640872\",\"rs3810291\",\"rs6050446\",\"rs67844506\",\"rs6001870\"),\n  c(\"rs12024554\",\"rs35722922\",\"rs593010\",\"rs10803762\",\"rs6772763\",\"rs80082351\",\"rs1471740\",\"rs2606228\",\"rs13107325\",\"rs13176429\",\"rs1919243\",\"rs286818\",\"rs3844598\",\"rs7755574\",\"rs3843540\",\"rs17716502\",\"rs1411432\",\"rs2267958\",\"rs2450447\",\"rs11826177\",\"rs7124681\",\"rs317687\",\"rs3897102\",\"rs9506311\",\"rs35193668\",\"rs3759584\",\"rs862320\",\"rs7774\",\"rs8078135\",\"rs11653258\",\"rs56212061\",\"rs11084554\",\"rs2153740\",\"rs8134638\",\"rs2837398\"),\n  c(\"rs115866895\",\"rs6687953\",\"rs935166\",\"rs13002946\",\"rs72820274\",\"rs12619626\",\"rs13427822\",\"rs72967047\",\"rs9843653\",\"rs1454687\",\"rs34811474\",\"rs17085463\",\"rs1383723\",\"rs35853157\",\"rs1503526\",\"rs1477290\",\"rs9463511\",\"rs9277992\",\"rs236660\",\"rs2045293\",\"rs10954772\",\"rs77883185\",\"rs35529153\",\"rs11782074\",\"rs2398861\",\"rs7030732\",\"rs61845249\",\"rs4595495\",\"rs61903695\",\"rs1048932\",\"rs2292238\",\"rs116394958\",\"rs2933223\",\"rs217672\",\"rs2333012\",\"rs11855853\",\"rs62037365\",\"rs34966008\",\"rs1788808\",\"rs784257\",\"rs11666480\"),\n  c(\"rs4648450\",\"rs12031634\",\"rs1778830\",\"rs61826867\",\"rs170553\",\"rs10185199\",\"rs6545144\",\"rs3806572\",\"rs2861685\",\"rs7557796\",\"rs72844755\",\"rs115584509\",\"rs35882248\",\"rs34373881\",\"rs4377469\",\"rs75557510\",\"rs2035936\",\"rs66679256\",\"rs73213484\",\"rs6831020\",\"rs11099020\",\"rs750090\",\"rs6536575\",\"rs7701777\",\"rs9291822\",\"rs10059453\",\"rs2118793\",\"rs4921301\",\"rs4467770\",\"rs7453694\",\"rs9342196\",\"rs9489620\",\"rs13210756\",\"rs3807652\",\"rs7810870\",\"rs7461253\",\"rs2616192\",\"rs12681792\",\"rs55781253\",\"rs13289199\",\"rs2254331\",\"rs1327808\",\"rs6597653\",\"rs2439823\",\"rs6591\",\"rs75936055\",\"rs34292685\",\"rs7940866\",\"rs329651\",\"rs12422552\",\"rs4761401\",\"rs1901241\",\"rs11613680\",\"rs9579775\",\"rs7995015\",\"rs9527895\",\"rs9522183\",\"rs145946602\",\"rs8027969\",\"rs117632017\",\"rs113182412\",\"rs756717\",\"rs11656076\",\"rs2332306\",\"rs7237783\",\"rs1942826\",\"rs45486197\",\"rs76040172\",\"rs28489620\"),\n  c(\"rs61743745\",\"rs10921760\",\"rs2141004\",\"rs754481\",\"rs80330591\",\"rs199750218\",\"rs12479357\",\"rs4485556\",\"rs2569993\",\"rs9847186\",\"rs13076052\",\"rs762705\",\"rs550669262\",\"rs10865612\",\"rs6780459\",\"rs73169730\",\"rs10938397\",\"rs35851183\",\"rs190301182\",\"rs1428120\",\"rs245775\",\"rs1775255\",\"rs2749929\",\"rs6973656\",\"rs12537134\",\"rs1425717\",\"rs11012732\",\"rs12260817\",\"rs112921972\",\"rs61871615\",\"rs10749233\",\"rs845084\",\"rs67609008\",\"rs2035806\",\"rs12575252\",\"rs555754158\",\"rs2234458\",\"rs704061\",\"rs13353100\",\"rs7498044\",\"rs2342892\",\"rs55931203\",\"rs60764613\",\"rs2155869\",\"rs1389067\",\"rs2247593\",\"rs273505\",\"rs10404726\",\"rs112693590\")\n)\n\nExtract instruments from the population and sibling GWASs\n\nwfest <- ieugwasr::associations(unlist(bmi_inst), \"ieu-b-4815\")\npopest <- ieugwasr::associations(unlist(bmi_inst), \"ieu-b-4816\")\ngiantest <- ieugwasr::associations(unlist(bmi_inst), \"ieu-b-40\")\nukbest <- ieugwasr::associations(unlist(bmi_inst), \"ukb-b-19953\")\n\nReplication rates function\n\nexp_rep <- function(b_disc, b_rep, se_disc, se_rep, alpha)\n{\n  p_sign <- pnorm(-abs(b_disc) / se_disc) * pnorm(-abs(b_disc) / se_rep) + ((1 - pnorm(-abs(b_disc) / se_disc)) * (1 - pnorm(-abs(b_disc) / se_rep)))\n  p_sig <- pnorm(-abs(b_disc) / se_rep + qnorm(alpha / 2)) + (1 - pnorm(-abs(b_disc) / se_rep - qnorm(alpha / 2)))\n  p_rep <- pnorm(abs(b_rep)/se_rep, lower.tail=FALSE)\n  res <- tibble::tibble(\n    nsnp=length(b_disc),\n    metric=c(\"Sign\", \"Sign\", \"P-value\", \"P-value\"),\n    datum=c(\"Expected\", \"Observed\", \"Expected\", \"Observed\"),\n    value=c(sum(p_sign, na.rm=TRUE), sum(sign(b_disc) == sign(b_rep)), sum(p_sig, na.rm=TRUE), sum(p_rep < alpha, na.rm=TRUE))\n  )\n  return(list(res=res, variants=dplyr::tibble(sig=p_sig, sign=p_sign)))\n}\n\nAnalysis\n\nests <- bind_rows(giantest, wfest, popest, ukbest) %>%\n  mutate(cluster=NA)\nfor(i in 1:length(bmi_inst))\n{\n  ests$cluster[ests$rsid %in% bmi_inst[[i]]] <- i\n}\n\nests %>%\n  group_by(id, cluster) %>%\n  summarise(n=n(), psig = sum(p < 5e-3)/n) %>%\n  ggplot(., aes(x=as.factor(cluster), y=psig)) +\n  geom_bar(position=\"dodge\", stat=\"identity\", aes(fill=id))\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\n\n\n\n\nExpected vs observed replication rates\n\no <- lapply(1:length(bmi_inst), function(i)\n{\n  x <- bmi_inst[[i]]\n  dat <- inner_join(\n    subset(popest, rsid %in% x),\n    subset(wfest, rsid %in% x),\n    by=\"rsid\"\n  )\n  exp_rep(dat$beta.x, dat$beta.y, dat$se.x, dat$se.y, 1e-3)[[1]] %>%\n    mutate(cluster=i)\n})\no %>% bind_rows() %>%\n  ggplot(aes(x=as.factor(cluster), y=value/nsnp)) +\n  geom_point(aes(colour=datum)) +\n  facet_grid(. ~ metric) +\n  labs(x=\"Cluster\", y=\"Fraction of sig. instruments\", colour=\"\")"
  },
  {
    "objectID": "posts/2022-12-18-bmi-wf-replication/index.html#summary",
    "href": "posts/2022-12-18-bmi-wf-replication/index.html#summary",
    "title": "BMI instrument replication",
    "section": "Summary",
    "text": "Summary\nAll clusters appear to replicate as expected in the within-family GWAS, which is consistent with there being almost no shrinkage of the effect sizes."
  },
  {
    "objectID": "posts/2022-12-18-bmi-wf-replication/index.html#childhood-vs-adulthood-relationship-to-ses",
    "href": "posts/2022-12-18-bmi-wf-replication/index.html#childhood-vs-adulthood-relationship-to-ses",
    "title": "BMI instrument replication",
    "section": "Childhood vs adulthood relationship to SES",
    "text": "Childhood vs adulthood relationship to SES\n\nlibrary(TwoSampleMR)\n\nTwoSampleMR version 0.5.6 \n[>] New: Option to use non-European LD reference panels for clumping etc\n[>] Some studies temporarily quarantined to verify effect allele\n[>] See news(package='TwoSampleMR') and https://gwas.mrcieu.ac.uk for further details\n\n\n\nAttaching package: 'TwoSampleMR'\n\n\nThe following object is masked from 'package:ieugwasr':\n\n    ld_matrix\n\nchild <- make_dat(\"ukb-b-10011\", \"ebi-a-GCST90002409\")\n\nExtracting data for 18 SNP(s) from 1 GWAS(s)\n\n\nHarmonising Townsend deprivation index at recruitment || id:ukb-b-10011 (ukb-b-10011) and Childhood body mass index || id:ebi-a-GCST90002409 (ebi-a-GCST90002409)\n\nmr(child) %>% select(nsnp, b, se, pval)\n\nAnalysing 'ukb-b-10011' on 'ebi-a-GCST90002409'\n\n\n  nsnp           b        se      pval\n1   18  0.00605344 1.4172450 0.9966448\n2   18 -0.07468209 0.2308366 0.7462954\n3   18 -0.05801819 0.2184155 0.7905224\n4   18 -0.11644263 0.4977415 0.8178241\n5   18 -0.11644263 0.4702948 0.8074135\n\n\n\nadult <- make_dat(\"ukb-b-10011\", \"ukb-b-19953\")\n\nExtracting data for 18 SNP(s) from 1 GWAS(s)\n\n\nHarmonising Townsend deprivation index at recruitment || id:ukb-b-10011 (ukb-b-10011) and Body mass index (BMI) || id:ukb-b-19953 (ukb-b-19953)\n\nmr(adult) %>% select(nsnp, b, se, pval)\n\nAnalysing 'ukb-b-10011' on 'ukb-b-19953'\n\n\n  nsnp          b         se         pval\n1   18 -0.2812238 0.80612571 7.317440e-01\n2   18  0.3450191 0.07873753 1.176549e-05\n3   18  0.4835539 0.13327136 2.852487e-04\n4   18  0.3588983 0.15546210 3.380204e-02\n5   18  0.3128024 0.12623146 2.400304e-02\n\n\nReplication of clustered instruments from adult to child\n\nukbest <- ieugwasr::associations(unlist(bmi_inst), \"ukb-b-19953\")\nchildest <- ieugwasr::associations(unlist(bmi_inst), \"ebi-a-GCST90002409\")\n\n\no1 <- lapply(1:length(bmi_inst), function(i)\n{\n  x <- bmi_inst[[i]]\n  dat <- inner_join(\n    subset(ukbest, rsid %in% x),\n    subset(childest, rsid %in% x),\n    by=\"rsid\"\n  )\n  exp_rep(dat$beta.x, dat$beta.y, dat$se.x, dat$se.y, 1e-3)[[1]] %>%\n    mutate(cluster=i)\n})\no1 %>% bind_rows() %>%\n  ggplot(aes(x=as.factor(cluster), y=value/nsnp)) +\n  geom_point(aes(colour=datum)) +\n  facet_grid(. ~ metric) +\n  labs(x=\"Cluster\", y=\"Fraction of sig. instruments\", colour=\"\")\n\n\n\n\n\nSummary\n\nSES has an influence on BMI in adulthood but not childhood\nThe replication rate amongst clusters appears to be relatively consistent, except cluster 2 replicates particularly well"
  },
  {
    "objectID": "posts/2022-05-08-prs-vs-ivw/index.html#simulation-study",
    "href": "posts/2022-05-08-prs-vs-ivw/index.html#simulation-study",
    "title": "PRS vs IVW",
    "section": "Simulation study",
    "text": "Simulation study\n\nlibrary(simulateGP)\ngeno1 <- make_geno(10000, 500, 0.5)\nb <- choose_effects(500, 0.3)\nx1 <- make_phen(b, geno1)\ny1 <- make_phen(0.4, x1)\n\ngeno2 <- make_geno(1000, 500, 0.5)\nx2 <- make_phen(b, geno2)\ny2 <- make_phen(0.4, x2)\n\nbhat <- gwas(x1, geno1)\nb_unweighted <- sign(b)\n\n\nStandard unweighted PRS analysis\n\nprs_unweighted <- geno2 %*% b_unweighted\nsummary(lm(x2 ~ prs_unweighted))\n\n\nCall:\nlm(formula = x2 ~ prs_unweighted)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2356 -0.5547 -0.0330  0.6087  3.1932 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -0.321168   0.035444  -9.061   <2e-16 ***\nprs_unweighted  0.027292   0.001791  15.239   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9011 on 998 degrees of freedom\nMultiple R-squared:  0.1888,    Adjusted R-squared:  0.1879 \nF-statistic: 232.2 on 1 and 998 DF,  p-value: < 2.2e-16\n\n\n\n\nMeta analysing per-SNP PRS scores\n\nlibrary(meta)\n\nLoading 'meta' package (version 6.0-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\no <- sapply(1:ncol(geno2), function(i)\n{\n  prs_unweighted <- geno2[,i] * b_unweighted[i]\n  summary(lm(x2 ~ prs_unweighted))$coef[2,1:2]\n})\nmetafor::rma(yi=o[1,], sei=o[2,], method=\"EE\")\n\n\nEqual-Effects Model (k = 500)\n\nI^2 (total heterogeneity / total variability):   17.58%\nH^2 (total variability / sampling variability):  1.21\n\nTest for Heterogeneity:\nQ(df = 499) = 605.4622, p-val = 0.0007\n\nModel Results:\n\nestimate      se     zval    pval   ci.lb   ci.ub      \n  0.0277  0.0020  13.8615  <.0001  0.0238  0.0316  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nStandard errors with number of SNPs\n\nses_unweighted <- sapply(1:ncol(geno2), function(i)\n{\n  prs_unweighted <- geno2[,1:i, drop=FALSE] %*% b_unweighted[1:i]\n  summary(lm(x2 ~ prs_unweighted))$coef[2,2]\n})\nplot(ses_unweighted)"
  }
]