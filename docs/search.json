[
  {
    "objectID": "posts/2022-01-09-did-models-confounding/index.html",
    "href": "posts/2022-01-09-did-models-confounding/index.html",
    "title": "Difference in difference models in observational data are still potentially confounded",
    "section": "",
    "text": "Motivated by discussion with Tim Cadman relating to this Renzi et al (2019). Long-Term PM10 Exposure and Cause-Specific Mortality in the Latium Region (Italy): A Difference-in-Differences Approach.\nFor argument’s sake suppose that air pollution (PM10) isn’t causal for deaths. Simulate a situation where wealth varies by region, and each region has a different wealth trajectory over time. Some info:\n\nWealth, PM10 and deaths are measured for 10 years in 300 regions.\nWealth causes PM10 and deaths.\nThere is a global confounder for the start point of wealth and deaths\nTime has an additional effect on both deaths and wealth - i.e. it’s also a global confounder\nThe causal effect of wealth on death = 1. We want our model to get that right.\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nnregion <- 300\nregion <- 1:300\nwealth_int <- rnorm(nregion)\nwealth_slope <- rnorm(nregion)\nglobal_confounder <- rnorm(nregion, sd=10)\nnyear <- 10\n\ndat <- lapply(1:nregion, function(i)\n  {\n    tibble(\n      region=i,\n      year=1:nyear,\n      # wealth goes up due to global confounder, year, random error\n      wealth = global_confounder[i] + wealth_int[i] + year * wealth_slope[i] + rnorm(nyear),\n      # PM10 only related to wealth and random error\n      pm10 = wealth + rnorm(nyear, sd=5),\n      # deaths go up due to global confounders, wealth, year and random term\n      deaths = global_confounder[i] + wealth + rnorm(nyear) + year\n    )\n}) %>%\n  bind_rows()\n\n\ndat\n\n# A tibble: 3,000 × 5\n   region  year wealth   pm10 deaths\n    <int> <int>  <dbl>  <dbl>  <dbl>\n 1      1     1   2.25  2.28    5.18\n 2      1     2   1.81  0.393   5.79\n 3      1     3   2.25  4.05    9.20\n 4      1     4   2.89 12.0    11.6 \n 5      1     5   4.02  7.55   12.7 \n 6      1     6   3.15  3.08   12.7 \n 7      1     7   1.67  1.51   11.3 \n 8      1     8   2.83  4.92   13.4 \n 9      1     9   5.08  8.42   16.2 \n10      1    10   4.96 10.1    18.4 \n# … with 2,990 more rows\n\n\nPlot it showing change in deaths over time by region\n\nggplot(dat, aes(x=year, y=deaths)) +\n  geom_point(aes(group=as.factor(region))) +\n  geom_line(aes(group=as.factor(region)))\n\n\n\n\nTry again but just regression lines per region\n\nggplot(dat, aes(x=year, y=deaths)) +\n  geom_point(aes(group=as.factor(region))) +\n  geom_smooth(method=\"lm\", aes(group=as.factor(region)), se=FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nShow that PM10 is closely coupled with wealth\n\nplot(pm10 ~ wealth, dat)\n\n\n\n\nUse regression to test for influence of wealth on deaths - this gives a very confounded result because of the global confounder.\n\nsummary(lm(deaths ~ wealth, dat))\n\n\nCall:\nlm(formula = deaths ~ wealth, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.6518  -4.4416  -0.7553   3.7309  25.9024 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 5.760562   0.115301   49.96   <2e-16 ***\nwealth      1.715797   0.009365  183.21   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.315 on 2998 degrees of freedom\nMultiple R-squared:  0.918, Adjusted R-squared:  0.918 \nF-statistic: 3.357e+04 on 1 and 2998 DF,  p-value: < 2.2e-16\n\n\nSame will be true for PM10\n\nsummary(lm(deaths ~ pm10, dat))\n\n\nCall:\nlm(formula = deaths ~ pm10, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.668  -6.799  -0.196   6.836  38.319 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.88951    0.18634    31.6   <2e-16 ***\npm10         1.46754    0.01399   104.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.21 on 2998 degrees of freedom\nMultiple R-squared:  0.7858,    Adjusted R-squared:  0.7857 \nF-statistic: 1.1e+04 on 1 and 2998 DF,  p-value: < 2.2e-16\n\n\nSo PM10 is confounded at two levels - the between-region (global) level and the within-region level.\nNow do a DiD model for wealth - it should give us an unbiased estimate of 1. Note that this long form of DiD is sometimes called a fixed-effects model.\n\nsummary(lm(deaths ~ wealth + as.factor(region) + as.factor(year), dat)) %>%\n  coefficients %>% as_tibble %>% slice(n=2)\n\n# A tibble: 1 × 4\n  Estimate `Std. Error` `t value` `Pr(>|t|)`\n     <dbl>        <dbl>     <dbl>      <dbl>\n1    0.993      0.00590      168.          0\n\n\nIt does. What about DiD model for PM10? This should be less confounded because it eliminates the global confounder, but still confounded by the structural confounding that happens at all areas\n\nsummary(lm(deaths ~ pm10 + as.factor(region) + as.factor(year), dat)) %>%\n  coefficients %>% as_tibble %>% slice(n=2)\n\n# A tibble: 1 × 4\n  Estimate `Std. Error` `t value` `Pr(>|t|)`\n     <dbl>        <dbl>     <dbl>      <dbl>\n1    0.293      0.00931      31.4  4.94e-185\n\n\nIf we control for wealth, the effect of PM10 will be unbiased because the within-region bias has been removed, and the global bias acted via wealth anyway so that’s been removed also\n\nsummary(lm(deaths ~ pm10 + wealth, dat)) %>%\n  coefficients %>% as_tibble %>% slice(n=2)\n\n# A tibble: 1 × 4\n  Estimate `Std. Error` `t value` `Pr(>|t|)`\n     <dbl>        <dbl>     <dbl>      <dbl>\n1  -0.0224       0.0231    -0.969      0.333\n\n\nWe could try to simplify by doing the more explicit difference in difference estimate. Compare the change in deaths with the change in wealth (or PM10) over the 10 year period.\n\ndat2 <- group_by(dat, region) %>%\n  summarise(\n    delta_deaths = deaths[nyear] - deaths[1],\n    delta_wealth = wealth[nyear] - wealth[1],\n    delta_pm10 = pm10[nyear] - pm10[1]\n  )\ndat2\n\n# A tibble: 300 × 4\n   region delta_deaths delta_wealth delta_pm10\n    <int>        <dbl>        <dbl>      <dbl>\n 1      1        13.2         2.71        7.80\n 2      2        19.6        12.9        18.6 \n 3      3        20.6        10.6         6.65\n 4      4         4.24       -3.10      -10.7 \n 5      5         9.08       -0.285      -9.31\n 6      6        17.7         9.94        8.22\n 7      7        10.7         2.15       10.4 \n 8      8        16.7         6.61       -3.01\n 9      9         6.55       -2.51       13.1 \n10     10        17.9         9.83       10.8 \n# … with 290 more rows\n\n\nDo the DiD estimates using these - should recapitulate what we got above\n\nsummary(lm(delta_deaths ~ delta_wealth, dat2))\n\n\nCall:\nlm(formula = delta_deaths ~ delta_wealth, data = dat2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6788 -0.9511  0.1077  1.0252  3.6020 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.063688   0.082771   109.5   <2e-16 ***\ndelta_wealth 0.990565   0.008896   111.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.434 on 298 degrees of freedom\nMultiple R-squared:  0.9765,    Adjusted R-squared:  0.9764 \nF-statistic: 1.24e+04 on 1 and 298 DF,  p-value: < 2.2e-16\n\n\n\nsummary(lm(delta_deaths ~ delta_pm10, dat2))\n\n\nCall:\nlm(formula = delta_deaths ~ delta_pm10, data = dat2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.0715  -3.9080  -0.3154   3.7670  16.6385 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  9.09954    0.33647   27.04   <2e-16 ***\ndelta_pm10   0.61677    0.02844   21.69   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.827 on 298 degrees of freedom\nMultiple R-squared:  0.6122,    Adjusted R-squared:  0.6109 \nF-statistic: 470.5 on 1 and 298 DF,  p-value: < 2.2e-16\n\n\nWeirdly, it works for wealth but we do get a slightly difference answer for PM10. There is probably a lot of literature on what makes a fixed effects model (the first version of the DiD we did above) different from an explicit version like this one.\nTo summarise - the DiD model is useful to account for unmeasured global confounders (including time), but it might not be too surprising that it doesn’t control for all confounders - you really do need some sort of experiment / randomisation that specifically mimics the exact intervention you want to make to get to completely unconfounded effects.\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.0 dplyr_1.0.10 \n\nloaded via a namespace (and not attached):\n [1] pillar_1.8.1      compiler_4.2.1    tools_4.2.1       digest_0.6.31    \n [5] lattice_0.20-45   nlme_3.1-158      jsonlite_1.8.4    evaluate_0.19    \n [9] lifecycle_1.0.3   tibble_3.1.8      gtable_0.3.1      mgcv_1.8-40      \n[13] pkgconfig_2.0.3   rlang_1.0.6       Matrix_1.4-1      DBI_1.1.3        \n[17] cli_3.5.0         yaml_2.3.6        xfun_0.36         fastmap_1.1.0    \n[21] withr_2.5.0       stringr_1.5.0     knitr_1.41        generics_0.1.3   \n[25] vctrs_0.5.1       htmlwidgets_1.5.4 grid_4.2.1        tidyselect_1.2.0 \n[29] glue_1.6.2        R6_2.5.1          fansi_1.0.3       rmarkdown_2.16   \n[33] farver_2.1.1      magrittr_2.0.3    scales_1.2.1      htmltools_0.5.4  \n[37] splines_4.2.1     assertthat_0.2.1  colorspace_2.0-3  labeling_0.4.2   \n[41] utf8_1.2.2        stringi_1.7.8     munsell_0.5.0"
  },
  {
    "objectID": "posts/2023-02-10-vqtl/index.html",
    "href": "posts/2023-02-10-vqtl/index.html",
    "title": "Sandwich variance estimators to control LD leakage",
    "section": "",
    "text": "See https://explodecomputer.github.io/lab-book/posts/2022-12-16-vqtl/ for inflation issues with vQTLs.\nCan sandwich estimators help https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5943197/\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(sandwich)\nset.seed(12345)\n\ntest_drm <- function(g, y, sandwich)\n{\n  nom <- c(b=\"Estimate\", se=\"Std. Error\", tval=\"t value\", pval=\"Pr(>|t|)\")\n  y.i <- tapply(y, g, median, na.rm=T)\n  z.ij <- abs(y - y.i[g+1])\n  a <- lm(z.ij ~ g)\n  m1 <- a %>%\n    summary %>%\n    coef %>%\n    as_tibble() %>%\n    # rename(all_of(nom)) %>%\n    slice(2) %>%\n    mutate(method=\"drm\")\n  if(sandwich)\n  {\n    o <- sandwich::vcovHC(a, type=\"HC\")\n    m2 <- m1\n    m2$`Std. Error` <- sqrt(o[2,2])\n    m2$`t value` <- m2$Estimate / m2$`Std. Error`\n    m2$`Pr(>|t|)` <- pnorm(m2$`t value`, lower.tail=TRUE)\n    m2$method <- \"drm sandwich\"\n    return(m2)\n  }\n  return(m1)\n}\n\ncorrelated_binomial <- function (nid, p1, p2, rho, n = 2, round = TRUE, print = FALSE) \n{\n    p <- p1\n    q <- p2\n    a <- function(rho, p, q) {\n        rho * sqrt(p * q * (1 - p) * (1 - q)) + (1 - p) * (1 - q)\n    }\n    a.0 <- a(rho, p, q)\n    prob <- c(`(0,0)` = a.0, `(1,0)` = 1 - q - a.0, `(0,1)` = 1 - \n        p - a.0, `(1,1)` = a.0 + p + q - 1)\n    if (min(prob) < 0) {\n        print(prob)\n        stop(\"Error: a probability is negative.\")\n    }\n    n.sim <- nid\n    u <- sample.int(4, n.sim * n, replace = TRUE, prob = prob)\n    y <- floor((u - 1)/2)\n    x <- 1 - u%%2\n    x <- colSums(matrix(x, nrow = n))\n    y <- colSums(matrix(y, nrow = n))\n    if (round) {\n        x <- round(x)\n        y <- round(y)\n    }\n    if (print) {\n        print(table(x, y))\n        print(stats::cor(x, y))\n    }\n    return(cbind(x, y))\n}\n\ngendatp <- function(n, p1, p2, p3, r1)\n{\n    dat <- correlated_binomial(n, p1, p2, r1) %>% as_tibble()\n    names(dat) <- c(\"y1\", \"y2\")\n    dat$y3 <- rbinom(n, 1, p3)\n    return(dat)\n}\n\nrun_simp <- function(param, i)\n{\n    set.seed(i*10)\n    dat <- gendatp(param$n[i], param$p1[i], param$p2[i], param$p3[i], param$r1[i])\n    x <- dat$y1 + rnorm(nrow(dat), sd=sd(dat$y1)/4)\n    mod1 <- lm(x ~ y2 + y3, dat)\n    mod2 <- lm(x ~ y2 + y3 + y2*y3, dat)\n    amod <- anova(mod1, mod2)\n    param$F[i] <- amod$P[2]\n    o1 <- test_drm(dat$y1, x, param$sandwich[i])\n    o2 <- test_drm(dat$y2, x, param$sandwich[i])\n    o3 <- test_drm(dat$y3, x, param$sandwich[i])\n    param$drm1[i] <- o1$`Pr(>|t|)`\n    param$drm2[i] <- o2$`Pr(>|t|)`\n    param$drm3[i] <- o3$`Pr(>|t|)`\n    return(param[i,])\n}\n\nparam <- expand.grid(\n    sandwich=c(T,F),\n    p1=0.1,\n    p2=0.1,\n    p3=0.5,\n    p4=0.1,\n    n=1000,\n    r1=seq(0, 1, by=0.2),\n    sim=1:250,\n    r2=NA,\n    F=NA,\n    drm1=NA,\n    drm2=NA,\n    drm3=NA\n)\n\nresp <- lapply(1:nrow(param), function(x) run_simp(param, x)) %>% bind_rows()\nstr(resp)\n\n'data.frame':   3000 obs. of  13 variables:\n $ sandwich: logi  TRUE FALSE TRUE FALSE TRUE FALSE ...\n $ p1      : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p2      : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p3      : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n $ p4      : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ n       : num  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ...\n $ r1      : num  0 0 0.2 0.2 0.4 0.4 0.6 0.6 0.8 0.8 ...\n $ sim     : int  1 1 1 1 1 1 1 1 1 1 ...\n $ r2      : logi  NA NA NA NA NA NA ...\n $ F       : num  0.4136 0.4151 0.0649 0.1828 0.3566 ...\n $ drm1    : num  0.8477 0.376 0.9038 0.0149 0.3456 ...\n $ drm2    : num  0.988716 0.12422 1 0.000167 1 ...\n $ drm3    : num  0.45884 0.14746 0.00898 0.5823 0.18002 ...\n - attr(*, \"out.attrs\")=List of 2\n  ..$ dim     : Named int [1:13] 2 1 1 1 1 1 6 250 1 1 ...\n  .. ..- attr(*, \"names\")= chr [1:13] \"sandwich\" \"p1\" \"p2\" \"p3\" ...\n  ..$ dimnames:List of 13\n  .. ..$ sandwich: chr [1:2] \"sandwich=TRUE\" \"sandwich=FALSE\"\n  .. ..$ p1      : chr \"p1=0.1\"\n  .. ..$ p2      : chr \"p2=0.1\"\n  .. ..$ p3      : chr \"p3=0.5\"\n  .. ..$ p4      : chr \"p4=0.1\"\n  .. ..$ n       : chr \"n=1000\"\n  .. ..$ r1      : chr [1:6] \"r1=0.0\" \"r1=0.2\" \"r1=0.4\" \"r1=0.6\" ...\n  .. ..$ sim     : chr [1:250] \"sim=  1\" \"sim=  2\" \"sim=  3\" \"sim=  4\" ...\n  .. ..$ r2      : chr \"r2=NA\"\n  .. ..$ F       : chr \"F=NA\"\n  .. ..$ drm1    : chr \"drm1=NA\"\n  .. ..$ drm2    : chr \"drm2=NA\"\n  .. ..$ drm3    : chr \"drm3=NA\"\n\n\n\nggplot(resp, aes(x=r1, y=-log10(drm2))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\") +\nfacet_grid(. ~ sandwich)\n\n\n\n\nPower\n\nrun_simp2 <- function(param, i)\n{\n    set.seed(i*10)\n    dat <- gendatp(param$n[i], param$p1[i], param$p2[i], param$p3[i], param$r1[i])\n    x <- dat$y1 + dat$y2 * dat$y3 * param$b[i] + rnorm(nrow(dat), sd=sd(dat$y1)/4)\n    mod1 <- lm(x ~ y2 + y3, dat)\n    mod2 <- lm(x ~ y2 + y3 + y2*y3, dat)\n    amod <- anova(mod1, mod2)\n    param$F[i] <- amod$P[2]\n    o1 <- test_drm(dat$y1, x, param$sandwich[i])\n    o2 <- test_drm(dat$y2, x, param$sandwich[i])\n    o3 <- test_drm(dat$y3, x, param$sandwich[i])\n    param$drm1[i] <- o1$`Pr(>|t|)`\n    param$drm2[i] <- o2$`Pr(>|t|)`\n    param$drm3[i] <- o3$`Pr(>|t|)`\n    return(param[i,])\n}\n\nparam <- expand.grid(\n    sandwich=c(T,F),\n    b=c(0, 0.1, 1),\n    p1=0.1,\n    p2=0.1,\n    p3=0.5,\n    p4=0.1,\n    n=1000,\n    r1=seq(0, 1, by=0.2),\n    sim=1:250,\n    r2=NA,\n    F=NA,\n    drm1=NA,\n    drm2=NA,\n    drm3=NA\n)\n\nresp2 <- lapply(1:nrow(param), function(x) run_simp2(param, x)) %>% bind_rows()\nstr(resp)\n\n'data.frame':   3000 obs. of  13 variables:\n $ sandwich: logi  TRUE FALSE TRUE FALSE TRUE FALSE ...\n $ p1      : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p2      : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p3      : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n $ p4      : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ n       : num  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ...\n $ r1      : num  0 0 0.2 0.2 0.4 0.4 0.6 0.6 0.8 0.8 ...\n $ sim     : int  1 1 1 1 1 1 1 1 1 1 ...\n $ r2      : logi  NA NA NA NA NA NA ...\n $ F       : num  0.4136 0.4151 0.0649 0.1828 0.3566 ...\n $ drm1    : num  0.8477 0.376 0.9038 0.0149 0.3456 ...\n $ drm2    : num  0.988716 0.12422 1 0.000167 1 ...\n $ drm3    : num  0.45884 0.14746 0.00898 0.5823 0.18002 ...\n - attr(*, \"out.attrs\")=List of 2\n  ..$ dim     : Named int [1:13] 2 1 1 1 1 1 6 250 1 1 ...\n  .. ..- attr(*, \"names\")= chr [1:13] \"sandwich\" \"p1\" \"p2\" \"p3\" ...\n  ..$ dimnames:List of 13\n  .. ..$ sandwich: chr [1:2] \"sandwich=TRUE\" \"sandwich=FALSE\"\n  .. ..$ p1      : chr \"p1=0.1\"\n  .. ..$ p2      : chr \"p2=0.1\"\n  .. ..$ p3      : chr \"p3=0.5\"\n  .. ..$ p4      : chr \"p4=0.1\"\n  .. ..$ n       : chr \"n=1000\"\n  .. ..$ r1      : chr [1:6] \"r1=0.0\" \"r1=0.2\" \"r1=0.4\" \"r1=0.6\" ...\n  .. ..$ sim     : chr [1:250] \"sim=  1\" \"sim=  2\" \"sim=  3\" \"sim=  4\" ...\n  .. ..$ r2      : chr \"r2=NA\"\n  .. ..$ F       : chr \"F=NA\"\n  .. ..$ drm1    : chr \"drm1=NA\"\n  .. ..$ drm2    : chr \"drm2=NA\"\n  .. ..$ drm3    : chr \"drm3=NA\"\n\n\n\nggplot(resp2, aes(x=r1, y=-log10(drm2))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\") +\nfacet_grid(b ~ sandwich)\n\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] sandwich_3.0-2 ggplot2_3.4.0  dplyr_1.0.10  \n\nloaded via a namespace (and not attached):\n [1] RColorBrewer_1.1-3 pillar_1.8.1       compiler_4.2.1     tools_4.2.1       \n [5] digest_0.6.31      jsonlite_1.8.4     evaluate_0.19      lifecycle_1.0.3   \n [9] tibble_3.1.8       gtable_0.3.1       lattice_0.20-45    pkgconfig_2.0.3   \n[13] rlang_1.0.6        DBI_1.1.3          cli_3.5.0          yaml_2.3.6        \n[17] xfun_0.36          fastmap_1.1.0      withr_2.5.0        stringr_1.5.0     \n[21] knitr_1.41         generics_0.1.3     vctrs_0.5.1        htmlwidgets_1.5.4 \n[25] grid_4.2.1         tidyselect_1.2.0   glue_1.6.2         R6_2.5.1          \n[29] fansi_1.0.3        rmarkdown_2.16     farver_2.1.1       magrittr_2.0.3    \n[33] scales_1.2.1       htmltools_0.5.4    assertthat_0.2.1   colorspace_2.0-3  \n[37] labeling_0.4.2     utf8_1.2.2         stringi_1.7.8      munsell_0.5.0     \n[41] zoo_1.8-11"
  },
  {
    "objectID": "posts/2023-03-24-ldl/index.html",
    "href": "posts/2023-03-24-ldl/index.html",
    "title": "LDL and drug adjustment",
    "section": "",
    "text": "People with high LDL cholesterol tend to take medication that lowers their observed LDL\nHow does this affect genetic associations?\n\nGet a rough distribution of LDL cholesterol (e.g. like this)\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nhist(rnorm(10000, 85, sd=20), breaks=100)\n\n\n\n\nGenerate genetic effect for LDL cholesterol with effect b_gy\n\nn <- 100000\ng <- rnorm(n)\nmean_ldl <- 85\nsd_ldl <- 20\nb_gy <- 10\ny <- mean_ldl + g * b_gy + rnorm(n, sd=sqrt(sd_ldl^2 - b_gy^2))\nhist(y, breaks=100)\n\n\n\n\nPeople with high LDL cholesterol more likely to take medication - make an adjusted LDL cholesterol measure which has a reduced value amongst people taking medications\n\nn <- 100000\ng <- rnorm(n)\nmean_ldl <- 130\nsd_ldl <- 20\nb_gy <- 1\nmed_effect <- 0.8\n\ny <- mean_ldl + g * b_gy + rnorm(n, sd=sqrt(sd_ldl^2-b_gy^2))\nmed <- rbinom(n, 1, plogis(scale(y)))\ny_obs <- y\ny_obs[as.logical(med)] <- y_obs[as.logical(med)] * med_effect\n\nrbind(\n  summary(lm(y ~ g))$coef[2,],\n  summary(lm(y_obs ~ g))$coef[2,]\n) %>% as_tibble() %>% mutate(measure=c(\"y\", \"y_obs\"))\n\n# A tibble: 2 × 5\n  Estimate `Std. Error` `t value` `Pr(>|t|)` measure\n     <dbl>        <dbl>     <dbl>      <dbl> <chr>  \n1    0.909       0.0632      14.4   7.62e-47 y      \n2    0.569       0.0552      10.3   6.46e-25 y_obs  \n\n\nWhat happens if you put in an approximate adjustment of y_obs. e.g. the effect of the drug is relative (20% reduction), we could erroneously add an absolute value onto med users to try to adjust\n\nn <- 100000\ng <- rnorm(n)\nmean_ldl <- 130\nsd_ldl <- 20\nb_gy <- 1\nmed_effect <- 0.8\n\ny <- mean_ldl + g * b_gy + rnorm(n, sd=sqrt(sd_ldl^2-b_gy^2))\nmed <- rbinom(n, 1, plogis(scale(y)))\ny_obs <- y\ny_obs[as.logical(med)] <- y_obs[as.logical(med)] * med_effect\n\ny_adj_true <- y_obs\ny_adj_true[as.logical(med)] <- y_obs[as.logical(med)] / med_effect\n\ny_adj_approx <- y_obs\ny_adj_approx[as.logical(med)] <- y_obs[as.logical(med)] + 20\n\nrbind(\n  summary(lm(y ~ g))$coef[2,],\n  summary(lm(y_obs ~ g))$coef[2,],\n  summary(lm(y_adj_true ~ g))$coef[2,],\n  summary(lm(y_adj_approx ~ g))$coef[2,]\n) %>% as_tibble() %>% mutate(measure=c(\"y\", \"y_obs\", \"y_adj_true\", \"y_adj_approx\"))\n\n# A tibble: 4 × 5\n  Estimate `Std. Error` `t value` `Pr(>|t|)` measure     \n     <dbl>        <dbl>     <dbl>      <dbl> <chr>       \n1    0.936       0.0631      14.8   1.07e-49 y           \n2    0.602       0.0547      11.0   3.95e-28 y_obs       \n3    0.936       0.0631      14.8   1.07e-49 y_adj_true  \n4    0.789       0.0538      14.7   1.41e-48 y_adj_approx\n\n\nDoes collider bias have an impact? Statins are administered due to having a cardio event or being high risk e.g. due to family history. So there could be other non-LDL genetic factors that influence medication usage, and selecting or adjusting for medication usage could induce a collider that associates non-LDL genotypes with the adjusted LDL phenotype. e.g. simulate a large non-LDL factor that influences medication for illustration\n\nn <- 1000000\ng <- rnorm(n)\ng_other <- rnorm(n)\nmean_ldl <- 130\nsd_ldl <- 20\nb_gy <- 1\nb_omed <- 1\nmed_effect <- 0.8\n\ny <- mean_ldl + g * b_gy + rnorm(n, sd=sqrt(sd_ldl^2-b_gy^2))\nmed <- rbinom(n, 1, plogis(scale(y) + g_other * b_omed))\ny_obs <- y\ny_obs[as.logical(med)] <- y_obs[as.logical(med)] * med_effect\n\ny_adj_true <- y_obs\ny_adj_true[as.logical(med)] <- y_obs[as.logical(med)] / med_effect\n\ny_adj_approx <- y_obs\ny_adj_approx[as.logical(med)] <- y_obs[as.logical(med)] + 20\n\nResult for LDL genotype\n\nrbind(\n  summary(lm(y ~ g))$coef[2,],\n  summary(lm(y_obs ~ g))$coef[2,],\n  summary(lm(y_adj_true ~ g))$coef[2,],\n  summary(lm(y_adj_approx ~ g))$coef[2,]\n) %>% as_tibble() %>% mutate(measure=c(\"y\", \"y_obs\", \"y_adj_true\", \"y_adj_approx\"))\n\n# A tibble: 4 × 5\n  Estimate `Std. Error` `t value` `Pr(>|t|)` measure     \n     <dbl>        <dbl>     <dbl>      <dbl> <chr>       \n1    1.03        0.0200      51.5  0         y           \n2    0.680       0.0181      37.6  5.73e-309 y_obs       \n3    1.03        0.0200      51.5  0         y_adj_true  \n4    0.871       0.0172      50.5  0         y_adj_approx\n\n\nResult for non-LDL genotype\n\nrbind(\n  summary(lm(y ~ g_other))$coef[2,],\n  summary(lm(y_obs ~ g_other))$coef[2,],\n  summary(lm(y_adj_true ~ g_other))$coef[2,],\n  summary(lm(y_adj_approx ~ g_other))$coef[2,]\n) %>% as_tibble() %>% mutate(measure=c(\"y\", \"y_obs\", \"y_adj_true\", \"y_adj_approx\"))\n\n# A tibble: 4 × 5\n  Estimate `Std. Error` `t value` `Pr(>|t|)` measure     \n     <dbl>        <dbl>     <dbl>      <dbl> <chr>       \n1  -0.0168       0.0200    -0.841      0.401 y           \n2  -4.72         0.0175  -270.         0     y_obs       \n3  -0.0168       0.0200    -0.841      0.401 y_adj_true  \n4  -1.10         0.0172   -63.8        0     y_adj_approx\n\n\n\n\n\nA relative reduction in observed LDL measures due to medication will lead to attenuated genetic effect estimates\nAdjustment to correct LDL values amongst medication users potentially resolves the issue\nEven an inaccurate adjustment can improve the estimate\nCollider bias induces a negative association between unadjusted LDL and non-LDL genetic factors, but not a great deal of issue at the LDL locus\nPerfect adjustment of medication would resolve this problem\nImperfect adjustment partially avoids the issue but some bias remains at the non-LDL genetic factor\n\n\n\nsessionInfo()\n\nR version 4.2.3 Patched (2023-03-15 r84020)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dplyr_1.0.10\n\nloaded via a namespace (and not attached):\n [1] knitr_1.41        magrittr_2.0.3    tidyselect_1.2.0  R6_2.5.1         \n [5] rlang_1.0.6       fastmap_1.1.0     fansi_1.0.3       stringr_1.5.0    \n [9] tools_4.2.3       xfun_0.36         utf8_1.2.2        cli_3.5.0        \n[13] DBI_1.1.3         htmltools_0.5.4   assertthat_0.2.1  yaml_2.3.6       \n[17] digest_0.6.31     tibble_3.1.8      lifecycle_1.0.3   htmlwidgets_1.5.4\n[21] vctrs_0.5.1       glue_1.6.2        evaluate_0.19     rmarkdown_2.16   \n[25] stringi_1.7.8     compiler_4.2.3    pillar_1.8.1      generics_0.1.3   \n[29] jsonlite_1.8.4    pkgconfig_2.0.3"
  },
  {
    "objectID": "posts/2022-11-01-mvnorm-max-variable/index.html",
    "href": "posts/2022-11-01-mvnorm-max-variable/index.html",
    "title": "Probability of a random variable being larger than all other random variables in a multivariate normal vector",
    "section": "",
    "text": "I have 1k SNPs in a region. I know the causal variant and the LD matrix. The effect size at each SNP will be related to the allele frequency and the LD at all other variants. The SE across the SNPs will be correlated in relation to the LD. I can generate the expected effect size and the variance covariance matrix of the effects. Once I have that, I can generate beta values from a multivariate normal distribution, and determine how often each of the SNPs is the top SNP.\nIs there a faster way to do this by getting the probability from a multivariate normal distribution?\nRelated to this question: https://stats.stackexchange.com/a/4181\n\nlibrary(MCMCpack)\n\nLoading required package: coda\n\n\nLoading required package: MASS\n\n\n##\n## Markov Chain Monte Carlo Package (MCMCpack)\n\n\n## Copyright (C) 2003-2022 Andrew D. Martin, Kevin M. Quinn, and Jong Hee Park\n\n\n##\n## Support provided by the U.S. National Science Foundation\n\n\n## (Grants SES-0350646 and SES-0350613)\n##\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(simulateGP)\nlibrary(MASS)\nlibrary(mvtnorm)\n\nEmpirical simulation for probabilities, case of 3 variables\n\nn <- 3\nmu <- rnorm(n)\nS <- rwish(n, diag(n))\nemp <- mvrnorm(1000, mu, S)\nres <- apply(emp, 1, function(x) which.max(x)) %>% table() %>% prop.table()\nres\n\n.\n    1     2     3 \n0.666 0.146 0.188 \n\n\n\nA <- matrix(c(1,-1,0, 1,0,-1), nrow = 2, byrow = TRUE)\nnewMu <- as.vector(A %*% mu)\nnewS <- A %*% S %*% t(A)\npmvnorm(lower=c(0,0), mean = newMu, sigma = newS)\n\n[1] 0.6660382\nattr(,\"error\")\n[1] 1e-15\nattr(,\"msg\")\n[1] \"Normal Completion\"\n\nA <- matrix(c(1,-1,0, 1,0,-1), nrow = 2, byrow = TRUE)\nA <- A[,c(2,1,3)]\nnewMu <- as.vector(A %*% mu)\nnewS <- A %*% S %*% t(A)\npmvnorm(lower=c(0,0), mean = newMu, sigma = newS)\n\n[1] 0.1487365\nattr(,\"error\")\n[1] 1e-15\nattr(,\"msg\")\n[1] \"Normal Completion\"\n\nA <- matrix(c(1,-1,0, 1,0,-1), nrow = 2, byrow = TRUE)\nA <- A[,c(2,3,1)]\nnewMu <- as.vector(A %*% mu)\nnewS <- A %*% S %*% t(A)\npmvnorm(lower=c(0,0), mean = newMu, sigma = newS)\n\n[1] 0.1852253\nattr(,\"error\")\n[1] 1e-15\nattr(,\"msg\")\n[1] \"Normal Completion\"\n\n\nIncrease to arbitrary variables\nUse wishart distribution to generate random vcov matrix\n\nn <- 100\nmu <- rnorm(n)\nS <- rwish(n, diag(n))\n\nEmpirically generate correlated variables and count how often each one is the largest\n\nsamp <- mvrnorm(10000, mu, S)\nres <- apply(samp, 1, function(x) which.max(x)) %>% table() %>% prop.table()\nres\n\n.\n     1      2      3      4      5      6      7      8      9     10     11 \n0.0232 0.0085 0.0211 0.0072 0.0152 0.0118 0.0038 0.0161 0.0058 0.0094 0.0080 \n    12     13     14     15     16     17     18     19     20     21     22 \n0.0083 0.0152 0.0250 0.0103 0.0131 0.0115 0.0112 0.0093 0.0105 0.0084 0.0066 \n    23     24     25     26     27     28     29     30     31     32     33 \n0.0055 0.0088 0.0150 0.0098 0.0182 0.0171 0.0065 0.0090 0.0074 0.0134 0.0057 \n    34     35     36     37     38     39     40     41     42     43     44 \n0.0125 0.0084 0.0129 0.0088 0.0110 0.0222 0.0154 0.0079 0.0046 0.0111 0.0045 \n    45     46     47     48     49     50     51     52     53     54     55 \n0.0045 0.0071 0.0034 0.0045 0.0133 0.0053 0.0096 0.0013 0.0078 0.0138 0.0053 \n    56     57     58     59     60     61     62     63     64     65     66 \n0.0138 0.0039 0.0025 0.0101 0.0100 0.0059 0.0050 0.0206 0.0043 0.0157 0.0068 \n    67     68     69     70     71     72     73     74     75     76     77 \n0.0068 0.0119 0.0097 0.0117 0.0030 0.0162 0.0082 0.0040 0.0077 0.0156 0.0037 \n    78     79     80     81     82     83     84     85     86     87     88 \n0.0135 0.0082 0.0168 0.0033 0.0095 0.0072 0.0062 0.0124 0.0062 0.0156 0.0088 \n    89     90     91     92     93     94     95     96     97     98     99 \n0.0038 0.0141 0.0086 0.0089 0.0184 0.0117 0.0069 0.0180 0.0098 0.0110 0.0058 \n   100 \n0.0071 \n\n\nUse probability density function instead, evaluating for each variable the probability that it’s larger than all the other variables\n\n# Create design matrix\nswap_1 <- function(n, i)\n{\n  ind <- 1:n\n  if(i == 1) return(ind)\n  ind[i] <- 1\n  ind[1:(i-1)] <- 2:i\n  return(ind)\n}\n#sapply(1:7, function(i) swap_1(7, i))\nA <- cbind(\n  rep(1, n-1),\n  diag(rep(-1, n-1))\n) %>% as.matrix()\n\nemp <- sapply(1:n, function(i)\n{\n  A <- A[,swap_1(n,i)]\n  newMu <- as.vector(A %*% mu)\n  newS <- A %*% S %*% t(A)\n  pmvnorm(lower=rep(0,n-1), mean = newMu, sigma = newS)\n})\nplot(emp ~ as.numeric(res))\n\n\n\n\nTheoretical result works fine but is slower than empirical sampling."
  },
  {
    "objectID": "posts/2022-12-14-wf-rare-variants/index.html",
    "href": "posts/2022-12-14-wf-rare-variants/index.html",
    "title": "Sibling replication of Backman rare variants",
    "section": "",
    "text": "Backman et al 2022 (https://www.nature.com/articles/s41586-021-04103-z) performed exome GWAS of 4k traits, and found 564 rare-variant - trait pairs. They corrected for 20 PCs from common variants and 20 PCs from rare variants.\nQuestion: Is uncontrolled recent population stratification biasing these associations or even driving false positives?\nWe could use siblings to estimate the direct effects for these discovery associations. There would be two major hits to power, first only 20k of the 500k UK Biobank will be used. Second, power to detect direct effects using siblings is reduced. So we can’t conclude that these are false positives if they simply don’t replicate in the sibling analysis.\nInstead, we can ask to what extent are the replication associations consistent with the original estimates, given the reduction in precision. i.e. What fraction are expected to replicate at some nominal threshold, and what fraction are observed to replicate. If there is a significant difference between the observed and expected replication rate, then that indicates uncorrected inflation in the discovery exome GWAS.\nThis analysis: Power calculation - how much shrinkage would there need to be before there is 80% power to detect a difference between the sibling estimate and the population estimate?\nRead in Backman results\nThere were 564 rare-variant - trait pairs in Backman et al 2022. Of these 484 were for quantitative traits, restricting to those here for simplicity.\n\nlibrary(here)\n\nhere() starts at /Users/gh13047/repo/lab-book\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nset.seed(12345)\ndat <- read.csv(\"Book3.csv\", stringsAsFactors=FALSE, header=TRUE) %>% \n  as_tibble() %>%\n  mutate(se = (uci - beta)/1.96)\ndat %>% glimpse()\n\nRows: 484\nColumns: 14\n$ Effect..95..CI.                               <chr> \"-0.15 (-0.19, -0.11)\", …\n$ P.value                                       <dbl> 9.25e-12, 1.24e-24, 1.03…\n$ Effect.direction                              <chr> \"-\", \"-\", \"-\", \"+\", \"-\",…\n$ N.cases.with.0.1.2.copies.of.effect.allele    <chr> \"88038|1684|13\", \"418277…\n$ N.controls.with.0.1.2.copies.of.effect.allele <chr> \"NA|NA|NA\", \"NA|NA|NA\", …\n$ Effect.allele.frequency                       <dbl> 9.5e-03, 2.1e-04, 4.9e-0…\n$ AA                                            <int> 88038, 418277, 407938, 3…\n$ AB                                            <int> 1684, 177, 4027, 4076, 1…\n$ BB                                            <int> 13, 0, 11, 1, 8, 0, 1, 0…\n$ N                                             <int> 89735, 418454, 411976, 3…\n$ beta                                          <dbl> -0.150, -0.720, -0.100, …\n$ lci                                           <chr> \"-0.19\", \"-0.86\", \"-0.13…\n$ uci                                           <dbl> -0.110, -0.580, -0.079, …\n$ se                                            <dbl> 0.020408163, 0.071428571…\n\nplot(-log10(dat$P.value), abs(dat$beta)/dat$se)\n\n\n\n\nFunctions from Alex for relative sample size between designs:\n\nunrel_vs_sib = function(h2){\n  return(1+h2*(1-h2)/(4-h2))\n}\n\nunrel_vs_sib_inv = function(h2){\n  return((1+h2*(1-h2)/(4-h2))^(-1))\n}\n\nunrel_vs_trio = function(h2){\n  return(1+h2*(1-h2)/(3-h2*(1+h2/2)))\n}\n\nunrel_vs_trio_inv = function(h2){\n  return((1+h2*(1-h2)/(3-h2*(1+h2/2)))^(-1))\n}\n\nunrel_vs_direct_trio = function(h2){return(0.5/3)}\nunrel_vs_direct_sibdiff = function(h2){return((2*(2-h2))^(-1))}\nunrel_vs_direct_sibimp = function(h2){return((2+h2/2)/(6*(1-(h2/2)^2)))}\n\nFunction to estimate expected replication rate\n\nprop_overlap <- function(b_disc, b_rep, se_disc, se_rep, alpha)\n{\n  p_sign <- pnorm(-abs(b_disc) / se_disc) * pnorm(-abs(b_disc) / se_rep) + ((1 - pnorm(-abs(b_disc) / se_disc)) * (1 - pnorm(-abs(b_disc) / se_rep)))\n  p_sig <- pnorm(-abs(b_disc) / se_rep + qnorm(alpha / 2)) + (1 - pnorm(-abs(b_disc) / se_rep - qnorm(alpha / 2)))\n  p_rep <- pnorm(abs(b_rep)/se_rep, lower.tail=FALSE)\n  res <- tibble::tibble(\n    nsnp=length(b_disc),\n    metric=c(\"Sign\", \"Sign\", \"P-value\", \"P-value\"),\n    datum=c(\"Expected\", \"Observed\", \"Expected\", \"Observed\"),\n    value=c(sum(p_sign, na.rm=TRUE), sum(sign(b_disc) == sign(b_rep)), sum(p_sig, na.rm=TRUE), sum(p_rep < alpha, na.rm=TRUE))\n  )\n  pdif <- list(\n    Sign = binom.test(res$value[2], res$nsnp[1], res$value[1] / res$nsnp[1])$p.value,\n    `P-value` = binom.test(res$value[4], res$nsnp[1], res$value[3] / res$nsnp[1])$p.value\n  ) %>% as_tibble()\n  return(list(res=res, pdif=pdif, variants=dplyr::tibble(sig=p_sig, sign=p_sign)))\n}\n\nFunction to simulate assocs in siblings\n\nexpected_se <- function (beta, af, n, vy) \n{\n    sqrt(c(vy) - beta^2 * 2 * af * (1 - af))/sqrt(n) * (1/sqrt(2 * \n        af * (1 - af)))\n}\n\nexpected_vy <- function(beta, af, n, se)\n{\n  se^2 * 2 * af * (1 - af)*n + beta^2 * 2 * af * (1 - af)\n}\n\n# check algebra!\n# expected_se(0.2, 0.4, 1000, 10)\n# expected_vy(0.2, 0.4, 1000, expected_se(0.2, 0.4, 1000, 10))\n\nsimgwas_sib <- function(beta_disc, se_disc, af_disc, n_disc, prop_sibs, h2, alpha, shrinkage)\n{\n  d <- tibble(\n    beta_disc = beta_disc,\n    beta_true = beta_disc * shrinkage,\n    vy = expected_vy(beta_disc, af_disc, n_disc, se_disc),\n    n_rep = n_disc * prop_sibs * unrel_vs_direct_sibimp(h2),\n    se_rep = expected_se(beta_disc, af_disc, n_rep, vy),\n    beta_rep = rnorm(length(beta_disc), beta_true, se_rep)\n  ) \n  d %>%\n    {prop_overlap(.$beta_disc, .$beta_rep, se_disc, .$se_rep, alpha)}\n}\n\nPower analysis\n\nparams <- expand.grid(sim=1:500, shrinkage=seq(0,1,0.05))\nres <- lapply(1:nrow(params), function(i) \n  simgwas_sib(dat$beta, dat$se, dat$Effect.allele.frequency, dat$N, 20000/500000,0.5, 0.05/nrow(dat), params$shrinkage[i])\n  ) %>%\n  lapply(., function(x) x$pdif) %>% bind_rows() %>% bind_cols(., params)\nres\n\n# A tibble: 10,500 × 4\n        Sign `P-value`   sim shrinkage\n       <dbl>     <dbl> <int>     <dbl>\n 1 1.04e- 91  2.33e-11     1         0\n 2 3.19e- 95  2.33e-11     2         0\n 3 7.72e- 91  2.33e-11     3         0\n 4 4.13e- 89  2.33e-11     4         0\n 5 3.52e-105  2.33e-11     5         0\n 6 6.78e- 98  2.33e-11     6         0\n 7 1.85e- 93  2.33e-11     7         0\n 8 4.14e- 96  2.33e-11     8         0\n 9 1.76e-118  9.03e-10     9         0\n10 9.63e-112  2.33e-11    10         0\n# … with 10,490 more rows\n\n\n\nres %>% \n  group_by(shrinkage) %>%\n  summarise(Sign=sum(Sign < 0.05)/n(), `P-value`=sum(`P-value` < 0.05)/n()) %>%\n  tidyr::gather(key=\"key\", value=\"value\", c(`Sign`, `P-value`)) %>%\n  ggplot(., aes(x=shrinkage, y=value)) +\n    geom_point(aes(colour=key)) +\n    geom_line(aes(colour=key)) +\n    labs(x=\"Shrinkage of effect size, or 1-FDR\", y=\"Power to detect difference from expectation\")"
  },
  {
    "objectID": "posts/2022-12-14-wf-rare-variants/index.html#summary",
    "href": "posts/2022-12-14-wf-rare-variants/index.html#summary",
    "title": "Sibling replication of Backman rare variants",
    "section": "Summary",
    "text": "Summary\nPower is quite high to detect a difference!\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.0 dplyr_1.0.10  here_1.0.1   \n\nloaded via a namespace (and not attached):\n [1] pillar_1.8.1      compiler_4.2.1    tools_4.2.1       digest_0.6.31    \n [5] jsonlite_1.8.4    evaluate_0.19     lifecycle_1.0.3   tibble_3.1.8     \n [9] gtable_0.3.1      pkgconfig_2.0.3   rlang_1.0.6       cli_3.5.0        \n[13] DBI_1.1.3         yaml_2.3.6        xfun_0.36         fastmap_1.1.0    \n[17] withr_2.5.0       stringr_1.5.0     knitr_1.41        generics_0.1.3   \n[21] vctrs_0.5.1       htmlwidgets_1.5.4 rprojroot_2.0.3   grid_4.2.1       \n[25] tidyselect_1.2.0  glue_1.6.2        R6_2.5.1          fansi_1.0.3      \n[29] rmarkdown_2.16    farver_2.1.1      tidyr_1.2.1       purrr_1.0.0      \n[33] magrittr_2.0.3    ellipsis_0.3.2    scales_1.2.1      htmltools_0.5.4  \n[37] assertthat_0.2.1  colorspace_2.0-3  labeling_0.4.2    utf8_1.2.2       \n[41] stringi_1.7.8     munsell_0.5.0"
  },
  {
    "objectID": "posts/2022-08-12-correlated-snps/index.html",
    "href": "posts/2022-08-12-correlated-snps/index.html",
    "title": "Correlated SNPs",
    "section": "",
    "text": "One instrument for X and X has no influence on Y\n\nlibrary(TwoSampleMR)\n\nTwoSampleMR version 0.5.6 \n[>] New: Option to use non-European LD reference panels for clumping etc\n[>] Some studies temporarily quarantined to verify effect allele\n[>] See news(package='TwoSampleMR') and https://gwas.mrcieu.ac.uk for further details\n\nlibrary(simulateGP)\n\n\nAttaching package: 'simulateGP'\n\n\nThe following objects are masked from 'package:TwoSampleMR':\n\n    allele_frequency, contingency, get_population_allele_frequency\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nset.seed(12345)\nmap <- tibble(snp=1, af=0.5)\nparams_x <- generate_gwas_params(map=map, h2=0.01, S=-0.4, Pi=1)\nparams_y <- generate_gwas_params(map=map, h2=0.0, S=-0.4, Pi=1)\nnid <- 100000\nss <- summary_set(\n    beta_gx=params_x$beta,\n    beta_gy=params_y$beta,\n    af=params_x$af,\n    n_gx=10000,\n    n_gy=10000,\n    n_overlap=0,\n    cor_xy=0.5\n)\n\nPerform MR with single causal variant\n\nmr(ss) %>% glimpse()\n\nAnalysing 'X' on 'Y'\n\n\nRows: 1\nColumns: 9\n$ id.exposure <chr> \"X\"\n$ id.outcome  <chr> \"Y\"\n$ outcome     <chr> \"Y\"\n$ exposure    <chr> \"X\"\n$ method      <chr> \"Wald ratio\"\n$ nsnp        <dbl> 1\n$ b           <dbl> -0.08648138\n$ se          <dbl> 0.0892847\n$ pval        <dbl> 0.3327436\n\n\nPerform MR with causal variant + 100 correlated tag SNPs\n\nss2 <- ss[rep(1,100),] %>% mutate(SNP=1:100)\nmr(ss2, method_list=\"mr_ivw\") %>% glimpse()\n\nAnalysing 'X' on 'Y'\n\n\nRows: 1\nColumns: 9\n$ id.exposure <chr> \"X\"\n$ id.outcome  <chr> \"Y\"\n$ outcome     <chr> \"Y\"\n$ exposure    <chr> \"X\"\n$ method      <chr> \"Inverse variance weighted\"\n$ nsnp        <int> 100\n$ b           <dbl> -0.08648138\n$ se          <dbl> 0.00892847\n$ pval        <dbl> 3.457243e-22\n\n\nVery small p-value - inflated type 1 error"
  },
  {
    "objectID": "posts/2022-08-12-correlated-snps/index.html#grs-correlation-performance",
    "href": "posts/2022-08-12-correlated-snps/index.html#grs-correlation-performance",
    "title": "Correlated SNPs",
    "section": "GRS correlation performance",
    "text": "GRS correlation performance\nTag SNPs are perfectly correlated with causal variant\n\nsim <- function(nid=10000, nsnp=10)\n{\n  g <- matrix(0, nid, nsnp)\n  g[,1] <- rnorm(nid)\n  for(i in 2:nsnp)\n  {\n    g[,i] <- g[,1]\n  }\n  y <- g[,1] + rnorm(nid)\n  summary(lm(y ~ g[,1]))\n  grs <- rowSums(g)\n  return(c(cor(y, g[,1])^2, cor(y, grs)^2))\n}\nsapply(1:10, function(i) sim()) %>% rowMeans() %>% tibble(method=c(\"Causal variant only\", \"GRS\"), rsq=.)\n\n# A tibble: 2 × 2\n  method                rsq\n  <chr>               <dbl>\n1 Causal variant only 0.500\n2 GRS                 0.500\n\n\nGRS and single causal variant work the same as Jack showed.\nTag SNPs are imperfectly correlated with causal variant\n\nsim <- function(nid=10000, nsnp=10)\n{\n  g <- matrix(0, nid, nsnp)\n  g[,1] <- rnorm(nid)\n  for(i in 2:nsnp)\n  {\n    g[,i] <- g[,i] + rnorm(nid, sd=0.5)\n  }\n  y <- g[,1] + rnorm(nid)\n  summary(lm(y ~ g[,1]))\n  grs <- rowSums(g)\n  return(c(cor(y, g[,1])^2, cor(y, grs)^2))\n}\nsapply(1:10, function(i) sim()) %>% rowMeans() %>% tibble(method=c(\"Causal variant only\", \"GRS\"), rsq=.)\n\n# A tibble: 2 × 2\n  method                rsq\n  <chr>               <dbl>\n1 Causal variant only 0.500\n2 GRS                 0.156\n\n\nNow the GRS doesn’t work well because it includes the variance of the SNP + noise that isn’t causally related to the trait.\n\\[\nr^2 = \\frac{cov(grs, y)^2}{var(grs) var(y)}\n\\]\ni.e. cov(grs,y) isn’t increasing, but (var(y)) is."
  },
  {
    "objectID": "posts/2022-08-12-correlated-snps/index.html#checking",
    "href": "posts/2022-08-12-correlated-snps/index.html#checking",
    "title": "Correlated SNPs",
    "section": "Checking",
    "text": "Checking\n\nn <- 10000\nnsnp <- 10\ng <- matrix(0, n, nsnp)\ng[,1] <- rnorm(n)\nfor(i in 2:nsnp)\n{\n  g[,i] <- g[,i] + rnorm(n, sd=0.5)\n}\ny <- g[,1] + rnorm(n)\ngrs <- rowSums(g)\ncov(y, grs)\n\n[1] 1.003715\n\ncov(y, g[,1])\n\n[1] 0.9788621\n\nsd(grs)\n\n[1] 1.805151\n\nsd(g[,1])\n\n[1] 0.996039\n\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dplyr_1.0.10      simulateGP_0.1.2  TwoSampleMR_0.5.6\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9        plyr_1.8.7        compiler_4.2.1    pillar_1.8.1     \n [5] iterators_1.0.14  tools_4.2.1       mr.raps_0.2       digest_0.6.29    \n [9] jsonlite_1.8.0    evaluate_0.16     lifecycle_1.0.3   tibble_3.1.8     \n[13] lattice_0.20-45   pkgconfig_2.0.3   rlang_1.0.6       Matrix_1.4-1     \n[17] foreach_1.5.2     DBI_1.1.3         cli_3.4.1         yaml_2.3.5       \n[21] xfun_0.33         fastmap_1.1.0     stringr_1.4.1     knitr_1.40       \n[25] generics_0.1.3    htmlwidgets_1.5.4 vctrs_0.5.1       tidyselect_1.1.2 \n[29] glmnet_4.1-4      grid_4.2.1        nortest_1.0-4     glue_1.6.2       \n[33] R6_2.5.1          fansi_1.0.3       survival_3.4-0    rmarkdown_2.16   \n[37] purrr_0.3.4       magrittr_2.0.3    ellipsis_0.3.2    codetools_0.2-18 \n[41] htmltools_0.5.3   splines_4.2.1     assertthat_0.2.1  shape_1.4.6      \n[45] utf8_1.2.2        stringi_1.7.8"
  },
  {
    "objectID": "posts/2022-12-18-bmi-wf-replication/index.html",
    "href": "posts/2022-12-18-bmi-wf-replication/index.html",
    "title": "BMI instrument replication",
    "section": "",
    "text": "Liza’s analysis of BMI instruments clusters them by PheWAS and finds that cluster 4 relates to SES, and drives BMI-EDU biased effect. Are the instruments in cluster 4 solely due to dynastic confounding? If so they should fail to replicate in the sibling analysis."
  },
  {
    "objectID": "posts/2022-12-18-bmi-wf-replication/index.html#instruments",
    "href": "posts/2022-12-18-bmi-wf-replication/index.html#instruments",
    "title": "BMI instrument replication",
    "section": "Instruments",
    "text": "Instruments\n\nlibrary(ieugwasr)\n\nAPI: public: http://gwas-api.mrcieu.ac.uk/\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nbmi_inst <- list(\n  c(\"rs1097327\",\"rs2186120\",\"rs2166172\",\"rs75641275\",\"rs12037698\",\"rs1446585\",\"rs16846140\",\"rs13062093\",\"rs2051559\",\"rs6861649\",\"rs2281819\",\"rs12662900\",\"rs9388681\",\"rs17132130\",\"rs215634\",\"rs79682948\",\"rs2192649\",\"rs13294945\",\"rs7357754\",\"rs4749937\",\"rs1465900\",\"rs1799992\",\"rs55938344\",\"rs7987928\",\"rs7331420\",\"rs9522279\",\"rs55689274\",\"rs4777541\",\"rs7189149\",\"rs11079849\",\"rs113230003\",\"rs150998792\"),\n  c(\"rs3737992\",\"rs1167311\",\"rs12140153\",\"rs34361149\",\"rs12049202\",\"rs2181375\",\"rs17024393\",\"rs61813324\",\"rs815163\",\"rs2678204\",\"rs17014332\",\"rs563738408\",\"rs6751993\",\"rs4671328\",\"rs6545714\",\"rs4482463\",\"rs815715\",\"rs6769617\",\"rs1225004\",\"rs355777\",\"rs6776471\",\"rs869400\",\"rs4261944\",\"rs1296328\",\"rs6867471\",\"rs2307111\",\"rs28404639\",\"rs7442885\",\"rs55838622\",\"rs13174863\",\"rs146696797\",\"rs9368828\",\"rs34045288\",\"rs72892910\",\"rs2482398\",\"rs2253310\",\"rs765874\",\"rs9688977\",\"rs6950388\",\"rs4722398\",\"rs1470749\",\"rs147678035\",\"rs39330\",\"rs1805123\",\"rs6601527\",\"rs791405\",\"rs12679106\",\"rs201519328\",\"rs9297524\",\"rs17770336\",\"rs2440589\",\"rs3931548\",\"rs3861879\",\"rs2356376\",\"rs17399739\",\"rs6265\",\"rs491711\",\"rs7942037\",\"rs10898330\",\"rs12364470\",\"rs55726687\",\"rs1458156\",\"rs7138383\",\"rs147730268\",\"rs9507895\",\"rs10507483\",\"rs11148421\",\"rs1949204\",\"rs1441264\",\"rs7990098\",\"rs8015400\",\"rs7141420\",\"rs1286138\",\"rs6575340\",\"rs12881629\",\"rs3803286\",\"rs7183417\",\"rs4776970\",\"rs2870111\",\"rs2046002\",\"rs56803094\",\"rs12926311\",\"rs879620\",\"rs7193783\",\"rs4402589\",\"rs11642015\",\"rs117342986\",\"rs11150461\",\"rs4790841\",\"rs56161855\",\"rs11150745\",\"rs9319615\",\"rs57636386\",\"rs111640872\",\"rs3810291\",\"rs6050446\",\"rs67844506\",\"rs6001870\"),\n  c(\"rs12024554\",\"rs35722922\",\"rs593010\",\"rs10803762\",\"rs6772763\",\"rs80082351\",\"rs1471740\",\"rs2606228\",\"rs13107325\",\"rs13176429\",\"rs1919243\",\"rs286818\",\"rs3844598\",\"rs7755574\",\"rs3843540\",\"rs17716502\",\"rs1411432\",\"rs2267958\",\"rs2450447\",\"rs11826177\",\"rs7124681\",\"rs317687\",\"rs3897102\",\"rs9506311\",\"rs35193668\",\"rs3759584\",\"rs862320\",\"rs7774\",\"rs8078135\",\"rs11653258\",\"rs56212061\",\"rs11084554\",\"rs2153740\",\"rs8134638\",\"rs2837398\"),\n  c(\"rs115866895\",\"rs6687953\",\"rs935166\",\"rs13002946\",\"rs72820274\",\"rs12619626\",\"rs13427822\",\"rs72967047\",\"rs9843653\",\"rs1454687\",\"rs34811474\",\"rs17085463\",\"rs1383723\",\"rs35853157\",\"rs1503526\",\"rs1477290\",\"rs9463511\",\"rs9277992\",\"rs236660\",\"rs2045293\",\"rs10954772\",\"rs77883185\",\"rs35529153\",\"rs11782074\",\"rs2398861\",\"rs7030732\",\"rs61845249\",\"rs4595495\",\"rs61903695\",\"rs1048932\",\"rs2292238\",\"rs116394958\",\"rs2933223\",\"rs217672\",\"rs2333012\",\"rs11855853\",\"rs62037365\",\"rs34966008\",\"rs1788808\",\"rs784257\",\"rs11666480\"),\n  c(\"rs4648450\",\"rs12031634\",\"rs1778830\",\"rs61826867\",\"rs170553\",\"rs10185199\",\"rs6545144\",\"rs3806572\",\"rs2861685\",\"rs7557796\",\"rs72844755\",\"rs115584509\",\"rs35882248\",\"rs34373881\",\"rs4377469\",\"rs75557510\",\"rs2035936\",\"rs66679256\",\"rs73213484\",\"rs6831020\",\"rs11099020\",\"rs750090\",\"rs6536575\",\"rs7701777\",\"rs9291822\",\"rs10059453\",\"rs2118793\",\"rs4921301\",\"rs4467770\",\"rs7453694\",\"rs9342196\",\"rs9489620\",\"rs13210756\",\"rs3807652\",\"rs7810870\",\"rs7461253\",\"rs2616192\",\"rs12681792\",\"rs55781253\",\"rs13289199\",\"rs2254331\",\"rs1327808\",\"rs6597653\",\"rs2439823\",\"rs6591\",\"rs75936055\",\"rs34292685\",\"rs7940866\",\"rs329651\",\"rs12422552\",\"rs4761401\",\"rs1901241\",\"rs11613680\",\"rs9579775\",\"rs7995015\",\"rs9527895\",\"rs9522183\",\"rs145946602\",\"rs8027969\",\"rs117632017\",\"rs113182412\",\"rs756717\",\"rs11656076\",\"rs2332306\",\"rs7237783\",\"rs1942826\",\"rs45486197\",\"rs76040172\",\"rs28489620\"),\n  c(\"rs61743745\",\"rs10921760\",\"rs2141004\",\"rs754481\",\"rs80330591\",\"rs199750218\",\"rs12479357\",\"rs4485556\",\"rs2569993\",\"rs9847186\",\"rs13076052\",\"rs762705\",\"rs550669262\",\"rs10865612\",\"rs6780459\",\"rs73169730\",\"rs10938397\",\"rs35851183\",\"rs190301182\",\"rs1428120\",\"rs245775\",\"rs1775255\",\"rs2749929\",\"rs6973656\",\"rs12537134\",\"rs1425717\",\"rs11012732\",\"rs12260817\",\"rs112921972\",\"rs61871615\",\"rs10749233\",\"rs845084\",\"rs67609008\",\"rs2035806\",\"rs12575252\",\"rs555754158\",\"rs2234458\",\"rs704061\",\"rs13353100\",\"rs7498044\",\"rs2342892\",\"rs55931203\",\"rs60764613\",\"rs2155869\",\"rs1389067\",\"rs2247593\",\"rs273505\",\"rs10404726\",\"rs112693590\")\n)\n\nExtract instruments from the population and sibling GWASs\n\nwfest <- ieugwasr::associations(unlist(bmi_inst), \"ieu-b-4815\")\npopest <- ieugwasr::associations(unlist(bmi_inst), \"ieu-b-4816\")\ngiantest <- ieugwasr::associations(unlist(bmi_inst), \"ieu-b-40\")\nukbest <- ieugwasr::associations(unlist(bmi_inst), \"ukb-b-19953\")\n\nReplication rates function\n\nexp_rep <- function(b_disc, b_rep, se_disc, se_rep, alpha)\n{\n  p_sign <- pnorm(-abs(b_disc) / se_disc) * pnorm(-abs(b_disc) / se_rep) + ((1 - pnorm(-abs(b_disc) / se_disc)) * (1 - pnorm(-abs(b_disc) / se_rep)))\n  p_sig <- pnorm(-abs(b_disc) / se_rep + qnorm(alpha / 2)) + (1 - pnorm(-abs(b_disc) / se_rep - qnorm(alpha / 2)))\n  p_rep <- pnorm(abs(b_rep)/se_rep, lower.tail=FALSE)\n  res <- tibble::tibble(\n    nsnp=length(b_disc),\n    metric=c(\"Sign\", \"Sign\", \"P-value\", \"P-value\"),\n    datum=c(\"Expected\", \"Observed\", \"Expected\", \"Observed\"),\n    value=c(sum(p_sign, na.rm=TRUE), sum(sign(b_disc) == sign(b_rep)), sum(p_sig, na.rm=TRUE), sum(p_rep < alpha, na.rm=TRUE))\n  )\n  return(list(res=res, variants=dplyr::tibble(sig=p_sig, sign=p_sign)))\n}\n\nAnalysis\n\nests <- bind_rows(giantest, wfest, popest, ukbest) %>%\n  mutate(cluster=NA)\nfor(i in 1:length(bmi_inst))\n{\n  ests$cluster[ests$rsid %in% bmi_inst[[i]]] <- i\n}\n\nests %>%\n  group_by(id, cluster) %>%\n  summarise(n=n(), psig = sum(p < 5e-3)/n) %>%\n  ggplot(., aes(x=as.factor(cluster), y=psig)) +\n  geom_bar(position=\"dodge\", stat=\"identity\", aes(fill=id))\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\n\n\n\n\nExpected vs observed replication rates\n\no <- lapply(1:length(bmi_inst), function(i)\n{\n  x <- bmi_inst[[i]]\n  dat <- inner_join(\n    subset(popest, rsid %in% x),\n    subset(wfest, rsid %in% x),\n    by=\"rsid\"\n  )\n  exp_rep(dat$beta.x, dat$beta.y, dat$se.x, dat$se.y, 1e-3)[[1]] %>%\n    mutate(cluster=i)\n})\no %>% bind_rows() %>%\n  ggplot(aes(x=as.factor(cluster), y=value/nsnp)) +\n  geom_point(aes(colour=datum)) +\n  facet_grid(. ~ metric) +\n  labs(x=\"Cluster\", y=\"Fraction of sig. instruments\", colour=\"\")"
  },
  {
    "objectID": "posts/2022-12-18-bmi-wf-replication/index.html#summary",
    "href": "posts/2022-12-18-bmi-wf-replication/index.html#summary",
    "title": "BMI instrument replication",
    "section": "Summary",
    "text": "Summary\nAll clusters appear to replicate as expected in the within-family GWAS, which is consistent with there being almost no shrinkage of the effect sizes."
  },
  {
    "objectID": "posts/2022-12-18-bmi-wf-replication/index.html#childhood-vs-adulthood-relationship-to-ses",
    "href": "posts/2022-12-18-bmi-wf-replication/index.html#childhood-vs-adulthood-relationship-to-ses",
    "title": "BMI instrument replication",
    "section": "Childhood vs adulthood relationship to SES",
    "text": "Childhood vs adulthood relationship to SES\n\nlibrary(TwoSampleMR)\n\nTwoSampleMR version 0.5.6 \n[>] New: Option to use non-European LD reference panels for clumping etc\n[>] Some studies temporarily quarantined to verify effect allele\n[>] See news(package='TwoSampleMR') and https://gwas.mrcieu.ac.uk for further details\n\n\n\nAttaching package: 'TwoSampleMR'\n\n\nThe following object is masked from 'package:ieugwasr':\n\n    ld_matrix\n\nchild <- make_dat(\"ukb-b-10011\", \"ebi-a-GCST90002409\")\n\nExtracting data for 18 SNP(s) from 1 GWAS(s)\n\n\nHarmonising Townsend deprivation index at recruitment || id:ukb-b-10011 (ukb-b-10011) and Childhood body mass index || id:ebi-a-GCST90002409 (ebi-a-GCST90002409)\n\nmr(child) %>% select(nsnp, b, se, pval)\n\nAnalysing 'ukb-b-10011' on 'ebi-a-GCST90002409'\n\n\n  nsnp           b        se      pval\n1   18  0.00605344 1.4172450 0.9966448\n2   18 -0.07468209 0.2430043 0.7585935\n3   18 -0.05801819 0.2184155 0.7905224\n4   18 -0.11644263 0.4970446 0.8175736\n5   18 -0.11644263 0.5171974 0.8245533\n\n\n\nadult <- make_dat(\"ukb-b-10011\", \"ukb-b-19953\")\n\nExtracting data for 18 SNP(s) from 1 GWAS(s)\n\n\nHarmonising Townsend deprivation index at recruitment || id:ukb-b-10011 (ukb-b-10011) and Body mass index (BMI) || id:ukb-b-19953 (ukb-b-19953)\n\nmr(adult) %>% select(nsnp, b, se, pval)\n\nAnalysing 'ukb-b-10011' on 'ukb-b-19953'\n\n\n  nsnp          b         se         pval\n1   18 -0.2812238 0.80612571 7.317440e-01\n2   18  0.3450191 0.08009526 1.650282e-05\n3   18  0.4835539 0.13327136 2.852487e-04\n4   18  0.3588983 0.16017759 3.869475e-02\n5   18  0.3128024 0.11209131 1.255107e-02\n\n\nReplication of clustered instruments from adult to child\n\nukbest <- ieugwasr::associations(unlist(bmi_inst), \"ukb-b-19953\")\nchildest <- ieugwasr::associations(unlist(bmi_inst), \"ebi-a-GCST90002409\")\n\n\no1 <- lapply(1:length(bmi_inst), function(i)\n{\n  x <- bmi_inst[[i]]\n  dat <- inner_join(\n    subset(ukbest, rsid %in% x),\n    subset(childest, rsid %in% x),\n    by=\"rsid\"\n  )\n  exp_rep(dat$beta.x, dat$beta.y, dat$se.x, dat$se.y, 1e-3)[[1]] %>%\n    mutate(cluster=i)\n})\no1 %>% bind_rows() %>%\n  ggplot(aes(x=as.factor(cluster), y=value/nsnp)) +\n  geom_point(aes(colour=datum)) +\n  facet_grid(. ~ metric) +\n  labs(x=\"Cluster\", y=\"Fraction of sig. instruments\", colour=\"\")\n\n\n\n\n\nSummary\n\nSES has an influence on BMI in adulthood but not childhood\nThe replication rate amongst clusters appears to be relatively consistent, except cluster 2 replicates particularly well"
  },
  {
    "objectID": "posts/2023-06-07-cross-group-effect-comparison/2023-06-09-interaction-distribution/index.html",
    "href": "posts/2023-06-07-cross-group-effect-comparison/2023-06-09-interaction-distribution/index.html",
    "title": "Differential susceptibility explained by interactors and mediators with different distributions",
    "section": "",
    "text": "What leads to different effect estimates in different populations? If the DAG is the same, then different distributions of mediators and interactions could explain different effects\n\nn <- 100000\n\ng1 <- rbinom(n, 2, 0.3)\ng2 <- rbinom(n, 2, 0.3)\n\n# The liability for the mediator \n# Influenced by g (same effect in both populations)\n# Influenced by some other variable that gives it a different mean in the two populations\n# Now liability of mediator has different means\nml1 <- plogis(scale(g1) + rnorm(n, 2, 1))\nml2 <- plogis(scale(g2) + rnorm(n, 0, 1))\n\nhist(ml1)\n\n\n\nhist(ml2)\n\n\n\n\n\n# Generate mediators - they have different means\nm1 <- rbinom(n, 1, ml1)\nm2 <- rbinom(n, 1, ml2)\nmean(m1)\n\n[1] 0.81663\n\nmean(m2)\n\n[1] 0.49808\n\n\nDisease influenced by mediator and noise (same effects)\n\nd1 <- rbinom(n, 1, plogis(m1 + rnorm(n)))\nd2 <- rbinom(n, 1, plogis(m2 + rnorm(n)))\n\nmean(d1)\n\n[1] 0.66026\n\nmean(d2)\n\n[1] 0.59796\n\n\nSlightly different prevalence in different populations.\nThe genotypic effect is now different in the two populations due to different\n\nglm(d1 ~ g1, family=\"binomial\")\n\n\nCall:  glm(formula = d1 ~ g1, family = \"binomial\")\n\nCoefficients:\n(Intercept)           g1  \n     0.5736       0.1538  \n\nDegrees of Freedom: 99999 Total (i.e. Null);  99998 Residual\nNull Deviance:      128200 \nResidual Deviance: 128000   AIC: 128000\n\nglm(d2 ~ g2, family=\"binomial\")\n\n\nCall:  glm(formula = d2 ~ g2, family = \"binomial\")\n\nCoefficients:\n(Intercept)           g2  \n     0.2551       0.2405  \n\nDegrees of Freedom: 99999 Total (i.e. Null);  99998 Residual\nNull Deviance:      134800 \nResidual Deviance: 134200   AIC: 134200\n\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.0    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.3.0       htmltools_0.5.5   rstudioapi_0.14   yaml_2.3.7       \n [9] rmarkdown_2.22    knitr_1.43        xfun_0.39         digest_0.6.31    \n[13] jsonlite_1.8.5    rlang_1.1.1       evaluate_0.21"
  },
  {
    "objectID": "posts/2023-06-07-cross-group-effect-comparison/index.html",
    "href": "posts/2023-06-07-cross-group-effect-comparison/index.html",
    "title": "Cross group effect comparison",
    "section": "",
    "text": "GWAS being performed on multiple ancestries, then meta-analysing to mitigate problems of LD tagging. QUESTION - how to determine rate of agreement of associations across ancestries. Power differs due to different allele frequencies and sample sizes (reflected in the SE of the assoc).\n\nHow often does the sign in pop1 agree with the sign in pop2?\nTest statistic comparing observed vs replication rates\n\nSimulate 100 effects for 3 populations. For each population select a few SNPs to be null, otherwise all SNPs have the same effect in each population. Each population has a different sample size.\n\nlibrary(simulateGP)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)\nmap <- tibble(snp=paste0(\"rs\",1:100), af=runif(100, 0.01, 0.99))\nparams <- generate_gwas_params(map=map, h2=0.2, S=-0.4, Pi=1)\n\ntemp <- params; temp$beta[1:10] <- 0\nss_eur <- generate_gwas_ss(temp, 100000)\n\ntemp <- params; temp$beta[11:20] <- 0\nss_afr <- generate_gwas_ss(temp, 10000)\n\ntemp <- params; temp$beta[15:30] <- 0\nss_sas <- generate_gwas_ss(temp, 30000)\n\nSo now each population has SNP effect estimates for 100 SNPs, an example of what the dataset looks like:\n\nss_eur\n\n# A tibble: 100 × 7\n   snp      af      se     bhat   fval      n  pval\n   <chr> <dbl>   <dbl>    <dbl>  <dbl>  <dbl> <dbl>\n 1 rs1   0.330 0.00476 -0.00147 0.0960 100000 0.757\n 2 rs2   0.229 0.00532 -0.00110 0.0430 100000 0.836\n 3 rs3   0.133 0.00658  0.0100  2.33   100000 0.127\n 4 rs4   0.417 0.00454  0.00134 0.0873 100000 0.768\n 5 rs5   0.190 0.00570 -0.00276 0.235  100000 0.628\n 6 rs6   0.362 0.00465  0.00309 0.442  100000 0.506\n 7 rs7   0.273 0.00502 -0.00535 1.13   100000 0.287\n 8 rs8   0.497 0.00447  0.00192 0.184  100000 0.668\n 9 rs9   0.195 0.00564 -0.00308 0.298  100000 0.585\n10 rs10  0.426 0.00452  0.00524 1.34   100000 0.246\n# ℹ 90 more rows"
  },
  {
    "objectID": "posts/2023-06-07-cross-group-effect-comparison/index.html#heterogeneity",
    "href": "posts/2023-06-07-cross-group-effect-comparison/index.html#heterogeneity",
    "title": "Cross group effect comparison",
    "section": "Heterogeneity",
    "text": "Heterogeneity\nFor each SNP test for heterogeneity of effects between populations. This uses Cochrane’s Q test statistic.\nAnalysis functions:\n\nfixed_effects_meta_analysis <- function(beta_vec, se_vec)\n{\n    w <- 1 / se_vec^2\n    beta <- sum(beta_vec * w) / sum(w)\n    se <- sqrt(1 / sum(w))\n    pval <- pnorm(abs(beta / se), lower.tail = FALSE)\n    Qj <- w * (beta-beta_vec)^2\n    Q <- sum(Qj)\n    Qdf <- length(beta_vec)-1\n    Qjpval <- pchisq(Qj, 1, lower.tail=FALSE)\n    Qpval <- pchisq(Q, Qdf, lower.tail=FALSE)\n    return(list(beta=beta, se=se, Qpval=Qpval, Qj=Qj, Qjpval=Qjpval))\n}\n# fixed_effects_meta_analysis(c(1,2,3), c(0.3, 0.3, 0.3))\n# fixed_effects_meta_analysis(c(1,1,1), c(0.3, 0.3, 0.3))\n\n#' Test for heterogeneity of effect estimates between populations\n#' \n#' @description For each SNP this function will provide a Cochran's Q test statistic - a measure of heterogeneity of effect sizes between populations. A low p-value means high heterogeneity.\n#' In addition, for every SNP it gives a per population p-value - this can be interpreted as asking for each SNP is a particular giving an outlier estimate.\n#' \n#' @param sslist Named list of data frames, one for each population, with at least bhat, se and snp columns\n#' \n#' @return List\n#' - Q = vector of p-values for Cochrane's Q statistic for each SNP\n#' - Qj = Data frame of per-population outlier q values for each SNP\nheterogeneity_test <- function(sslist) \n{\n    b <- lapply(sslist, \\(x) x$bhat) %>% bind_cols\n    se <- lapply(sslist, \\(x) x$se) %>% bind_cols\n    o <- lapply(1:nrow(b), \\(i) {\n        fixed_effects_meta_analysis(as.numeric(b[i,]), as.numeric(se[i,]))\n    })\n    Q <- tibble(snp = sslist[[1]]$snp, Qpval = sapply(o, \\(x) x$Qpval))\n    Qj <- lapply(o, \\(x) x$Qjpval) %>% do.call(rbind, .) %>% \n        as_tibble() %>%\n        rename(setNames(paste0(\"V\", 1:length(sslist)), names(sslist))) %>%\n        mutate(snp = sslist[[1]]$snp)\n    return(list(Q=Q, Qj=Qj))\n}\n\nRun analysis:\n\no <- heterogeneity_test(list(eur=ss_eur, afr=ss_afr, sas=ss_sas))\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\no$Q\n\n# A tibble: 100 × 2\n   snp      Qpval\n   <chr>    <dbl>\n 1 rs1   1.19e-18\n 2 rs2   1.00e-16\n 3 rs3   4.70e-27\n 4 rs4   3.01e- 6\n 5 rs5   2.31e-12\n 6 rs6   1.31e- 2\n 7 rs7   1.75e- 8\n 8 rs8   3.59e- 3\n 9 rs9   1.94e- 4\n10 rs10  9.73e- 1\n# ℹ 90 more rows\n\nggplot(o$Q, aes(x=snp, y=-log10(Qpval))) +\ngeom_point() +\ngeom_hline(yintercept=-log10(0.05/nrow(o$Q)))\n\n\n\n\nThis shows the heterogeneity for each SNP. Alternatively we could look at the contribution of each population to the heterogeneity of each SNP\n\ngather(as.data.frame(o$Qj), \"key\", \"value\", -snp) %>%\nggplot(., aes(x=snp, y=-log10(value))) +\ngeom_point(aes(colour=key)) +\nscale_colour_brewer(type=\"qual\")\n\n\n\n\nWe could then dig deeper and look at the per-population estimates for the SNPs that have some heterogeneity\n\n# Which SNPs have heterogeneity after multiple testing correction\nindex <- which(o$Q$Qpval < 0.05/nrow(o$Q))\n\n# Get per-population effect estimates for those SNPs\n# Make forest plots\nbind_rows(\n    ss_eur[index,] %>% mutate(pop=\"eur\"),\n    ss_afr[index,] %>% mutate(pop=\"afr\"),\n    ss_sas[index,] %>% mutate(pop=\"sas\")\n) %>% ggplot(., aes(x=bhat, y=pop)) +\ngeom_point(aes(colour=pop)) +\ngeom_errorbarh(aes(colour=pop, xmin=bhat-se*1.96, xmax=bhat+se*1.96), height=0) +\nfacet_grid(snp ~ .) +\ngeom_vline(xintercept=0, linetype=\"dotted\")"
  },
  {
    "objectID": "posts/2023-06-07-cross-group-effect-comparison/index.html#expected-vs-observed-replication",
    "href": "posts/2023-06-07-cross-group-effect-comparison/index.html#expected-vs-observed-replication",
    "title": "Cross group effect comparison",
    "section": "Expected vs observed replication",
    "text": "Expected vs observed replication\nThis might not be as useful as the heterogeneity stuff above, but some example code below\n\n#' Expected vs observed replication rates\n#' \n#' @description For a set of effects that have discovery and replication betas and SEs, this function determines the extent to which the observed replication rate matches the expected replication rate. \n#' The expected replication rate is based on the assumption that the replication dataset has the same effect sizes but that the power may be different (e.g. due to allele frequencies or sample sizes) and is reflected in the replication standard errors. \n#' It assesses replication based on concordance of effect direction across discovery and replication, and p-values surpassing a user-specified p-value threshold.\n#' \n#' @param b_disc vector of clumped incidence hit effects\n#' @param se_disc the standard errors for incidence effects\n#' @param b_rep corresponding vector of associations in progression\n#' @param se_rep standard errors of effects in progression\n#' @param alpha p-value threshold to check for replication of incidence hits in progression (e.g. try 0.05 or 1e-5)\nexpected_vs_observed_replication <- function(b_disc, b_rep, se_disc, se_rep, alpha)\n{\n    p_sign <- pnorm(-abs(b_disc) / se_disc) * pnorm(-abs(b_disc) / se_rep) + ((1 - pnorm(-abs(b_disc) / se_disc)) * (1 - pnorm(-abs(b_disc) / se_rep)))\n    p_sig <- pnorm(-abs(b_disc) / se_rep + qnorm(alpha / 2)) + (1 - pnorm(-abs(b_disc) / se_rep - qnorm(alpha / 2)))\n    p_rep <- pnorm(abs(b_rep)/se_rep, lower.tail=FALSE)\n    res <- tibble::tibble(\n        nsnp=length(b_disc),\n        metric=c(\"Sign\", \"Sign\", \"P-value\", \"P-value\"),\n        datum=c(\"Expected\", \"Observed\", \"Expected\", \"Observed\"),\n        value=c(sum(p_sign, na.rm=TRUE), sum(sign(b_disc) == sign(b_rep)), sum(p_sig, na.rm=TRUE), sum(p_rep < alpha, na.rm=TRUE)),\n        pdiff=c(NA_real_, binom.test(value[2], nsnp[2], value[1]/nsnp[2])$p.value, NA_real_, binom.test(value[4], nsnp[4], value[3]/nsnp[4])$p.value)\n    )\n    res_per_variant <- tibble(\n        expected_pval = p_sig,\n        observed_pval = p_rep < alpha,\n        replication_fail = expected_pval > 0.95 & ! observed_pval,\n        expected_sign = p_sign,\n        observed_sign = sign(b_disc) == sign(b_rep),\n        sign_fail = expected_sign > 0.95 & ! observed_sign\n    )\n    return(list(res=res, variants=res_per_variant))\n}\n\nforest_plot <- function(sslist, snp)\n{\n    tibble(\n        beta = sapply(sslist, \\(x) x$bhat[snp]),\n        se = sapply(sslist, \\(x) x$se[snp]),\n        label = names(sslist)\n    ) %>%\n    ggplot(., aes(x=beta, y=label)) +\n    geom_point() +\n    geom_errorbarh(aes(xmin=beta-se*1.96, xmax=beta+se*1.96), height=0) +\n    geom_vline(xintercept=0, linetype=\"dotted\") +\n    labs(x=\"beta\", y=\"population\")\n}\n\n\nindex <- which(ss_eur$pval < 5e-8)\no_eur_afr <- expected_vs_observed_replication(ss_eur$bhat[index], ss_afr$bhat[index], ss_eur$se[index], ss_afr$se[index], 0.05)\no_eur_afr\n\n$res\n# A tibble: 4 × 5\n   nsnp metric  datum    value       pdiff\n  <int> <chr>   <chr>    <dbl>       <dbl>\n1    59 Sign    Expected  58.7 NA         \n2    59 Sign    Observed  54    0.00000976\n3    59 P-value Expected  52.1 NA         \n4    59 P-value Observed  48    0.103     \n\n$variants\n# A tibble: 59 × 6\n   expected_pval observed_pval replication_fail expected_sign observed_sign\n           <dbl> <lgl>         <lgl>                    <dbl> <lgl>        \n 1         1.00  TRUE          FALSE                    1     TRUE         \n 2         1.00  FALSE         TRUE                     1.00  FALSE        \n 3         1.00  TRUE          FALSE                    1.00  FALSE        \n 4         0.996 FALSE         TRUE                     1.00  TRUE         \n 5         1.00  FALSE         TRUE                     1.00  TRUE         \n 6         0.940 FALSE         FALSE                    1.00  FALSE        \n 7         0.765 FALSE         FALSE                    0.996 FALSE        \n 8         1     FALSE         TRUE                     1     FALSE        \n 9         1.00  TRUE          FALSE                    1     TRUE         \n10         0.996 TRUE          FALSE                    1.00  TRUE         \n# ℹ 49 more rows\n# ℹ 1 more variable: sign_fail <lgl>\n\n\n\nforest_plot(list(eur=ss_eur, afr=ss_afr, sas=ss_sas), index[which(o_eur_afr$variants$sign_fail)[3]])\n\n\n\n\n\nindex <- which(ss_afr$pval < 5e-8)\no_afr_sas <- expected_vs_observed_replication(ss_afr$bhat[index], ss_afr$bhat[index], ss_sas$se[index], ss_sas$se[index], 0.05)\no_afr_sas\n\n$res\n# A tibble: 4 × 5\n   nsnp metric  datum    value pdiff\n  <int> <chr>   <chr>    <dbl> <dbl>\n1    21 Sign    Expected    21    NA\n2    21 Sign    Observed    21     1\n3    21 P-value Expected    21    NA\n4    21 P-value Observed    21     1\n\n$variants\n# A tibble: 21 × 6\n   expected_pval observed_pval replication_fail expected_sign observed_sign\n           <dbl> <lgl>         <lgl>                    <dbl> <lgl>        \n 1             1 TRUE          FALSE                        1 TRUE         \n 2             1 TRUE          FALSE                        1 TRUE         \n 3             1 TRUE          FALSE                        1 TRUE         \n 4             1 TRUE          FALSE                        1 TRUE         \n 5             1 TRUE          FALSE                        1 TRUE         \n 6             1 TRUE          FALSE                        1 TRUE         \n 7             1 TRUE          FALSE                        1 TRUE         \n 8             1 TRUE          FALSE                        1 TRUE         \n 9             1 TRUE          FALSE                        1 TRUE         \n10             1 TRUE          FALSE                        1 TRUE         \n# ℹ 11 more rows\n# ℹ 1 more variable: sign_fail <lgl>\n\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.2    tidyr_1.3.0      dplyr_1.1.2      simulateGP_0.1.2\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2        cli_3.6.1          knitr_1.43         rlang_1.1.1       \n [5] xfun_0.39          purrr_1.0.1        generics_0.1.3     jsonlite_1.8.4    \n [9] labeling_0.4.2     glue_1.6.2         colorspace_2.1-0   htmltools_0.5.5   \n[13] scales_1.2.1       fansi_1.0.4        rmarkdown_2.22     grid_4.3.0        \n[17] munsell_0.5.0      evaluate_0.21      tibble_3.2.1       fastmap_1.1.1     \n[21] yaml_2.3.7         lifecycle_1.0.3    compiler_4.3.0     RColorBrewer_1.1-3\n[25] htmlwidgets_1.6.2  pkgconfig_2.0.3    rstudioapi_0.14    farver_2.1.1      \n[29] digest_0.6.31      R6_2.5.1           tidyselect_1.2.0   utf8_1.2.3        \n[33] pillar_1.9.0       magrittr_2.0.3     withr_2.5.0        gtable_0.3.3      \n[37] tools_4.3.0"
  },
  {
    "objectID": "posts/2023-05-13-mr-confounder/index.html",
    "href": "posts/2023-05-13-mr-confounder/index.html",
    "title": "Genetic confounding as a function of sample size",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(simulateGP)\nlibrary(dplyr)\nlibrary(TwoSampleMR)\nlibrary(purrr)\nlibrary(pwr)"
  },
  {
    "objectID": "posts/2023-05-13-mr-confounder/index.html#bias-from-g-u-instruments",
    "href": "posts/2023-05-13-mr-confounder/index.html#bias-from-g-u-instruments",
    "title": "Genetic confounding as a function of sample size",
    "section": "Bias from G-U instruments",
    "text": "Bias from G-U instruments\nThis is the expected effect estimate of X on Y using an instrument that arises via U:\n\\[\n\\beta_{IV,u} = \\frac{\\beta_{gu} \\beta_{uy} + \\beta_{gu} \\beta_{ux} \\beta_{xy}}{\\beta_{gu} \\beta_{ux}}\n\\]\nwhich simplifies to\n\\[\n\\beta_{IV,u} = \\frac{\\beta_{uy}}{\\beta_{ux}} + \\beta_{xy}\n\\]\nCheck that this is correct\n\nnid <- 10000\ngx <- rbinom(nid, 2, 0.3)\ngu <- rbinom(nid, 2, 0.3)\n\nu <- gu + rnorm(nid)\nxl <- gx + rnorm(nid)\n\nparam <- expand.grid(\n    bux = seq(-1, 1, by=0.2),\n    buy = seq(-1, 1, by=0.2),\n    bxy = c(0, 0.5)\n) %>% filter(bux != 0, buy != 0)\n\nout <- lapply(1:nrow(param), function(i)\n{\n    x <- xl + u * param$bux[i]\n    y <- x * param$bxy[i] + u * param$buy[i] + rnorm(nid)\n    p <- param[i,]\n    p$bgx <- get_effs(x, y, matrix(gx, nid, 1)) %>% mr() %>% {.$b}\n    p$bgu <- get_effs(x, y, matrix(gu, nid, 1)) %>% mr() %>% {.$b}\n    return(p)\n}) %>% bind_rows()\n\nout$exp_gu <- out$buy / out$bux + out$bxy\nplot(bgu ~ exp_gu, subset(out, !is.infinite(exp_gu)))\n\n\n\n\nSimilarly, the bias in observational studies is\n\\[\n\\beta_{OLS} = \\beta_{xy} + \\beta_{ux}\\beta_{uy}\n\\]"
  },
  {
    "objectID": "posts/2023-05-13-mr-confounder/index.html#bias-in-mr-with-heritable-confounders",
    "href": "posts/2023-05-13-mr-confounder/index.html#bias-in-mr-with-heritable-confounders",
    "title": "Genetic confounding as a function of sample size",
    "section": "Bias in MR with heritable confounders",
    "text": "Bias in MR with heritable confounders\nSimulate summary statistics for nsnpx causal variants on \\(x\\) and nsnpu causal variants on \\(u\\) according to a standard polygenic architecture\n\\[\n\\beta_{g.} \\sim N(0, [2p(1-p)^S \\sigma^2_g])\n\\]\nwhere \\(p\\) is the allele frequency, \\(S\\) is the selection coefficient and \\(\\sigma^2_g\\) is a scaling parameter relating to the additive genetic variance. For SNPs that influence \\(x\\) via \\(u\\), the \\(\\beta_{gx} = \\beta_{gu}\\beta_{ux}\\). For varying values of \\(\\beta_{ux}\\) and \\(\\beta_{uy}\\) we can calculate the expected bias in the MR estimate for each SNP as\n\\[\n$b_{MR,j} \\beta_{g_ju}\\frac{\\beta_{uy}}{\\beta_{ux}}\n\\]\nAs sample size increases, the probability of inclusion of a SNP from GWAS discovery in the exposure \\(x\\) increases. So the overall bias of an MR estimate, \\(b_{MR}\\) will be the inverse variance weighted contribution of a SNP given it’s probability of discovery\n\\[\nb_{MR} = \\frac{\\sum^{M}_{j=1} \\beta_{g_ju} w_j z_j}{\\sum^{M}_{j=1} w_j z_j}\n\\]\nwhere \\(w_j = 1/2p_j(1-p_j)\\) and z_j is the power of detection of the SNP at pval < 5e-8 based on its correlation with the trait \\(r = 2p(1-p)\\beta_{gx}^2\\) (assumes variance of x = 1) and for a given sample size.\nHere is an example of how\n\nsimfn2 <- function(nsnpx, nsnpu, bux, buy, nid, h2x, h2u, Sx, Su, bax=1) {\n    args <- environment() %>% as.list() %>% as_tibble()\n    mapx <- tibble(snp=paste0(1:nsnpx, \"x\"), af=runif(nsnpx, 0.01, 0.99))\n    mapu <- tibble(snp=paste0(1:nsnpu, \"u\"), af=runif(nsnpu, 0.01, 0.99))\n    paramsx <- generate_gwas_params(map=mapx, h2=h2x, S=Sx, Pi=1)\n    paramsu <- generate_gwas_params(map=mapu, h2=h2u, S=Su, Pi=1)\n    params <- rbind(paramsx %>% mutate(beta=beta*bax), paramsu %>% mutate(beta=beta * bux))\n    o <- map(nid, \\(i){\n        ssx <- generate_gwas_ss(params, i)\n        ssx$beta <- params$beta\n        ssx$w <- 1/ssx$se^2\n        ssx$bias <- buy/bux\n        ssx$bias[grepl(\"x\", ssx$snp)] <- 0\n        ssx <- ssx %>% mutate(h2 = beta^2 * 2 * af * (1-af))\n        ssx <- ssx %>% arrange(pval)\n        x <- ssx %>%\n            mutate(\n                n=i,\n                pow = pwr.r.test(n=i, r=sqrt(ssx$h2), sig.level=5e-8)$power,\n                u_indicator = as.numeric(grepl(\"u\", snp))\n            ) %>%\n            summarise(\n                bias = sum(bias * w * pow) / sum(w * pow),\n                nsnp = sum(pow),\n                proph2 = sum(h2/sum(h2) * pow),\n                fracu = sum(pow * u_indicator) / sum(pow)\n            )\n        return(x)\n    }) %>% bind_rows()\n    bind_cols(args, o)\n}\nr1 <- simfn2(\n    nsnpx = 2000,\n    nsnpu = 2000, \n    bux = 0.1, \n    buy = 0.1, \n    nid = seq(10000, 10000000, by=10000), \n    h2x = 0.4,\n    h2u = 0.4, \n    Sx = 1,\n    Su = 1)\n\nSo as sample size increases this is the expected bias in the MR estimate\n\nggplot(r1, aes(x=nid, y=bias)) +\ngeom_point() +\ngeom_hline(yintercept=0.1*0.1) +\nannotate(\"text\", x=0, y=0.1*0.1, label=\"OLS bias\")\n\n\n\n\nWhich is essentially the same the proportion of discovered variants that influence x through u\n\nggplot(r1, aes(x=nid, y=fracu)) +\ngeom_point()\n\n\n\n\nThe shape is somewhat different when looking at it from the perspective of number of variants discovered\n\nggplot(r1, aes(x=nsnp, y=bias)) +\ngeom_point() +\ngeom_hline(yintercept=0.1*0.1) +\nannotate(\"text\", x=0, y=0.1*0.1, label=\"OLS bias\")\n\n\n\n\nAnd also in terms of variance x explained by the discovered SNPs\n\nggplot(r1, aes(x=proph2, y=bias)) +\ngeom_point() +\ngeom_hline(yintercept=0.1*0.1) +\nannotate(\"text\", x=0, y=0.1*0.1, label=\"OLS bias\")\n\n\n\n\nSo under this model it’s not until you explain quite a large fraction of total heritability does the bias become problematic. This is quite unrealistic because the model assumes that SNPs influencing \\(x\\) not through a confounder have larger effects. But perhaps more realistic is that all SNPs influence \\(x\\) through a mediator, but some of those mediators are confounders of the \\(x-y\\) relationship and some are not.\nSo if the distribution of instrument effects on x are the same whether going through a confounder or not, the bias is no longer strongly related to sample size\n\nr2 <- simfn2(\n    nsnpx = 2000,\n    nsnpu = 2000, \n    bax = 0.1,\n    bux = 0.1, \n    buy = 0.1, \n    nid = seq(10000, 10000000, by=10000), \n    h2x = 0.4,\n    h2u = 0.4, \n    Sx = 1,\n    Su = 1)\nggplot(r2, aes(x=nid, y=bias)) +\ngeom_point() +\ngeom_hline(yintercept=0.1*0.1) +\nannotate(\"text\", x=0, y=0.1*0.1, label=\"OLS bias\")\n\n\n\n\nTry increasing polygenicity of \\(u\\)\n\nr3 <- simfn2(\n    nsnpx = 2000,\n    nsnpu = 4000, \n    bax = 1,\n    bux = 0.1, \n    buy = 0.1, \n    nid = seq(10000, 10000000, by=10000), \n    h2x = 0.4,\n    h2u = 0.4, \n    Sx = 1,\n    Su = 1)\nggplot(r3, aes(x=nid, y=bias)) +\ngeom_point() +\ngeom_hline(yintercept=0.1*0.1) +\nannotate(\"text\", x=0, y=0.1*0.1, label=\"OLS bias\")\n\n\n\n\nThe rate at which higher sample size leads MR estimates becomes biased depends on how quickly the GWAS starts to identify instruments acting through \\(u\\). This will increase under the following conditions\n\nThe effects on \\(u\\) are larger than those not through \\(u\\)\nThere are many different \\(u\\) variables through which instruments can be detected\nThe heritability of \\(u\\) is higher\nThe polygenicity of \\(u\\) is lower (i.e. effects are more discoverable)"
  },
  {
    "objectID": "posts/2023-05-13-mr-confounder/index.html#ignore-the-rest",
    "href": "posts/2023-05-13-mr-confounder/index.html#ignore-the-rest",
    "title": "Genetic confounding as a function of sample size",
    "section": "Ignore the rest",
    "text": "Ignore the rest\n\nnsnp <- 5000\nmapx <- tibble(snp=paste0(1:nsnp, \"x\"), af=runif(nsnp, 0.01, 0.99))\nmapu <- tibble(snp=paste0(1:nsnp, \"u\"), af=runif(nsnp, 0.01, 0.99))\nparamsx <- generate_gwas_params(map=mapx, h2=0.4, S=-0.4, Pi=1)\nparamsu <- generate_gwas_params(map=mapu, h2=0.4, S=-0.4, Pi=1)\n\nbux <- 0.1\nbuy <- 0.1\nparamsx <- rbind(paramsx, paramsu %>% mutate(beta=beta * bux))\n\nssx <- generate_gwas_ss(paramsx, 1000000)\nssx$w <- 1/ssx$se^2\nssx$bias <- buy/bux\nssx$bias[grepl(\"x\", ssx$snp)] <- 0\nssx <- ssx %>% arrange(pval)\no <- map(1:nrow(ssx), \\(i){\n    x <- ssx[1:i,]\n    tibble(\n        nsnp=i,\n        bias=sum(x$bias*x$w) / sum(x$w)\n    )\n}) %>% bind_rows()\no\nggplot(o, aes(x=nsnp, y=bias)) +\ngeom_point() +\ngeom_hline(yintercept=bux*buy)\n\n\nsimfn <- function(nsnpx, nsnpu, bux, buy, nid, h2x, h2u, Sx, Su, bax=1) {\n    args <- environment() %>% as.list() %>% as_tibble()\n    mapx <- tibble(snp=paste0(1:nsnpx, \"x\"), af=runif(nsnpx, 0.01, 0.99))\n    mapu <- tibble(snp=paste0(1:nsnpu, \"u\"), af=runif(nsnpu, 0.01, 0.99))\n    paramsx <- generate_gwas_params(map=mapx, h2=h2x, S=Sx, Pi=1)\n    paramsu <- generate_gwas_params(map=mapu, h2=h2u, S=Su, Pi=1)\n    params <- rbind(paramsx %>% mutate(beta=beta*bax), paramsu %>% mutate(beta=beta * bux))\n    ssx <- generate_gwas_ss(params, nid)\n    ssx$beta <- params$beta\n    ssx$w <- 1/ssx$se^2\n    ssx$bias <- buy/bux\n    ssx$bias[grepl(\"x\", ssx$snp)] <- 0\n    ssx <- ssx %>% mutate(h2 = beta^2 * 2 * af * (1-af))\n    ssx <- ssx %>% arrange(pval)\n    o <- map(1:nrow(ssx), \\(i){\n        x <- ssx[1:i,]\n        tibble(\n            nsnp=i,\n            propsnp=nsnp/nrow(ssx),\n            proph2=sum(x$h2) / sum(ssx$h2),\n            bias=sum(x$bias*x$w) / sum(x$w)\n        )\n    }) %>% bind_rows()\n    bind_cols(args, o)\n}\no <- simfn(1000, 1000, 0.1, 0.4, 10000000, 0.3, 0.3, 0, 0)\nggplot(o, aes(x=nsnp, y=bias)) +\ngeom_point() +\ngeom_hline(yintercept=o$bux[1]*o$buy[1])\n\n\nparams <- expand.grid(\n    bux = seq(-1, 1, by=0.2),\n    buy = seq(-1, 1, by=0.2),\n    nsnpx = c(2000),\n    nsnpu = c(2000, 4000)\n)\n\nres <- map(1:nrow(params), \\(i) {\n    simfn(params$nsnpx[i], params$nsnpu[i], params$bux[i], params$buy[i], 1000000, 0.4, 0.4, 0, 0)\n}, .progress=TRUE) %>% bind_rows()\n\n\nres <- res %>% \n    group_by(bux, buy, nsnpx, nsnpu) %>%\n    mutate(propsnp=nsnp/max(nsnp))\nobsbias <- res %>% group_by(bux, buy) %>%\n    summarise(obsbias=bux[1] * buy[1])\np1 <- res %>%\n    filter(bux != 0, buy != 0) %>%\n    ggplot(., aes(x=propsnp, y=bias, group=as.factor(nsnpu))) +\n    geom_line(aes(colour=as.factor(nsnpu))) +\n    geom_hline(data=obsbias, aes(yintercept=obsbias)) +\n    facet_grid(bux ~ buy)\n\np1\nggsave(p1, file=\"res.pdf\", width=15, height=15)\n\n\nparams <- expand.grid(\n    bax = c(0.1, 1),\n    bux = c(0.1, 0.5, 1),\n    buy = c(0.1, 0.5, 1),\n    nsnpx = c(2000),\n    nsnpu = c(2000, 4000)\n)\n\nres2 <- map(1:nrow(params), \\(i) {\n    simfn(params$nsnpx[i], params$nsnpu[i], params$bux[i], params$buy[i], 1000000, 0.4, 0.4, 0, 0, params$bax[i])\n}, .progress=TRUE) %>% bind_rows()\n\n\nobsbias <- res2 %>% group_by(bax, bux, buy) %>%\n    summarise(obsbias=bux[1] * buy[1])\np2 <- res2 %>%\n    filter(bux != 0, buy != 0) %>%\n    ggplot(., aes(x=proph2, y=bias, group=as.factor(nsnpu))) +\n    geom_line(aes(colour=as.factor(nsnpu))) +\n    geom_hline(data=obsbias, aes(yintercept=obsbias)) +\n    facet_grid(bux ~ buy + bax, labeller = label_both)\n\np2\nggsave(p2, file=\"res2.pdf\", width=15, height=15)\n\nUse power\n\nsimfn2 <- function(nsnpx, nsnpu, bux, buy, nid, h2x, h2u, Sx, Su, bax=1) {\n    args <- environment() %>% as.list() %>% as_tibble()\n    mapx <- tibble(snp=paste0(1:nsnpx, \"x\"), af=runif(nsnpx, 0.01, 0.99))\n    mapu <- tibble(snp=paste0(1:nsnpu, \"u\"), af=runif(nsnpu, 0.01, 0.99))\n    paramsx <- generate_gwas_params(map=mapx, h2=h2x, S=Sx, Pi=1)\n    paramsu <- generate_gwas_params(map=mapu, h2=h2u, S=Su, Pi=1)\n    params <- rbind(paramsx %>% mutate(beta=beta*bax), paramsu %>% mutate(beta=beta * bux))\n    o <- map(nid, \\(i){\n        ssx <- generate_gwas_ss(params, i)\n        ssx$w <- 1/ssx$se^2\n        ssx$bias <- buy/bux\n        ssx$bias[grepl(\"x\", ssx$snp)] <- 0\n        ssx <- ssx %>% mutate(h2 = bhat^2 * 2 * af * (1-af))\n        ssx <- ssx %>% arrange(pval)\n        x <- ssx %>%\n            mutate(\n                n=i,\n                pow = pwr.r.test(n=i, r=sqrt(ssx$h2), sig.level=5e-8)$power,\n            ) %>%\n            summarise(\n                bias = sum(bias * w * pow) / sum(w * pow),\n                nsnp = sum(pow),\n                proph2 = sum(h2/sum(h2) * pow),\n            )\n        return(x)\n    }) %>% bind_rows()\n    bind_cols(args, o)\n}\n\nparams <- expand.grid(\n    bax = c(0.1, 1),\n    bux = c(0.1, 0.5, 1),\n    buy = c(0.1, 0.5, 1),\n    nsnpx = c(2000),\n    nsnpu = c(2000, 4000)\n)\n\nres3 <- map(1:nrow(params), \\(i) {\n    simfn2(params$nsnpx[i], params$nsnpu[i], params$bux[i], params$buy[i], seq(10000,10000000,by=10000), 0.4, 0.4, 0, 0, params$bax[i])\n}, .progress=TRUE) %>% bind_rows()\n\nobsbias <- res3 %>% group_by(bax, bux, buy) %>%\n    summarise(obsbias=bux[1] * buy[1])\np3 <- res3 %>%\n    filter(bux != 0, buy != 0, nid < 1000000, bax==0.1, nsnp >= 1) %>%\n    ggplot(., aes(x=nid, y=bias, group=as.factor(nsnpu))) +\n    geom_line(aes(colour=as.factor(nsnpu))) +\n    geom_hline(data=obsbias, aes(yintercept=obsbias)) +\n    facet_grid(bux ~ buy, labeller = label_both)\n\nggsave(p3, file=\"res3.pdf\", width=15, height=15)\n\n\nu <- rnorm(1000000)\nx <- scale(rnorm(1000000, sd=0.9) + u * 0.1)\ny <- scale(rnorm(1000000, sd=0.9) + u * 0.1)\n\ncor(u,x)\ncor(u,y)\ncor(y,x)\n\n\n\nsessionInfo()\n\nR version 4.2.3 Patched (2023-03-15 r84020)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] pwr_1.3-0         purrr_1.0.1       TwoSampleMR_0.5.6 simulateGP_0.1.2 \n[5] ggplot2_3.4.0     dplyr_1.0.10     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9        plyr_1.8.7        pillar_1.8.1      compiler_4.2.3   \n [5] tools_4.2.3       digest_0.6.31     jsonlite_1.8.4    evaluate_0.19    \n [9] lifecycle_1.0.3   tibble_3.1.8      gtable_0.3.1      pkgconfig_2.0.3  \n[13] rlang_1.0.6       DBI_1.1.3         cli_3.5.0         yaml_2.3.6       \n[17] xfun_0.36         fastmap_1.1.0     withr_2.5.0       stringr_1.5.0    \n[21] knitr_1.41        generics_0.1.3    vctrs_0.5.1       htmlwidgets_1.5.4\n[25] grid_4.2.3        tidyselect_1.2.0  glue_1.6.2        R6_2.5.1         \n[29] fansi_1.0.3       rmarkdown_2.16    farver_2.1.1      magrittr_2.0.3   \n[33] scales_1.2.1      htmltools_0.5.4   assertthat_0.2.1  colorspace_2.0-3 \n[37] labeling_0.4.2    utf8_1.2.2        stringi_1.7.8     munsell_0.5.0"
  },
  {
    "objectID": "posts/2022-07-24-case-control-power/index.html",
    "href": "posts/2022-07-24-case-control-power/index.html",
    "title": "Power of GWAS in ascertained case control datasets",
    "section": "",
    "text": "Case control studies ascertain a fixed number of cases and controls. This changes the distribution of genetic liability in the selected sample - e.g. if the prevalence is low then the liability will be a truncated distribution for cases ascertained for the tail of the distribution, and truncated for the controls ascertained for a depletion of values in the tail (e.g. see here for illustrations https://pubmed.ncbi.nlm.nih.gov/21376301/).\nThe more rare the disease, the larger the variance of the liability when cases and controls are matched. This should improve statistical power because the cases and controls are ascertained to be more genetically distinct from each other.\nHowever, the Genetic Power Calculator concludes the opposite, as prevalence gets lower the power goes down (https://zzz.bwh.harvard.edu/gpc/cc2.html). e.g. for OR=1.1, ncase=1000, ncontrol=1000, af=0.5, for 80% power:\n\nprev = 0.001, power = 4e-5\nprev = 0.4, power = 0.71\n\nQuick simulation to investigate:\n\nlibrary(simulateGP)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nGenerate a function that will\n\ncreate a population with some genetic liability\nstochastically assign disease status based on heritability and prevalence\nascertain cases and controls\nidentify how many significant associations in the case/control sample\n\n\nsims <- function(ncase, ncontrol, nsnp, prev, hsq=0.5, thresh=5e-8)\n{\n  # Determine minimum sample size required to ascertain required number of cases and controls\n  n_req <- round(max(ncase / prev, ncontrol / prev) + 10000)\n  \n  # Generate matrix of genotype values\n  g <- make_geno(n_req, nsnp, 0.5)\n  \n  # Effect sizes for each SNP\n  b <- rnorm(nsnp)\n\n  dat <- tibble(\n    id = 1:n_req,\n    l = scale(g %*% b),               # genetic liability\n    p = gx_to_gp(l, hsq, prev),       # convert to disease probability\n    d = rbinom(n_req, 1, p)           # sample disease status from probability\n  )\n  \n  # Ascertain cases and controls \n  dat <- rbind(\n    subset(dat, d == 0)[1:ncase,],\n    subset(dat, d == 1)[1:ncontrol,]\n  )\n\n  # Perform GWAS\n  res <- gwas(dat$d, g[dat$id,], logistic=TRUE)\n  \n  # Count number of significant assocs\n  return(sum(res$pval < thresh))\n}\n\nRun a bunch of simulations\n\nparams <- expand.grid(\n  ncase = 1000,\n  ncontrol = 1000, \n  nsnp = c(2, 10, 100),\n  repeats = 1:10,\n  prev = seq(0.01, 0.3, by=0.01),\n  hsq=0.5,\n  thresh=5e-8\n) %>% select(-repeats)\nparams$nsig <- sapply(1:nrow(params), function(i) do.call(sims, params[i,]))\n\nPlot\n\nggplot(params, aes(x=prev, y=nsig/nsnp)) +\n  geom_point() +\n  facet_grid(nsnp ~ ., labeller=label_both, scale=\"free_y\") +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nSo power increases when prevalence is lower. Maybe this is related to polygenicity.\nSo what’s GPC doing differently to get the opposite result (lower prev = lower power)?"
  },
  {
    "objectID": "posts/2023-03-11-mz-vqtl-power/index.html",
    "href": "posts/2023-03-11-mz-vqtl-power/index.html",
    "title": "Boosting MZ vQTL power",
    "section": "",
    "text": "The MZ difference design is reasonably well powered against DRM method used for population-based analysis. However, are these two estimates orthogonal even if estimated in the same set of MZs?\n\nMZ difference\nDRM in MZ1 + MZ2 accounting for relatedness\nMeta analyse (1) and (2)"
  },
  {
    "objectID": "posts/2023-03-11-mz-vqtl-power/index.html#simulations",
    "href": "posts/2023-03-11-mz-vqtl-power/index.html#simulations",
    "title": "Boosting MZ vQTL power",
    "section": "Simulations",
    "text": "Simulations\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(simulateGP)\nlibrary(here)\n\nhere() starts at /Users/gh13047/repo/lab-book\n\nlibrary(parallel)\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(lmerTest)\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(metafor)\n\nLoading required package: metadat\n\n\n\nLoading the 'metafor' package (version 3.8-1). For an\nintroduction to the package please type: help(metafor)\n\nsim_pop <- function(n, beta1, beta2, af, h2)\n{\n  g <- rbinom(n, 2, af)\n  prs <- g * beta1\n  vg <- rnorm(n, 0, h2)\n  v <- rnorm(n, 0, beta2 * g)\n  ve <- rnorm(n, 0, sqrt(1 - var(vg) - var(v) - var(prs)))\n  y <- prs + v + vg + ve\n  return(tibble(\n    g, y\n  ))\n}\n\nsim_mz <- function(n, beta1, beta2, af, h2)\n{\n  g <- rbinom(n, 2, af)\n  prs <- g * beta1\n  vg <- rnorm(n, 0, h2)\n  v1 <- rnorm(n, 0, beta2 * g)\n  ve1 <- rnorm(n, 0, sqrt(1 - var(vg) - var(v1) - var(prs)))\n  y1 <- prs + v1 + vg + ve1\n  v2 <- rnorm(n, 0, beta2 * g)\n  ve2 <- rnorm(n, 0, sqrt(1 - var(vg) - var(v2) - var(prs)))\n  y2 <- prs + v2 + vg + ve2\n  return(tibble(\n    g, y1, y2\n  ))\n}\n\ntest_drm <- function(g, y)\n{\n  y.i <- tapply(y, g, median, na.rm=T)  \n  z.ij <- abs(y - y.i[g+1])\n  fast_assoc(z.ij, g) %>%\n    as_tibble() %>%\n    mutate(method=\"drm\")\n}\n\ntest_mz <- function(g, y1, y2)\n{\n  yd1 <- abs(y1-y2)\n  r1 <- fast_assoc(yd1, g) %>%\n    as_tibble() %>%\n    mutate(method=\"mzdiff\")\n  r1\n}\n\ntest_drm_lme4 <- function(g, y1, y2)\n{\n    dat <- tibble(g=c(g,g), y=c(y1,y2), twin=rep(1:length(y1), each=2))\n    y.i <- tapply(dat$y, dat$g, median, na.rm=T)  \n    dat$z.ij <- abs(dat$y - y.i[dat$g+1])\n\n    out <- lmer(z.ij ~ g + (1 | twin), dat) %>% \n        summary() %>%\n        coef() %>%\n        {\n            tibble(ahat=.[1,1], bhat=.[2,1], se=.[2,2], fval=.[2,4]^2, pval=.[2,5], n=nrow(dat), method=\"drm_lmer\")\n        }\n    return(out)\n}\n\nCheck FDR of drm_lme4\n\nset.seed(1234)\na <- sapply(1:500, function(i)\n{\n    # no vQTL effect\n    temp <- sim_mz(1000, 0.1, 0, 0.5, 0.5)\n    c(\n        test_drm(temp$g, temp$y1)$pval,\n        test_drm_lme4(temp$g, temp$y1, temp$y2)$pval\n    )    \n})\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\napply(a, 1, function(x) sum(x < 0.05)/length(x))\n\n[1] 0.038 0.044\n\n\nSeems ok. Test power difference\n\nset.seed(1234)\na <- sapply(1:500, function(i)\n{\n    # no vQTL effect\n    temp <- sim_mz(1000, 0.1, 0.2, 0.5, 0.5)\n    c(\n        test_drm(temp$g, temp$y1)$pval,\n        test_drm_lme4(temp$g, temp$y1, temp$y2)$pval\n    )    \n})\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\napply(a, 1, function(x) sum(x < 0.05)/length(x))\n\n[1] 0.234 0.378\n\n\nOk this seems to boost power. Now meta-analyse with MZ difference\n\nset.seed(1234)\ntemp <- sim_mz(1000, 0.1, 0.2, 0.5, 0.5)\ntest_meta <- function(g, y1, y2)\n{\n    # mz\n    r1 <- test_mz(g, y1, y2)\n    # drm using just one twin\n    r2 <- test_drm(g, y1)\n    # drm using lme4\n    r3 <- test_drm_lme4(g, y1, y2)\n    # meta_analyse mz + drm_lmer\n    r4 <- metafor::rma(\n        c(r1$bhat, r3$bhat),\n        c(r1$se^2, r3$se^2),\n        method=\"FE\"\n    ) %>%\n    {\n        tibble(ahat=NA, bhat=.$b[1], se=.$se, fval=.$zval^2, pval=.$pval, n=length(g), method=\"mz+drm_lmer\")\n    }\n    # meta_analyse mz + drm\n    r5 <- metafor::rma(\n        c(r1$bhat, r2$bhat),\n        c(r1$se^2, r2$se^2),\n        method=\"FE\"\n    ) %>%\n    {\n        tibble(ahat=NA, bhat=.$b[1], se=.$se, fval=.$zval^2, pval=.$pval, n=length(g), method=\"mz+drm\")\n    }\n    return(bind_rows(r1, r2, r3, r4, r5))\n}\ntest_meta(temp$g, temp$y1, temp$y2)\n\nboundary (singular) fit: see help('isSingular')\n\n\n# A tibble: 5 × 7\n    ahat     bhat     se   fval   pval     n method     \n   <dbl>    <dbl>  <dbl>  <dbl>  <dbl> <int> <chr>      \n1  0.948 -0.00760 0.0314 0.0586 0.809   1000 mzdiff     \n2  0.766  0.0255  0.0258 0.976  0.323   1000 drm        \n3  0.743  0.0424  0.0184 5.33   0.0211  2000 drm_lmer   \n4 NA      0.0297  0.0159 3.50   0.0615  1000 mz+drm_lmer\n5 NA      0.0122  0.0200 0.371  0.542   1000 mz+drm     \n\n\nCheck fdr\n\na <- lapply(1:500, function(i)\n{\n    temp <- sim_mz(1000, 0.1, 0, 0.5, 0.5)\n    test_meta(temp$g, temp$y1, temp$y2) %>% mutate(sim=i)\n}) %>% bind_rows()\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\na %>%\n    group_by(method) %>%\n    summarise(fdr=sum(pval < 0.05)/n())\n\n# A tibble: 5 × 2\n  method        fdr\n  <chr>       <dbl>\n1 drm         0.044\n2 drm_lmer    0.058\n3 mz+drm      0.088\n4 mz+drm_lmer 0.124\n5 mzdiff      0.046\n\n\ncheck power\n\nb <- lapply(1:500, function(i)\n{\n    temp <- sim_mz(1000, 0.1, 0.2, 0.3, 0.5)\n    test_meta(temp$g, temp$y1, temp$y2) %>% mutate(sim=i)\n}) %>% bind_rows()\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\nb %>%\n    group_by(method) %>%\n    summarise(pow=sum(pval < 0.05)/n())\n\n# A tibble: 5 × 2\n  method        pow\n  <chr>       <dbl>\n1 drm         0.138\n2 drm_lmer    0.212\n3 mz+drm      0.33 \n4 mz+drm_lmer 0.398\n5 mzdiff      0.228\n\n\n\nlibrary(tidyr)\nspread(a %>% dplyr::select(method, bhat, sim), key=method, value=bhat) %>% \n    dplyr::select(-sim) %>%\n    cor\n\n                  drm  drm_lmer    mz+drm mz+drm_lmer    mzdiff\ndrm         1.0000000 0.7144922 0.8649999   0.6733524 0.3783519\ndrm_lmer    0.7144922 1.0000000 0.7578695   0.9406147 0.5299286\nmz+drm      0.8649999 0.7578695 1.0000000   0.8698533 0.7912825\nmz+drm_lmer 0.6733524 0.9406147 0.8698533   1.0000000 0.7860999\nmzdiff      0.3783519 0.5299286 0.7912825   0.7860999 1.0000000\n\n\nThis could work if the meta analysis could take into consideration that DRM + MZ have a 0.5 correlation… how do you do that\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] metafor_3.8-1    metadat_1.2-0    lmerTest_3.1-3   lme4_1.1-30     \n [5] Matrix_1.5-3     here_1.0.1       simulateGP_0.1.2 tidyr_1.2.1     \n [9] ggplot2_3.4.0    dplyr_1.0.10    \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9          mathjaxr_1.6-0      nloptr_2.0.3       \n [4] pillar_1.8.1        compiler_4.2.1      tools_4.2.1        \n [7] boot_1.3-28         digest_0.6.31       nlme_3.1-158       \n[10] lattice_0.20-45     jsonlite_1.8.4      evaluate_0.19      \n[13] lifecycle_1.0.3     tibble_3.1.8        gtable_0.3.1       \n[16] pkgconfig_2.0.3     rlang_1.0.6         DBI_1.1.3          \n[19] cli_3.5.0           yaml_2.3.6          xfun_0.36          \n[22] fastmap_1.1.0       withr_2.5.0         stringr_1.5.0      \n[25] knitr_1.41          generics_0.1.3      vctrs_0.5.1        \n[28] htmlwidgets_1.5.4   rprojroot_2.0.3     grid_4.2.1         \n[31] tidyselect_1.2.0    glue_1.6.2          R6_2.5.1           \n[34] fansi_1.0.3         rmarkdown_2.16      minqa_1.2.4        \n[37] purrr_1.0.0         magrittr_2.0.3      MASS_7.3-58.1      \n[40] splines_4.2.1       scales_1.2.1        htmltools_0.5.4    \n[43] assertthat_0.2.1    colorspace_2.0-3    numDeriv_2016.8-1.1\n[46] utf8_1.2.2          stringi_1.7.8       munsell_0.5.0"
  },
  {
    "objectID": "posts/2023-07-09-meta-regression-instrument-strength/index.html",
    "href": "posts/2023-07-09-meta-regression-instrument-strength/index.html",
    "title": "2023-07-09-meta-regression-instrument-strength",
    "section": "",
    "text": "sessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.0    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.3.0       htmltools_0.5.5   rstudioapi_0.14   yaml_2.3.7       \n [9] rmarkdown_2.22    knitr_1.43        xfun_0.39         digest_0.6.31    \n[13] jsonlite_1.8.5    rlang_1.1.1       evaluate_0.21"
  },
  {
    "objectID": "posts/2023-03-15-gene-environment-equivalence/index.html",
    "href": "posts/2023-03-15-gene-environment-equivalence/index.html",
    "title": "Gene-environment equivalence",
    "section": "",
    "text": "Latest PRS for height explains ~40% of the variance. Heritability ~0.7. Height has a negative association with coronary heart disease. Height is fixed after adolescence, which means it can only be influenced by a) genetic factors; b) early life exposures; c) stochasticity.\nQuestion: Is the influence of height on CHD different when the modifier is genetic vs non-genetic?\nAnalysis: Estimate the association of height PRS on CHD, and then residualise height for the PRS and determine the residual height association with CHD."
  },
  {
    "objectID": "posts/2023-03-15-gene-environment-equivalence/index.html#model",
    "href": "posts/2023-03-15-gene-environment-equivalence/index.html#model",
    "title": "Gene-environment equivalence",
    "section": "Model",
    "text": "Model\n\nGene environment equivalence\nUnder gene-environment equivalence, the following model:\n\\[\ny_i = \\alpha + \\beta_{xy} x_i + \\epsilon_i\n\\]\nwhere \\(y_i\\) is the outcome (e.g. CHD), \\(\\beta_{xy}\\) is the causal effect of the exposure on the outcome and\n\\[\nx_i = a + \\sqrt{h^2} g_i + \\sqrt{1-h^2} e_i\n\\]\nwhere \\(x_i\\) is the exposure (height) in the \\(i\\)th individual, \\(h^2\\) is the heritability of height, \\(g_i\\) is the (perfectly measured) genetic value for individual \\(i\\) and \\(e_i\\) is the non-genetic component of height. There is no confounding in this simple model. What is the expected association of g on y?\n\\[\n\\begin{aligned}\n\\beta_{gy} &= \\frac{cov(y, g)}{var(g)} \\\\\n&= \\frac{cov(\\beta_{xy}x, g)}{ var(g) } \\\\\n&= \\frac{cov(\\beta_{xy}\\sqrt{h^2}g, g)}{ var(g) } \\\\\n&= \\frac{\\beta_{xy}\\sqrt{h^2}var(g)}{ var(g) } \\\\\n&= \\beta_{xy}\\sqrt{h^2}\n\\end{aligned}\n\\]\nSo this is expected and the basic result for MR. What is the expected association of x on y after it has been residualised for g? First get the residual assuming perfect information of \\(g\\)\n\\[\n\\begin{aligned}\n\\hat{e_i} &= x_i - \\hat{\\sqrt{h^2}}g_i \\\\\n&\\approx \\sqrt{1-h^2}e_i\n\\end{aligned}\n\\]\nNow find the association of \\(\\hat{e}_i\\) with \\(y_i\\)\n\\[\n\\begin{aligned}\n\\beta_{\\hat{e}y} &= \\frac{cov(\\hat{e}, y)}{var(\\hat{e})} \\\\\n&= \\frac{cov(\\sqrt{1-h^2}e, \\beta_{xy}(\\sqrt{1-h^2}e)}{(1-h^2)var(e)} \\\\\n&= \\frac{\\beta_{xy}(1-h^2)var(e)}{(1-h^2)var(e)} \\\\\n&= \\beta_{xy}\n\\end{aligned}\n\\]\nRESULT: The residual height association with y is expected to be equal to the raw height association with y.\n\n\nGene environment non-equivalence\nNow allow the genetic and non-genetic components of height to have independent influences on CHD.\n\\[\nx_i = \\sqrt{h^2}g_i + \\sqrt{1-h^2}e_i\n\\]\nas before, and now\n\\[\ny_i = \\beta_g g_i + \\beta_e e_i + \\epsilon_i\n\\]\nExpected association of x on y\n\\[\n\\begin{aligned}\n\\beta_{xy} &= \\frac{cov(\\sqrt{h^2}g_i + \\sqrt{1-h^2}e_i,\\beta_g g_i + \\beta_e e_i)}{var(x)} \\\\\n&= \\frac{\\sqrt{h^2}\\beta_g var(g) + \\sqrt{1-h^2}\\beta_e var(e)}{h^2 var(g) + (1-h^2) var(e)} \\\\\n\\end{aligned}\n\\]\nAssuming \\(var(g)=var(e)=1\\) this reduces to\n\\[\n\\beta_{xy} = \\sqrt{h^2}\\beta_g + \\sqrt{1-h^2}\\beta_e\n\\]\nNow what is the expected association of x on y after it has been residualised for g?\n\\[\n\\begin{aligned}\n\\beta_{\\hat{e}y} &= cov(\\hat{e}, y) / var(\\hat{e}) \\\\\n&= \\frac{cov(\\sqrt{1-h^2}e, \\beta_g g + \\beta_e e)}{(1-h^2)var(e)} \\\\\n&= \\frac{\\sqrt{1-h^2}\\beta_evar(e)}{(1-h^2)var(e)} \\\\\n&= \\sqrt{\\frac{(1-h^2)\\beta_e^2 var(e)^2}{(1-h^2)^2 var(e)^2}} \\\\\n&= \\frac{\\beta_e}{\\sqrt{1-h^2}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2023-03-15-gene-environment-equivalence/index.html#check-with-simulations",
    "href": "posts/2023-03-15-gene-environment-equivalence/index.html#check-with-simulations",
    "title": "Gene-environment equivalence",
    "section": "Check with simulations",
    "text": "Check with simulations\n\nGene environment equivalence\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nn <- 100000\nbxy <- 0.3\nh2 <- 0.7\ng <- rnorm(n)\ne <- rnorm(n)\nx <- sqrt(h2) * g + sqrt(1-h2) * e\ny <- x * bxy + rnorm(n)\n\nGet residual\n\nxres <- residuals(lm(x ~ g)) # sim\nxres_exp <- sqrt(1-h2) * e # expected\n# check they're the same\ncor(xres, xres_exp)\n\n[1] 0.9999938\n\nlm(xres ~ xres_exp)$coef[2]\n\n xres_exp \n0.9999876 \n\n\nAssoc of x on y should be\n\nlm(y ~ x)$coef[2] # sim\n\n        x \n0.3002834 \n\nbxy # expected\n\n[1] 0.3\n\n\nExpected assoc of g on y\n\nlm(y ~ g)$coef[2] # sim\n\n        g \n0.2523371 \n\nsqrt(h2)*bxy # expected\n\n[1] 0.250998\n\n\nExpected assoc of residual x on y\n\nlm(y ~ xres)$coef[2] # sim\n\n     xres \n0.2988134 \n\nbxy\n\n[1] 0.3\n\n\n\n\nGene environment non-equivalence\n\nn <- 100000\nbg <- 0.3\nbe <- 0.6\nh2 <- 0.7\ng <- rnorm(n)\ne <- rnorm(n)\nx <- sqrt(h2) * g + sqrt(1-h2) * e\ny <- g * bg + e * be + rnorm(n)\n\nGet residual\n\nxres <- residuals(lm(x ~ g)) # sim\nxres_exp <- sqrt(1-h2) * e # expected\n# check they're the same\ncor(xres, xres_exp)\n\n[1] 0.9999998\n\nlm(xres ~ xres_exp)$coef[2]\n\n xres_exp \n0.9999996 \n\n\nAssociation of xres and y\n\nlm(y ~ xres)$coef[2] # sim\n\n    xres \n1.086526 \n\nbe / sqrt(1-h2)\n\n[1] 1.095445\n\n\nAssoc of x and y - simulation\n\nlm(y ~ x)$coef[2] # sim\n\n        x \n0.5760577 \n\nbg * sqrt(h2) + be * sqrt(1-h2) # exp\n\n[1] 0.5796315"
  },
  {
    "objectID": "posts/2023-03-15-gene-environment-equivalence/index.html#incomplete-adjustment-of-g",
    "href": "posts/2023-03-15-gene-environment-equivalence/index.html#incomplete-adjustment-of-g",
    "title": "Gene-environment equivalence",
    "section": "Incomplete adjustment of g",
    "text": "Incomplete adjustment of g\nDoes incomplete adjustment of g change things much?\n\nGene environment equivalence\n\nn <- 100000\nbxy <- 0.3\nh2 <- 0.7\ng_exp <- 0.5 # proportion of g explained by prs\ng_explained <- rnorm(n) * sqrt(g_exp)\ng_unexplained <- rnorm(n) * sqrt(1-g_exp)\ng <- g_explained + g_unexplained\ne <- rnorm(n)\nx <- sqrt(h2) * g + sqrt(1-h2) * e\ny <- x * bxy + rnorm(n)\n\nGet residual - now this is different from expected above due to residual including some unadjusted genetic variance, but linearly still related\n\nxres <- residuals(lm(x ~ g_explained)) # sim\nxres_exp <- sqrt(1-h2) * e # expected\n# check they're the same\ncor(xres, xres_exp)\n\n[1] 0.6786454\n\nlm(xres ~ xres_exp)$coef[2]\n\nxres_exp \n1.001644 \n\n\nWhat is the PRS (explained g) assoc with y?\n\nlm(y ~ g_explained)$coef[2]\n\ng_explained \n  0.2557102 \n\n\nAssoc of height with y\n\nlm(y ~ x)$coef[2]\n\n        x \n0.3021427 \n\n\nAnd residual with y\n\nlm(y ~ xres)$coef[2]\n\n     xres \n0.3004327 \n\n\nSo the residual x effect remains identical to the raw effect of x."
  },
  {
    "objectID": "posts/2023-03-15-gene-environment-equivalence/index.html#in-progress-ignore-for-now",
    "href": "posts/2023-03-15-gene-environment-equivalence/index.html#in-progress-ignore-for-now",
    "title": "Gene-environment equivalence",
    "section": "In progress… (ignore for now)",
    "text": "In progress… (ignore for now)\nDoes confounding change things much?\n\nn <- 100000\nbxy <- 0.3\nbu <- 0.3\nh2 <- 0.7\nu <- rnorm(n)\ng <- rnorm(n)\ne <- rnorm(n)\nx <- sqrt(h2) * g + sqrt(1-h2) * e + u * bu\ny <- x * bxy + rnorm(n) + u * bu\n\ny ~ x\n\nlm(y ~ x)$coef[2]\n\n        x \n0.3807293 \n\n\ny ~ x_res\n\nxres <- residuals(lm(x ~ g)) # sim\nxres_exp <- sqrt(1-h2) * e # expected\n# check they're the same\ncor(xres, xres_exp)\n\n[1] 0.8756121\n\nlm(xres ~ xres_exp)$coef[2]\n\n xres_exp \n0.9991385 \n\n\n\nlm(y ~ xres)$coef[2]\n\n     xres \n0.5229343 \n\n\n\nlm(y ~ xres_exp)$coef[2]\n\n xres_exp \n0.2933105 \n\n\n\nlm(y ~ g)$coef[2]\n\n        g \n0.2532887 \n\n\n\ntestdat <- function(y, x, g)\n{\n  xres <- residuals(lm(x ~ g))\n  tribble(~model, ~beta,\n          \"y ~ x\", lm(y ~ x)$coef[2],\n          \"y ~ g\", lm(y ~ g)$coef[2],\n          \"y ~ xres\", lm(y ~ xres)$coef[2]\n  )\n}\n\nNo confounding, gene environment equivalence\n\nn <- 100000\nbxy <- -0.3\nh2 <- 0.7\ng <- rnorm(n)\ne <- rnorm(n)\nx <- sqrt(h2) * g + sqrt(1-h2) * e\ny <- x * bxy + rnorm(n)\ntestdat(y, x, g)\n\n# A tibble: 3 × 2\n  model      beta\n  <chr>     <dbl>\n1 y ~ x    -0.306\n2 y ~ g    -0.256\n3 y ~ xres -0.300\n\n\n\nn <- 100000\nbuy <- 0.3\nbux <- -0.3\nbxy <- 0.3\nh2 <- 0.7\nu <- rnorm(n)\ng <- rnorm(n)\ne <- rnorm(n)\nx <- sqrt(h2) * g + sqrt(1-h2) * e + bux * u\ny <- x * bxy + rnorm(n) + buy * u\ntestdat(y, x, g)\n\n# A tibble: 3 × 2\n  model      beta\n  <chr>     <dbl>\n1 y ~ x    0.217 \n2 y ~ g    0.254 \n3 y ~ xres 0.0633\n\n\n\nn <- 100000\nbuy <- 0.3\nbux <- -0.3\nbg <- -0.2\nbe <- -0.3\nh2 <- 0.7\nu <- rnorm(n)\ng <- rnorm(n)\ne <- rnorm(n)\nx <- sqrt(h2) * g + sqrt(1-h2) * e + bux * u\ny <- g * bg + e * be + rnorm(n) + buy * u\ntestdat(y, x, g)\n\n# A tibble: 3 × 2\n  model      beta\n  <chr>     <dbl>\n1 y ~ x    -0.392\n2 y ~ g    -0.205\n3 y ~ xres -0.655\n\n\n\nparam <- expand.grid(\n  buy = seq(-0.3, 0.3, by=0.3),\n  bux = seq(-0.3, 0.3, by=0.3),\n  bg = seq(-0.3, 0.3, by=0.3),\n  be = seq(-0.3, 0.3, by=0.3)\n)\n\nres <- lapply(1:nrow(param), function(i) {\n  n <- 100000\n  buy <- param$buy[i]\n  bux <- param$bux[i]\n  bg <- param$bg[i]\n  be <- param$be[i]\n  h2 <- 0.7\n  u <- rnorm(n)\n  g <- rnorm(n, sd=sqrt(h2))\n  e <- rnorm(n, sd=sqrt(1-h2))\n  x <- g + e + bux * u\n  y <- g * bg + e * be + rnorm(n) + buy * u\n  testdat(y, x, g) %>% bind_cols(param[i,])\n})\nres <- bind_rows(res)\n\n\nres %>% mutate(ge = bg-be) %>%\n  filter(bg == -0.3) %>%\n  #filter(round(bg, 1) == -0.2, be < 0, round(buy, 1) %in% c(-0.3, 0, 0.3), round(bux, 1) %in% c(-0.3, 0, 0.3)) %>%\nggplot(., aes(x=beta, y=ge)) +\n  geom_point(aes(colour=model)) +\n  geom_line(aes(colour=model)) +\n  facet_grid(buy ~ bux) +\n  scale_colour_brewer(type=\"qual\")\n\n\n\n\n\nn <- 100000\nbuy <- 0\nbux <- 0\nbg <- -0.4\nbe <- -0.2\nh2 <- 0.5\nu <- rnorm(n)\ng <- rnorm(n, sd=sqrt(h2))\ne <- rnorm(n, sd=sqrt(1-h2))\nx <- g + e\ny <- g * bg + e * be + rnorm(n)\ntestdat(y, x, g)\n\n# A tibble: 3 × 2\n  model      beta\n  <chr>     <dbl>\n1 y ~ x    -0.301\n2 y ~ g    -0.397\n3 y ~ xres -0.205\n\n\n\nlibrary(simulateGP)\ng <- make_geno(1000, 100, 0.3)\nb <- rnorm(100)\nprs <- g %*% b\nve <- var(prs) / 0.7 - var(prs)\ne <- rnorm(1000, sd=sqrt(ve))\nx <- prs + e\ncor(x, prs)^2\n\n          [,1]\n[1,] 0.6871713\n\nvar(prs) / var(x)\n\n          [,1]\n[1,] 0.6877084"
  },
  {
    "objectID": "posts/2023-02-18-overlapping-genes/index.html",
    "href": "posts/2023-02-18-overlapping-genes/index.html",
    "title": "Overlapping genes",
    "section": "",
    "text": "For a list of positions, identify all genes that overlap +/- 200kb.\nUse Ensembl database of gene positions\n\nlibrary(EnsDb.Hsapiens.v75) # For convenience hg19\nlibrary(ieugwasr)\nlibrary(dplyr)\ngenelist <- genes(EnsDb.Hsapiens.v75) %>%\n    subset(gene_biotype==\"protein_coding\")\ngenelist\n\nGRanges object with 22810 ranges and 6 metadata columns:\n                  seqnames            ranges strand |         gene_id\n                     <Rle>         <IRanges>  <Rle> |     <character>\n  ENSG00000186092        1       69091-70008      + | ENSG00000186092\n  ENSG00000237683        1     134901-139379      - | ENSG00000237683\n  ENSG00000235249        1     367640-368634      + | ENSG00000235249\n  ENSG00000185097        1     621059-622053      - | ENSG00000185097\n  ENSG00000269831        1     738532-739137      - | ENSG00000269831\n              ...      ...               ...    ... .             ...\n  ENSG00000187191        Y 26909216-26959626      - | ENSG00000187191\n  ENSG00000205916        Y 26980008-27053183      + | ENSG00000205916\n  ENSG00000185894        Y 27177048-27208695      - | ENSG00000185894\n  ENSG00000172288        Y 27768264-27771049      + | ENSG00000172288\n  ENSG00000269393        Y 28111776-28114889      - | ENSG00000269393\n                    gene_name   gene_biotype seq_coord_system      symbol\n                  <character>    <character>      <character> <character>\n  ENSG00000186092       OR4F5 protein_coding       chromosome       OR4F5\n  ENSG00000237683  AL627309.1 protein_coding       chromosome  AL627309.1\n  ENSG00000235249      OR4F29 protein_coding       chromosome      OR4F29\n  ENSG00000185097      OR4F16 protein_coding       chromosome      OR4F16\n  ENSG00000269831  AL669831.1 protein_coding       chromosome  AL669831.1\n              ...         ...            ...              ...         ...\n  ENSG00000187191        DAZ3 protein_coding       chromosome        DAZ3\n  ENSG00000205916        DAZ4 protein_coding       chromosome        DAZ4\n  ENSG00000185894       BPY2C protein_coding       chromosome       BPY2C\n  ENSG00000172288        CDY1 protein_coding       chromosome        CDY1\n  ENSG00000269393  AC007965.1 protein_coding       chromosome  AC007965.1\n                                    entrezid\n                                      <list>\n  ENSG00000186092                      79501\n  ENSG00000237683 101929819,100996768,728728\n  ENSG00000235249         729759,81399,26683\n  ENSG00000185097         729759,81399,26683\n  ENSG00000269831                       <NA>\n              ...                        ...\n  ENSG00000187191                57055,57054\n  ENSG00000205916           57135,57055,1617\n  ENSG00000185894         442868,442867,9083\n  ENSG00000172288                253175,9085\n  ENSG00000269393                       <NA>\n  -------\n  seqinfo: 273 sequences (1 circular) from GRCh37 genome\n\n\nSNP list\n\nsnps_cluster1 <- c(\"rs1097327\",\"rs2186120\",\"rs2166172\",\"rs75641275\",\"rs12037698\",\"rs1446585\",\"rs16846140\",\"rs13062093\",\"rs2051559\",\"rs6861649\",\"rs2281819\",\"rs12662900\",\"rs9388681\",\"rs17132130\",\"rs215634\",\"rs79682948\",\"rs2192649\",\"rs13294945\",\"rs7357754\",\"rs4749937\",\"rs1465900\",\"rs1799992\",\"rs55938344\",\"rs7987928\",\"rs7331420\",\"rs9522279\",\"rs55689274\",\"rs4777541\",\"rs7189149\",\"rs11079849\",\"rs113230003\",\"rs150998792\")\n\nGet positions\n\nbmi <- associations(snps_cluster1, \"ukb-a-248\")\ntarget <- GRanges(bmi$chr, IRanges(bmi$position - 200000, bmi$position + 200000))\ntarget\n\nGRanges object with 30 ranges and 0 metadata columns:\n       seqnames              ranges strand\n          <Rle>           <IRanges>  <Rle>\n   [1]        1   23137075-23537075      *\n   [2]        1   66253163-66653163      *\n   [3]        1   91008514-91408514      *\n   [4]        1   98127133-98527133      *\n   [5]        1 243384345-243784345      *\n   ...      ...                 ...    ...\n  [26]       15   47558909-47958909      *\n  [27]       15   72882240-73282240      *\n  [28]       16   68944151-69344151      *\n  [29]       17   46890785-47290785      *\n  [30]       19   18260956-18660956      *\n  -------\n  seqinfo: 16 sequences from an unspecified genome; no seqlengths\n\n\nFind overlaps with genelist\n\noverlaps <- findOverlaps(target, genelist)\noverlaps\n\nHits object with 116 hits and 0 metadata columns:\n        queryHits subjectHits\n        <integer>   <integer>\n    [1]         1         306\n    [2]         1         307\n    [3]         1         308\n    [4]         1         309\n    [5]         1         310\n    ...       ...         ...\n  [112]        30        9758\n  [113]        30        9759\n  [114]        30        9760\n  [115]        30        9761\n  [116]        30        9762\n  -------\n  queryLength: 30 / subjectLength: 22810\n\n\nOverall about 116 overlapping protein coding genes found for 30 SNPs.\n\nTry with all ensembl annotations, not just protein coding\n\ngenelist_all <- genes(EnsDb.Hsapiens.v75)\ngenelist_all\n\nGRanges object with 64102 ranges and 6 metadata columns:\n                  seqnames            ranges strand |         gene_id\n                     <Rle>         <IRanges>  <Rle> |     <character>\n  ENSG00000223972        1       11869-14412      + | ENSG00000223972\n  ENSG00000227232        1       14363-29806      - | ENSG00000227232\n  ENSG00000243485        1       29554-31109      + | ENSG00000243485\n  ENSG00000237613        1       34554-36081      - | ENSG00000237613\n  ENSG00000268020        1       52473-54936      + | ENSG00000268020\n              ...      ...               ...    ... .             ...\n  ENSG00000224240        Y 28695572-28695890      + | ENSG00000224240\n  ENSG00000227629        Y 28732789-28737748      - | ENSG00000227629\n  ENSG00000237917        Y 28740998-28780799      - | ENSG00000237917\n  ENSG00000231514        Y 28772667-28773306      - | ENSG00000231514\n  ENSG00000235857        Y 59001391-59001635      + | ENSG00000235857\n                    gene_name gene_biotype seq_coord_system      symbol\n                  <character>  <character>      <character> <character>\n  ENSG00000223972     DDX11L1   pseudogene       chromosome     DDX11L1\n  ENSG00000227232      WASH7P   pseudogene       chromosome      WASH7P\n  ENSG00000243485  MIR1302-10      lincRNA       chromosome  MIR1302-10\n  ENSG00000237613     FAM138A      lincRNA       chromosome     FAM138A\n  ENSG00000268020      OR4G4P   pseudogene       chromosome      OR4G4P\n              ...         ...          ...              ...         ...\n  ENSG00000224240     CYCSP49   pseudogene       chromosome     CYCSP49\n  ENSG00000227629  SLC25A15P1   pseudogene       chromosome  SLC25A15P1\n  ENSG00000237917     PARP4P1   pseudogene       chromosome     PARP4P1\n  ENSG00000231514     FAM58CP   pseudogene       chromosome     FAM58CP\n  ENSG00000235857     CTBP2P1   pseudogene       chromosome     CTBP2P1\n                                           entrezid\n                                             <list>\n  ENSG00000223972               100287596,100287102\n  ENSG00000227232                  100287171,653635\n  ENSG00000243485 100422919,100422834,100422831,...\n  ENSG00000237613              654835,645520,641702\n  ENSG00000268020                              <NA>\n              ...                               ...\n  ENSG00000224240                              <NA>\n  ENSG00000227629                              <NA>\n  ENSG00000237917                              <NA>\n  ENSG00000231514                              <NA>\n  ENSG00000235857                              <NA>\n  -------\n  seqinfo: 273 sequences (1 circular) from GRCh37 genome\n\noverlaps1 <- findOverlaps(target, genelist_all)\noverlaps1\n\nHits object with 277 hits and 0 metadata columns:\n        queryHits subjectHits\n        <integer>   <integer>\n    [1]         1         725\n    [2]         1         728\n    [3]         1         729\n    [4]         1         730\n    [5]         1         731\n    ...       ...         ...\n  [273]        30       26788\n  [274]        30       26789\n  [275]        30       26790\n  [276]        30       26791\n  [277]        30       26792\n  -------\n  queryLength: 30 / subjectLength: 64102\n\n\nStill only 277\n\nTry with HumanHT-12 v3.0 probes. Download manifest file to get probe coordinates:\n\ntemp <- tempfile()\ndownload.file(\"https://emea.support.illumina.com/content/dam/illumina-support/documents/downloads/productfiles/humanht-12/v3/humanht-12_v3_0_r3_11283641_a_txt.zip\", temp)\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:GenomicRanges':\n\n    shift\n\n\nThe following object is masked from 'package:IRanges':\n\n    shift\n\n\nThe following objects are masked from 'package:S4Vectors':\n\n    first, second\n\na <- fread(paste0(\"unzip -p \", temp), skip=8)\n\nTaking input= as a system command ('unzip -p /var/folders/pp/6w3v08jx2p3ct76tptkmt0qh0000gq/T//RtmpYrlwQo/file7e866d3df1ff') and a variable has been used in the expression passed to `input=`. Please use fread(cmd=...). There is a security concern if you are creating an app, and the app could have a malicious user, and the app is not running in a secure environment; e.g. the app is running as root. Please read item 5 in the NEWS file for v1.11.6 for more information and for the option to suppress this message.\n\n\nWarning in fread(paste0(\"unzip -p \", temp), skip = 8): Stopped early on line\n48813. Expected 28 fields but found 1. Consider fill=TRUE and comment.char=.\nFirst discarded non-empty line: <<[Controls]>>\n\nstr(a)\n\nClasses 'data.table' and 'data.frame':  48803 obs. of  28 variables:\n $ Species              : chr  \"Homo sapiens\" \"Homo sapiens\" \"Homo sapiens\" \"Homo sapiens\" ...\n $ Source               : chr  \"RefSeq\" \"Unigene\" \"RefSeq\" \"RefSeq\" ...\n $ Search_Key           : chr  \"ILMN_44919\" \"ILMN_127219\" \"ILMN_139282\" \"ILMN_5006\" ...\n $ Transcript           : chr  \"ILMN_44919\" \"ILMN_127219\" \"ILMN_139282\" \"ILMN_5006\" ...\n $ ILMN_Gene            : chr  \"LOC23117\" \"HS.575038\" \"FCGR2B\" \"TRIM44\" ...\n $ Source_Reference_ID  : chr  \"XM_933824.1\" \"Hs.575038\" \"XM_938851.1\" \"NM_017583.3\" ...\n $ RefSeq_ID            : chr  \"XM_933824.1\" \"\" \"XM_938851.1\" \"NM_017583.3\" ...\n $ Unigene_ID           : chr  \"\" \"Hs.575038\" \"\" \"\" ...\n $ Entrez_Gene_ID       : int  23117 NA 2213 54765 653895 158833 387701 NA 56905 NA ...\n $ GI                   : int  89040007 10437021 88952550 29029528 89033487 61888901 89031576 27826545 153251858 1999235 ...\n $ Accession            : chr  \"XM_933824.1\" \"AK024680\" \"XM_938851.1\" \"NM_017583.3\" ...\n $ Symbol               : chr  \"LOC23117\" \"\" \"FCGR2B\" \"TRIM44\" ...\n $ Protein_Product      : chr  \"XP_938917.1\" \"\" \"XP_943944.1\" \"NP_060053.2\" ...\n $ Probe_Id             : chr  \"ILMN_1725881\" \"ILMN_1910180\" \"ILMN_1804174\" \"ILMN_1796063\" ...\n $ Array_Address_Id     : int  1710221 5900364 2480717 1300239 4480719 6020725 3870215 2710020 870110 290020 ...\n $ Probe_Type           : chr  \"I\" \"S\" \"I\" \"S\" ...\n $ Probe_Start          : int  122 1409 1643 2901 25 782 301 324 3585 139 ...\n $ Probe_Sequence       : chr  \"GGCTCCTCTTTGGGCTCCTACTGGAATTTATCAGCCATCAGTGCATCTCT\" \"ACACCTTCAGGAGGGAAGCCCTTATTTCTGGGTTGAACTCCCCTTCCATG\" \"TAGGGGCAATAGGCTATACGCTACAGCCTAGGTGTGTAGTAGGCCACACC\" \"CCTGCCTGTCTGCCTGTGACCTGTGTACGTATTACAGGCTTTAGGACCAG\" ...\n $ Chromosome           : chr  \"16\" \"2\" \"\" \"11\" ...\n $ Probe_Chr_Orientation: chr  \"-\" \"+\" \"\" \"+\" ...\n $ Probe_Coordinates    : chr  \"21766363-21766363:21769901-21769949\" \"206352194-206352243\" \"\" \"35786070-35786119\" ...\n $ Cytoband             : chr  \"16p12.2a\" \"\" \"1q23.3b\" \"11p13a\" ...\n $ Definition           : chr  \"PREDICTED: Homo sapiens KIAA0220-like protein, transcript variant 11 (LOC23117), mRNA.\" \"Homo sapiens cDNA: FLJ21027 fis, clone CAE07110\" \"PREDICTED: Homo sapiens Fc fragment of IgG, low affinity IIb, receptor (CD32) (FCGR2B), mRNA.\" \"Homo sapiens tripartite motif-containing 44 (TRIM44), mRNA.\" ...\n $ Ontology_Component   : chr  \"\" \"\" \"The membrane surrounding a cell that separates the cell from its external environment. It consists of a phospho\"| __truncated__ \"\" ...\n $ Ontology_Process     : chr  \"\" \"\" \"Any immune system process that functions in the calibrated response of an organism to a potential internal or i\"| __truncated__ \"\" ...\n $ Ontology_Function    : chr  \"\" \"\" \"Combining with an extracellular or intracellular messenger to initiate a change in cell activity [goid 4872] [e\"| __truncated__ \"\" ...\n $ Synonyms             : chr  \"\" \"\" \"\" \"MGC3490; MC7; HSA249128; DIPB\" ...\n $ Obsolete_Probe_Id    : chr  \"\" \"\" \"\" \"MGC3490; MC7; HSA249128; DIPB\" ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\na$bp <- sapply(strsplit(a$Probe_Coordinates, \"-\"), function(o) as.numeric(o[1]))\nstr(a)\n\nClasses 'data.table' and 'data.frame':  48803 obs. of  29 variables:\n $ Species              : chr  \"Homo sapiens\" \"Homo sapiens\" \"Homo sapiens\" \"Homo sapiens\" ...\n $ Source               : chr  \"RefSeq\" \"Unigene\" \"RefSeq\" \"RefSeq\" ...\n $ Search_Key           : chr  \"ILMN_44919\" \"ILMN_127219\" \"ILMN_139282\" \"ILMN_5006\" ...\n $ Transcript           : chr  \"ILMN_44919\" \"ILMN_127219\" \"ILMN_139282\" \"ILMN_5006\" ...\n $ ILMN_Gene            : chr  \"LOC23117\" \"HS.575038\" \"FCGR2B\" \"TRIM44\" ...\n $ Source_Reference_ID  : chr  \"XM_933824.1\" \"Hs.575038\" \"XM_938851.1\" \"NM_017583.3\" ...\n $ RefSeq_ID            : chr  \"XM_933824.1\" \"\" \"XM_938851.1\" \"NM_017583.3\" ...\n $ Unigene_ID           : chr  \"\" \"Hs.575038\" \"\" \"\" ...\n $ Entrez_Gene_ID       : int  23117 NA 2213 54765 653895 158833 387701 NA 56905 NA ...\n $ GI                   : int  89040007 10437021 88952550 29029528 89033487 61888901 89031576 27826545 153251858 1999235 ...\n $ Accession            : chr  \"XM_933824.1\" \"AK024680\" \"XM_938851.1\" \"NM_017583.3\" ...\n $ Symbol               : chr  \"LOC23117\" \"\" \"FCGR2B\" \"TRIM44\" ...\n $ Protein_Product      : chr  \"XP_938917.1\" \"\" \"XP_943944.1\" \"NP_060053.2\" ...\n $ Probe_Id             : chr  \"ILMN_1725881\" \"ILMN_1910180\" \"ILMN_1804174\" \"ILMN_1796063\" ...\n $ Array_Address_Id     : int  1710221 5900364 2480717 1300239 4480719 6020725 3870215 2710020 870110 290020 ...\n $ Probe_Type           : chr  \"I\" \"S\" \"I\" \"S\" ...\n $ Probe_Start          : int  122 1409 1643 2901 25 782 301 324 3585 139 ...\n $ Probe_Sequence       : chr  \"GGCTCCTCTTTGGGCTCCTACTGGAATTTATCAGCCATCAGTGCATCTCT\" \"ACACCTTCAGGAGGGAAGCCCTTATTTCTGGGTTGAACTCCCCTTCCATG\" \"TAGGGGCAATAGGCTATACGCTACAGCCTAGGTGTGTAGTAGGCCACACC\" \"CCTGCCTGTCTGCCTGTGACCTGTGTACGTATTACAGGCTTTAGGACCAG\" ...\n $ Chromosome           : chr  \"16\" \"2\" \"\" \"11\" ...\n $ Probe_Chr_Orientation: chr  \"-\" \"+\" \"\" \"+\" ...\n $ Probe_Coordinates    : chr  \"21766363-21766363:21769901-21769949\" \"206352194-206352243\" \"\" \"35786070-35786119\" ...\n $ Cytoband             : chr  \"16p12.2a\" \"\" \"1q23.3b\" \"11p13a\" ...\n $ Definition           : chr  \"PREDICTED: Homo sapiens KIAA0220-like protein, transcript variant 11 (LOC23117), mRNA.\" \"Homo sapiens cDNA: FLJ21027 fis, clone CAE07110\" \"PREDICTED: Homo sapiens Fc fragment of IgG, low affinity IIb, receptor (CD32) (FCGR2B), mRNA.\" \"Homo sapiens tripartite motif-containing 44 (TRIM44), mRNA.\" ...\n $ Ontology_Component   : chr  \"\" \"\" \"The membrane surrounding a cell that separates the cell from its external environment. It consists of a phospho\"| __truncated__ \"\" ...\n $ Ontology_Process     : chr  \"\" \"\" \"Any immune system process that functions in the calibrated response of an organism to a potential internal or i\"| __truncated__ \"\" ...\n $ Ontology_Function    : chr  \"\" \"\" \"Combining with an extracellular or intracellular messenger to initiate a change in cell activity [goid 4872] [e\"| __truncated__ \"\" ...\n $ Synonyms             : chr  \"\" \"\" \"\" \"MGC3490; MC7; HSA249128; DIPB\" ...\n $ Obsolete_Probe_Id    : chr  \"\" \"\" \"\" \"MGC3490; MC7; HSA249128; DIPB\" ...\n $ bp                   : num  2.18e+07 2.06e+08 NA 3.58e+07 NA ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\na <- subset(a, !is.na(bp))\ndim(a)\n\n[1] 42945    29\n\nb <- GRanges(a$Chromosome, IRanges(a$bp, a$bp), Transcript=a$Transcript)\noverlaps2 <- findOverlaps(target, b)\noverlaps2\n\nHits object with 165 hits and 0 metadata columns:\n        queryHits subjectHits\n        <integer>   <integer>\n    [1]         1        3073\n    [2]         1        3658\n    [3]         1       20306\n    [4]         1       25574\n    [5]         1       42634\n    ...       ...         ...\n  [161]        30       34706\n  [162]        30       36225\n  [163]        30       39738\n  [164]        30       39832\n  [165]        30       42330\n  -------\n  queryLength: 30 / subjectLength: 42945\n\n\nStill only 165\n\nLooks like the overlaps are actually only for eQTL SNP positions, rather than probe or annotation locations. So get MuTHER eQTL results, and lookup how many eQTLs overlap BMI SNPs +/-200kb\n\ntemp <- tempfile()\ndownload.file(\"http://www.muther.ac.uk/DataForWebsite/MuTHER_top_cis_eQTL_per_probe_Fat.txt.gz\", temp)\nb <- fread(paste0(\"gunzip -c \", temp))\n\nTaking input= as a system command ('gunzip -c /var/folders/pp/6w3v08jx2p3ct76tptkmt0qh0000gq/T//RtmpYrlwQo/file7e8627edacbb') and a variable has been used in the expression passed to `input=`. Please use fread(cmd=...). There is a security concern if you are creating an app, and the app could have a malicious user, and the app is not running in a secure environment; e.g. the app is running as root. Please read item 5 in the NEWS file for v1.11.6 for more information and for the option to suppress this message.\n\nb\n\n       CHR        PROBE   Gene       TSS       TSE        SNP SNP_INFO\n    1:   1 ILMN_1651229  IPO13  44185064  44206280   rs621559    0.997\n    2:  16 ILMN_1651230 TESSP1   2788487   2795134  rs6600200    0.939\n    3:   4 ILMN_1651235  AFAP1   7811339   7992553 rs13122226    0.853\n    4:  16 ILMN_1651237   CDT1  87397686  87403167  rs7500824    0.924\n    5:  17 ILMN_1651238  TRPV1   3415489   3445874 rs11078512    1.000\n   ---                                                                \n23223:  16 ILMN_2415776   WWOX  76691051  76691596  rs4887991    0.918\n23224:   3 ILMN_2415786   CD96 112743615 112853896  rs2634545    0.968\n23225:  17 ILMN_2415826  CYTSB  19999993  20081083  rs9911605    0.907\n23226:  23 ILMN_2415911  ENOX2 129585037 129864889  rs5932853    1.000\n23227:   9 ILMN_2415949   MRRF 124066702 124125561 rs12156510    0.851\n        SNP_Coor A1 Freq_A1   Fat_beta Fat_sebeta     Fat_p     LCL_beta\n    1:  43417998  G  0.9370  0.0779514 0.02857380 0.0064000  0.017058900\n    2:   2383771  T  0.3520 -0.0145240 0.00513359 0.0047000  0.004936040\n    3:   8171020  T  0.7380  0.0182723 0.00577418 0.0016000 -0.007693140\n    4:  86856992  G  0.6750  0.0203629 0.00648465 0.0017000  0.046034300\n    5:   4345191  G  0.8340 -0.0239871 0.00733934 0.0011000 -0.000181549\n   ---                                                                  \n23223:  77478564  G  0.3230  0.0650150 0.01937710 0.0007929  0.001083580\n23224: 112459042  T  0.6060  0.0459368 0.01336570 0.0005884 -0.051241700\n23225:  19957683  T  0.1950  0.0240481 0.00645100 0.0001932 -0.003798860\n23226: 130113107  T  0.2265 -0.0298614 0.01057420 0.0047000 -0.024893400\n23227: 123959794  G  0.2710  0.0456188 0.01213610 0.0001706  0.006575610\n       LCL_sebeta  LCL_p    Skin_beta Skin_sebeta Skin_p\n    1: 0.02432060 0.4830 -0.010652100  0.02916640 0.7149\n    2: 0.00545402 0.3655 -0.005986190  0.00569460 0.2932\n    3: 0.00617687 0.2130  0.003836190  0.00581005 0.5091\n    4: 0.02385430 0.0536 -0.021171000  0.01904700 0.2663\n    5: 0.00746635 0.9806 -0.000843489  0.00764613 0.9122\n   ---                                                  \n23223: 0.01777060 0.9514 -0.003480850  0.02788380 0.9007\n23224: 0.03088650 0.0971 -0.015326200  0.01731080 0.3760\n23225: 0.00897785 0.6722  0.001083930  0.00802034 0.8925\n23226: 0.01307960 0.0570  0.001724490  0.01235640 0.8890\n23227: 0.01614070 0.6837  0.014399600  0.01556960 0.3550\n\nbg <- GRanges(b$CHR, IRanges(b$SNP_Coor, b$SNP_Coor), Gene=b$Gene)\nbg\n\nGRanges object with 23227 ranges and 1 metadata column:\n          seqnames    ranges strand |        Gene\n             <Rle> <IRanges>  <Rle> | <character>\n      [1]        1  43417998      * |       IPO13\n      [2]       16   2383771      * |      TESSP1\n      [3]        4   8171020      * |       AFAP1\n      [4]       16  86856992      * |        CDT1\n      [5]       17   4345191      * |       TRPV1\n      ...      ...       ...    ... .         ...\n  [23223]       16  77478564      * |        WWOX\n  [23224]        3 112459042      * |        CD96\n  [23225]       17  19957683      * |       CYTSB\n  [23226]       23 130113107      * |       ENOX2\n  [23227]        9 123959794      * |        MRRF\n  -------\n  seqinfo: 23 sequences from an unspecified genome; no seqlengths\n\noverlaps3 <- findOverlaps(target, bg)\nlength(unique(b$Gene[overlaps3@to]))\n\n[1] 106\n\n\nStill not much. But I think in the script it suggests that all cis-SNPs were included without filtering for p-value, so actually it’s looking for any overlap with any genes +/- 1Mb?\n\nbg <- GRanges(b$CHR, IRanges(b$TSS-1000000, b$TSS+1000000), Gene=b$Gene)\nbg\n\nGRanges object with 23227 ranges and 1 metadata column:\n          seqnames              ranges strand |        Gene\n             <Rle>           <IRanges>  <Rle> | <character>\n      [1]        1   43185064-45185064      * |       IPO13\n      [2]       16     1788487-3788487      * |      TESSP1\n      [3]        4     6811339-8811339      * |       AFAP1\n      [4]       16   86397686-88397686      * |        CDT1\n      [5]       17     2415489-4415489      * |       TRPV1\n      ...      ...                 ...    ... .         ...\n  [23223]       16   75691051-77691051      * |        WWOX\n  [23224]        3 111743615-113743615      * |        CD96\n  [23225]       17   18999993-20999993      * |       CYTSB\n  [23226]       23 128585037-130585037      * |       ENOX2\n  [23227]        9 123066702-125066702      * |        MRRF\n  -------\n  seqinfo: 23 sequences from an unspecified genome; no seqlengths\n\noverlaps3 <- findOverlaps(target, bg)\nlength(unique(b$Gene[overlaps3@to]))\n\n[1] 504\n\n\nNow it’s quite a it higher.\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] data.table_1.14.2         dplyr_1.0.10             \n [3] ieugwasr_0.1.5            EnsDb.Hsapiens.v75_2.99.0\n [5] ensembldb_2.22.0          AnnotationFilter_1.22.0  \n [7] GenomicFeatures_1.50.4    AnnotationDbi_1.60.0     \n [9] Biobase_2.56.0            GenomicRanges_1.50.2     \n[11] GenomeInfoDb_1.34.9       IRanges_2.32.0           \n[13] S4Vectors_0.36.1          BiocGenerics_0.44.0      \n\nloaded via a namespace (and not attached):\n [1] MatrixGenerics_1.8.1        httr_1.4.4                 \n [3] bit64_4.0.5                 jsonlite_1.8.4             \n [5] assertthat_0.2.1            BiocFileCache_2.4.0        \n [7] blob_1.2.3                  GenomeInfoDbData_1.2.8     \n [9] Rsamtools_2.14.0            yaml_2.3.6                 \n[11] progress_1.2.2              pillar_1.8.1               \n[13] RSQLite_2.2.20              lattice_0.20-45            \n[15] glue_1.6.2                  digest_0.6.31              \n[17] XVector_0.38.0              htmltools_0.5.4            \n[19] Matrix_1.5-3                XML_3.99-0.13              \n[21] pkgconfig_2.0.3             biomaRt_2.54.0             \n[23] zlibbioc_1.42.0             BiocParallel_1.30.4        \n[25] tibble_3.1.8                KEGGREST_1.38.0            \n[27] generics_0.1.3              ellipsis_0.3.2             \n[29] cachem_1.0.6                SummarizedExperiment_1.28.0\n[31] lazyeval_0.2.2              cli_3.5.0                  \n[33] magrittr_2.0.3              crayon_1.5.2               \n[35] memoise_2.0.1               evaluate_0.19              \n[37] fansi_1.0.3                 xml2_1.3.3                 \n[39] tools_4.2.1                 prettyunits_1.1.1          \n[41] hms_1.1.2                   BiocIO_1.8.0               \n[43] lifecycle_1.0.3             matrixStats_0.63.0         \n[45] stringr_1.5.0               DelayedArray_0.24.0        \n[47] Biostrings_2.66.0           compiler_4.2.1             \n[49] rlang_1.0.6                 grid_4.2.1                 \n[51] RCurl_1.98-1.9              rjson_0.2.21               \n[53] rappdirs_0.3.3              htmlwidgets_1.5.4          \n[55] bitops_1.0-7                rmarkdown_2.16             \n[57] restfulr_0.0.15             codetools_0.2-18           \n[59] DBI_1.1.3                   curl_4.3.3                 \n[61] R6_2.5.1                    GenomicAlignments_1.34.0   \n[63] knitr_1.41                  rtracklayer_1.58.0         \n[65] fastmap_1.1.0               bit_4.0.5                  \n[67] utf8_1.2.2                  filelock_1.0.2             \n[69] ProtGenerics_1.30.0         stringi_1.7.8              \n[71] parallel_4.2.1              Rcpp_1.0.9                 \n[73] vctrs_0.5.1                 png_0.1-8                  \n[75] dbplyr_2.2.1                tidyselect_1.2.0           \n[77] xfun_0.36"
  },
  {
    "objectID": "posts/2023-02-07-mqtl-interaction/index.html",
    "href": "posts/2023-02-07-mqtl-interaction/index.html",
    "title": "Cell-specific effects for mQTLs from bulk tissue",
    "section": "",
    "text": "What is the data generating model for the mQTL x celltype interaction analysis?\nDoes this rescue the per-celltype mQTL effects?"
  },
  {
    "objectID": "posts/2023-02-07-mqtl-interaction/index.html#basic-simulation",
    "href": "posts/2023-02-07-mqtl-interaction/index.html#basic-simulation",
    "title": "Cell-specific effects for mQTLs from bulk tissue",
    "section": "Basic simulation",
    "text": "Basic simulation\n\nFive cell types\n10k individuals\nDifferent SNP effect on methylation in each cell type\nEach individual has a different cell type proportion\nBulk tissue is the weighted average of all the cell types (weighted by cell type proportion in the individual)\nCan we recapitulate the cell-type specific effect through the interaction term?\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nsim <- function(nc, n)\n{\n    g <- rbinom(n, 2, 0.4)\n    betas <- runif(nc, -2, 2)\n    m <- sapply(1:nc, function(i)\n    {\n        g * betas[i] + rnorm(n)\n    })\n    # for each individual sample cell type proportions\n    cellprop <- sapply(1:n, function(x) {a <- runif(nc); a/sum(a)}) %>% t()\n    # weighted sum\n    M <- (scale(m) * cellprop) %>% rowSums\n    res <- sapply(1:nc, function(i)\n    {\n      summary(lm(M ~ g * cellprop[,i]))$coef[4,1]\n    })\n    return(tibble(res, betas))\n}\n\no <- lapply(1:1000, function(i) sim(5, 10000) %>% mutate(sim=i)) %>% bind_rows()\no\n\n# A tibble: 5,000 × 3\n       res   betas   sim\n     <dbl>   <dbl> <int>\n 1 -0.820  -1.59       1\n 2  0.358  -0.0905     1\n 3  0.0370 -0.481      1\n 4 -0.841  -1.62       1\n 5  1.24    0.459      1\n 6  1.70    1.34       2\n 7 -0.870  -1.35       2\n 8 -0.902  -1.39       2\n 9  0.942   0.500      2\n10 -0.788  -1.45       2\n# … with 4,990 more rows\n\n\n\nggplot(o, aes(x=betas, y=res)) +\ngeom_point() +\ngeom_abline(colour=\"red\") +\ngeom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nGenerally seems to work but expect some shrinkage of large effects"
  },
  {
    "objectID": "posts/2023-02-07-mqtl-interaction/index.html#introduce-measurement-error-in-cell-type-proportions",
    "href": "posts/2023-02-07-mqtl-interaction/index.html#introduce-measurement-error-in-cell-type-proportions",
    "title": "Cell-specific effects for mQTLs from bulk tissue",
    "section": "Introduce measurement error in cell-type proportions",
    "text": "Introduce measurement error in cell-type proportions\n\ncellprop_noise <- function(cellprop, sigma)\n{\n    apply(cellprop, 1, function(x)\n    {\n        a <- rnorm(length(x), x, sigma)\n        a / sum(a)\n    }) %>% t()\n}\n\n\nsim2 <- function(nc, n, noise_sigma)\n{\n    g <- rbinom(n, 2, 0.4)\n    betas <- runif(nc, -2, 2)\n    m <- sapply(1:nc, function(i)\n    {\n        g * betas[i] + rnorm(n)\n    })\n    # for each individual sample cell type proportions\n    cellprop <- sapply(1:n, function(x) {a <- runif(nc); a/sum(a)}) %>% t()\n    cpn <- cellprop_noise(cellprop, noise_sigma)\n    # weighted sum\n    M <- (scale(m) * cellprop) %>% rowSums\n    res <- sapply(1:nc, function(i)\n    {\n      summary(lm(M ~ g * cpn[,i]))$coef[4,1]\n    })\n    return(tibble(res, betas))\n}\n\no2 <- lapply(1:1000, function(i) {\n    s <- sample(c(0, 0.05, 0.1), 1)\n    sim2(5, 10000, s) %>% mutate(sim=i, s=s)\n}) %>% bind_rows()\no2\n\n# A tibble: 5,000 × 4\n        res  betas   sim     s\n      <dbl>  <dbl> <int> <dbl>\n 1  1.32     0.524     1  0.05\n 2 -0.106   -0.890     1  0.05\n 3 -0.231   -0.982     1  0.05\n 4 -0.480   -1.77      1  0.05\n 5 -0.482   -1.48      1  0.05\n 6  0.102    0.141     2  0.05\n 7 -0.00435  0.107     2  0.05\n 8 -0.738   -0.825     2  0.05\n 9 -0.160   -0.188     2  0.05\n10  0.787    0.834     2  0.05\n# … with 4,990 more rows\n\n\n\nggplot(o2, aes(x=betas, y=res)) +\ngeom_point() +\ngeom_smooth() +\ngeom_abline(colour=\"red\") +\nfacet_wrap(~ s)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nNoisy estimates of cell type proportions will lead to attenuated effect estimates\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.0 dplyr_1.0.10 \n\nloaded via a namespace (and not attached):\n [1] pillar_1.8.1      compiler_4.2.1    tools_4.2.1       digest_0.6.31    \n [5] jsonlite_1.8.4    evaluate_0.19     lifecycle_1.0.3   tibble_3.1.8     \n [9] gtable_0.3.1      nlme_3.1-158      lattice_0.20-45   mgcv_1.8-40      \n[13] pkgconfig_2.0.3   rlang_1.0.6       Matrix_1.4-1      DBI_1.1.3        \n[17] cli_3.5.0         yaml_2.3.6        xfun_0.36         fastmap_1.1.0    \n[21] withr_2.5.0       stringr_1.5.0     knitr_1.41        generics_0.1.3   \n[25] vctrs_0.5.1       htmlwidgets_1.5.4 grid_4.2.1        tidyselect_1.2.0 \n[29] glue_1.6.2        R6_2.5.1          fansi_1.0.3       rmarkdown_2.16   \n[33] farver_2.1.1      magrittr_2.0.3    scales_1.2.1      htmltools_0.5.4  \n[37] splines_4.2.1     assertthat_0.2.1  colorspace_2.0-3  labeling_0.4.2   \n[41] utf8_1.2.2        stringi_1.7.8     munsell_0.5.0"
  },
  {
    "objectID": "posts/2023-02-28-mz-vqtl/index.html",
    "href": "posts/2023-02-28-mz-vqtl/index.html",
    "title": "Variance QTL using MZs",
    "section": "",
    "text": "Data generating model\n\\[\ny_i = \\alpha + \\beta_{1,j}G_{ij} + z_i + v_i + e_i\n\\]\nwhere \\(\\alpha\\) is an intercept term, \\(\\beta_{1,j}\\) is the additive effect of SNP $j$, \\(G_{i,j}\\) is the genotype value for individual \\(i\\) at SNP $j$, \\(z_i\\) is the remaining polygenic risk\n\\[\nz_i \\sim N(0, \\sigma^2_{g} - 2p_j(1-p_j)\\beta_{1,j}^2)\n\\]\n\\(v_i\\) is the SNP’s influence on dispersion where\n\\[\nv_i \\sim N(0, \\beta_{2,j}G_{i,j})\n\\]\nand \\(e_i\\) is the residual variance\n\\[\ne_i \\sim N(0, 1 - \\sigma^2_g - \\sigma^2_v)\n\\]\nTo estimate dispersion effects amongst unrelateds use the deviation regression model (DRM) from Marderstein et al 2021 AJHG (https://doi.org/10.1016/j.ajhg.2020.11.016).\nTo estimate variance heterogeneity effects using MZs, simply\n\\[\n|y_{i,A} - y_{i,B}| = \\hat{\\beta}_{2,j}G_{i,j} + \\epsilon_i\n\\]\nwhere A and B represent the individuals in the MZ pair.\nTo estimate variance heterogeneity effects using siblings, it is identical to the MZ method but restricted to sibling pairs who have identity by state value of 2 (i.e. share the same genotype value at the SNP being tested)."
  },
  {
    "objectID": "posts/2023-02-28-mz-vqtl/index.html#simulations",
    "href": "posts/2023-02-28-mz-vqtl/index.html#simulations",
    "title": "Variance QTL using MZs",
    "section": "Simulations",
    "text": "Simulations\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(simulateGP)\n\nMethod for simulating dispersion effects in unrelated individuals\n\nsim_pop <- function(n, beta1, beta2, af, h2)\n{\n  g <- rbinom(n, 2, af)\n  prs <- g * beta1\n  vg <- rnorm(n, 0, h2)\n  v <- rnorm(n, 0, beta2 * g)\n  ve <- rnorm(n, 0, sqrt(1 - var(vg) - var(v) - var(prs)))\n  y <- prs + v + vg + ve\n  return(tibble(\n    g, y\n  ))\n}\na <- sim_pop(100000, 0.1, 0.5, 0.3, 0.1)\nvar(a)\n\n           g          y\ng 0.42156007 0.04317622\ny 0.04317622 1.00325833\n\n\nMethod for simulating dispersion effects in monozogytic twins\n\nsim_mz <- function(n, beta1, beta2, af, h2)\n{\n  g <- rbinom(n, 2, af)\n  prs <- g * beta1\n  vg <- rnorm(n, 0, h2)\n  v1 <- rnorm(n, 0, beta2 * g)\n  ve1 <- rnorm(n, 0, sqrt(1 - var(vg) - var(v1) - var(prs)))\n  y1 <- prs + v1 + vg + ve1\n  v2 <- rnorm(n, 0, beta2 * g)\n  ve2 <- rnorm(n, 0, sqrt(1 - var(vg) - var(v2) - var(prs)))\n  y2 <- prs + v2 + vg + ve2\n  return(tibble(\n    g, y1, y2\n  ))\n}\na <- sim_mz(100000, 0.1, 0.5, 0.3, 0.1)\nvar(a)\n\n            g         y1         y2\ng  0.42132978 0.04268190 0.04133108\ny1 0.04268190 0.99914199 0.01694312\ny2 0.04133108 0.01694312 1.00232809\n\n\nSee what dispersion looks like from this simulation\n\na %>%\n  ggplot(., aes(x=as.factor(g), y=y1)) +\n  geom_boxplot()\n\n\n\n\nSummarise the dispersion. Note - how would you scale it to the MZ pair mean?\n\na <- sim_mz(100000, 0.1, 0.5, 0.3, 0.8)\na %>% \n  group_by(g) %>% \n  summarise(\n    m=mean(y1), \n    v=var(y1), \n    mzv=mean(abs(y1-y2))\n  )\n\n# A tibble: 3 × 4\n      g       m     v   mzv\n  <int>   <dbl> <dbl> <dbl>\n1     0 0.00572 0.799 0.455\n2     1 0.102   1.05  0.720\n3     2 0.193   1.84  1.22 \n\n\nMethod for testing unrelateds using DRM\n\ntest_drm <- function(g, y)\n{\n  y.i <- tapply(y, g, median, na.rm=T)  \n  z.ij <- abs(y - y.i[g+1])\n  summary(lm(z.ij ~ g))$coef %>%\n    as_tibble() %>%\n    slice(2) %>%\n    mutate(method=\"drm\")\n}\ntest_drm(a$g, a$y1)\n\n# A tibble: 1 × 5\n  Estimate `Std. Error` `t value` `Pr(>|t|)` method\n     <dbl>        <dbl>     <dbl>      <dbl> <chr> \n1    0.154      0.00295      52.1          0 drm   \n\n\nMethod for testing using MZs\n\ntest_mz <- function(g, y1, y2)\n{\n  yd1 <- abs(y1-y2)\n  r1 <- summary(lm(yd1 ~ g))$coef %>%\n    as_tibble() %>%\n    slice(2) %>%\n    mutate(method=\"mzdiff\")\n  r1\n}\n\ntest_mz(a$g, a$y1, a$y2)\n\n# A tibble: 1 × 5\n  Estimate `Std. Error` `t value` `Pr(>|t|)` method\n     <dbl>        <dbl>     <dbl>      <dbl> <chr> \n1    0.336      0.00250      134.          0 mzdiff"
  },
  {
    "objectID": "posts/2023-02-28-mz-vqtl/index.html#power-simulations",
    "href": "posts/2023-02-28-mz-vqtl/index.html#power-simulations",
    "title": "Variance QTL using MZs",
    "section": "Power simulations",
    "text": "Power simulations\nA trait with high heritability will have vQTL (dispersion) effects that are relatively large in MZs, but heritability shouldn’t have a major part to play in unrelateds for estimating vQTL effects.\nStart with simulations where population is compared against mz and n pop = n mz pairs.\n\nparam <- expand.grid(\n  beta1 = 0,\n  beta2 = seq(0, 0.5, by=0.01),\n  h2 = c(0.1, 0.9),\n  af = 0.3,\n  n = 10000\n)\ndim(param)\n\n[1] 102   5\n\nres1 <- lapply(1:nrow(param), function(i)\n  {\n  a <- do.call(sim_mz, param[i,])\n  if(any(is.na(a$y1)) | any(is.na(a$y2)))\n  {\n    return(NULL)\n  }\n  bind_rows(\n    test_mz(a$g, a$y1, a$y2),\n    test_drm(a$g, a$y1)\n  ) %>%\n    bind_cols(., param[i,])\n}) %>%\n  bind_rows()\n\nWarning in sqrt(1 - var(vg) - var(v1) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v1) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v2) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v2) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v1) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v1) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v2) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v2) - var(prs))): NAs produced\n\n\n\nres1 %>% filter(n==10000) %>%\nggplot(., aes(x=beta2, y=-log10(`Pr(>|t|)`))) +\n  geom_line(aes(colour=method)) +\n  facet_grid(. ~ h2)\n\n\n\n\nThey are comparable at low heritability but as heritability increases, MZ method has a distinct advantage.\nNow compare with more realistic sample sizes, 10k mz pairs vs 500k unrelateds\n\nparam <- expand.grid(\n  beta1 = 0,\n  beta2 = seq(0, 0.5, by=0.01),\n  h2 = c(0.1, 0.9),\n  af = 0.32\n)\ndim(param)\n\n[1] 102   4\n\nres2 <- lapply(1:nrow(param), function(i)\n  {\n  a1 <- do.call(sim_mz, param[i,] %>% mutate(n=10000))\n  a2 <- do.call(sim_mz, param[i,] %>% mutate(n=500000))\n  if(any(is.na(a1$y1)) | any(is.na(a1$y2)) | any(is.na(a2$y1)) | any(is.na(a2$y2)))\n  {\n    return(NULL)\n  }\n  bind_rows(\n    test_mz(a1$g, a1$y1, a1$y2),\n    test_drm(a2$g, a2$y1)\n  ) %>%\n    bind_cols(., param[i,])\n}) %>%\n  bind_rows()\n\nWarning in sqrt(1 - var(vg) - var(v1) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v1) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v2) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v2) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v1) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v1) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v2) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v2) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v1) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v1) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v2) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v2) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v1) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v1) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v2) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v2) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v1) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v1) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v2) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v2) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v1) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v1) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v2) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v2) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v1) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v1) - var(prs))): NAs produced\n\n\nWarning in sqrt(1 - var(vg) - var(v2) - var(prs)): NaNs produced\n\n\nWarning in rnorm(n, 0, sqrt(1 - var(vg) - var(v2) - var(prs))): NAs produced\n\n\n\nggplot(res2, aes(x=beta2, y=-log10(`Pr(>|t|)`))) +\n  geom_line(aes(colour=method)) +\n  facet_grid(. ~ h2)\n\n\n\n\nIt looks like you’d just be better off with estimation in populations."
  },
  {
    "objectID": "posts/2023-02-28-mz-vqtl/index.html#type-1-error",
    "href": "posts/2023-02-28-mz-vqtl/index.html#type-1-error",
    "title": "Variance QTL using MZs",
    "section": "Type 1 error",
    "text": "Type 1 error\n\nparam <- expand.grid(\n  beta1 = seq(0, 0.5, by=0.002),\n  beta2 = 0,\n  h2 = c(0.1, 0.9),\n  af = 0.32\n)\ndim(param)\n\n[1] 502   4\n\nres3 <- lapply(1:nrow(param), function(i)\n  {\n  a1 <- do.call(sim_mz, param[i,] %>% mutate(n=10000))\n  a2 <- do.call(sim_mz, param[i,] %>% mutate(n=500000))\n  if(any(is.na(a1$y1)) | any(is.na(a1$y2)) | any(is.na(a2$y1)) | any(is.na(a2$y2)))\n  {\n    return(NULL)\n  }\n  bind_rows(\n    test_mz(a1$g, a1$y1, a1$y2),\n    test_drm(a2$g, a2$y1)\n  ) %>%\n    bind_cols(., param[i,])\n}) %>%\n  bind_rows()\n\nPlot type 1 error\n\nggplot(res3, aes(x=beta1, y=-log10(`Pr(>|t|)`))) +\n  geom_line(aes(colour=method)) +\n  facet_grid(. ~ h2)\n\n\n\n\nType 1 error and number of false positives after multiple testing correction\n\nres3 %>% \n  mutate(fdr=p.adjust(`Pr(>|t|)`, \"fdr\")) %>%\n  group_by(method, h2) %>%\n  summarise(\n    t1 = sum(`Pr(>|t|)` < 0.05, na.rm=T)/sum(!is.na(`Pr(>|t|)`)),\n    nfdr = sum(fdr < 0.05)/sum(!is.na(`Pr(>|t|)`))\n  )\n\n`summarise()` has grouped output by 'method'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   method [2]\n  method    h2     t1  nfdr\n  <chr>  <dbl>  <dbl> <dbl>\n1 drm      0.1 0.0478     0\n2 drm      0.9 0.0438     0\n3 mzdiff   0.1 0.0558     0\n4 mzdiff   0.9 0.0398     0\n\n\nUnder normal distribution type-1 error rate is well controlled."
  },
  {
    "objectID": "posts/2023-02-28-mz-vqtl/index.html#simulations-using-siblings",
    "href": "posts/2023-02-28-mz-vqtl/index.html#simulations-using-siblings",
    "title": "Variance QTL using MZs",
    "section": "Simulations using siblings",
    "text": "Simulations using siblings\nGenerate a set of sib pairs with the following specification\n\nHave PRS such that IBD ~ N(0.5, sqrt(0.037))\nh2 specified\nce specified (shared variance between sibs\none SNP has a mean and variance effect\nscaled phenotypes ~ N(0, 1)\n\n\nsim_sibs <- function(af, nfam, beta1, beta2, h2, c2)\n{\n  # Choose number of SNPs to be expected number of recombination events\n  # in order to give appropriate distribution of IBD\n    nsnp <- 87\n    af <- rep(af, nsnp)\n    dads <- matrix(0, nfam, nsnp)\n    mums <- matrix(0, nfam, nsnp)\n    sibs1 <- matrix(0, nfam, nsnp)\n    sibs2 <- matrix(0, nfam, nsnp)\n    ibd <- matrix(0, nfam, nsnp)\n    ibs <- matrix(0, nfam, nsnp)\n    for(i in 1:nsnp)\n    {\n        dad1 <- rbinom(nfam, 1, af[i]) + 1\n        dad2 <- (rbinom(nfam, 1, af[i]) + 1) * -1\n        mum1 <- rbinom(nfam, 1, af[i]) + 1\n        mum2 <- (rbinom(nfam, 1, af[i]) + 1) * -1\n\n        dadindex <- sample(c(TRUE, FALSE), nfam, replace=TRUE)\n        dadh <- rep(NA, nfam)\n        dadh[dadindex] <- dad1[dadindex]\n        dadh[!dadindex] <- dad2[!dadindex]\n\n        mumindex <- sample(c(TRUE, FALSE), nfam, replace=TRUE)\n        mumh <- rep(NA, nfam)\n        mumh[mumindex] <- mum1[mumindex]\n        mumh[!mumindex] <- mum2[!mumindex]\n\n        sib1 <- cbind(dadh, mumh)\n\n        dadindex <- sample(c(TRUE, FALSE), nfam, replace=TRUE)\n        dadh <- rep(NA, nfam)\n        dadh[dadindex] <- dad1[dadindex]\n        dadh[!dadindex] <- dad2[!dadindex]\n\n        mumindex <- sample(c(TRUE, FALSE), nfam, replace=TRUE)\n        mumh <- rep(NA, nfam)\n        mumh[mumindex] <- mum1[mumindex]\n        mumh[!mumindex] <- mum2[!mumindex]\n\n        sib2 <- cbind(dadh, mumh)\n\n        ibd[,i] <- (as.numeric(sib1[,1] == sib2[,1]) + as.numeric(sib1[,2] == sib2[,2])) / 2\n\n        sibs1[,i] <- rowSums(abs(sib1) - 1)\n        sibs2[,i] <- rowSums(abs(sib2) - 1)\n        dads[,i] <- dad1 - 1 + abs(dad2) - 1\n        mums[,i] <- mum1 - 1 + abs(mum2) - 1\n\n        # l[[i]] <- (sum(sib1[,1] == sib2[,1]) / nsnp + sum(sib1[,2] == sib2[,2]) / nsnp) / 2\n\n    }\n    n <- nfam\n    # Make phenotypes\n    ce <- rnorm(n, 0, sqrt(c2))\n    v1 <- rnorm(n, 0, beta2 * sibs1[,1])\n    v2 <- rnorm(n, 0, beta2 * sibs2[,1])\n    e1 <- rnorm(n, 0, sqrt(1 - h2 - c2 - var(v1)))\n    e2 <- rnorm(n, 0, sqrt(1 - h2 - c2 - var(v2)))\n    b <- rnorm(nsnp-1, 0, 1)\n    h2_1 <- beta1^2 * af[1] * (1-af[1]) * 2\n    h2_res <- h2 - h2_1\n    prs1 <- scale(sibs1[,-1] %*% b) * sqrt(h2_res)\n    prs2 <- scale(sibs2[,-1] %*% b) * sqrt(h2_res)\n    y1 <- sibs1[,1] * beta1 + prs1 + v1 + ce + e1\n    y2 <- sibs2[,1] * beta1 + prs2 + v2 + ce + e2\n    return(tibble(\n      ibd = rowMeans(ibd),\n      g1 = sibs1[,1],\n      g2 = sibs2[,1],\n      prs1,\n      prs2,\n      y1, \n      y2\n    ))\n}\n\nNotes\n\nIn this model if the allele frequency is higher, then the genotype class with the larger variance is more common, which means that effect alleles are not reflexive in terms of variances\nI think this use with siblings may have problems due to LD. If there is incomplete LD with another causal variant elsewhere close by then the mean effect of that causal variant will contribute to the variance effect estimated at the variant\n\n\nfam <- sim_sibs(0.3, 10000, 0.1, 0.4, 0.6, 0.1)\ncor(fam) %>% round(4)\n\n         ibd      g1      g2    prs1   prs2      y1     y2\nibd   1.0000 -0.0032 -0.0183 -0.0068 0.0024 -0.0020 0.0062\ng1   -0.0032  1.0000  0.4970  0.0130 0.0195  0.0704 0.0395\ng2   -0.0183  0.4970  1.0000  0.0074 0.0160  0.0434 0.0750\nprs1 -0.0068  0.0130  0.0074  1.0000 0.5023  0.7720 0.3840\nprs2  0.0024  0.0195  0.0160  0.5023 1.0000  0.4043 0.7777\ny1   -0.0020  0.0704  0.0434  0.7720 0.4043  1.0000 0.4139\ny2    0.0062  0.0395  0.0750  0.3840 0.7777  0.4139 1.0000\n\n\nNow how to estimate variance effect? At a locus restrict to sib pairs who are IBD = 1\n\nfam <- sim_sibs(af=0.5, nfam=40000, beta1=0.1, beta2=0.2, h2=0.3, c2=0.1)\nfam\n\n# A tibble: 40,000 × 7\n     ibd    g1    g2 prs1[,1] prs2[,1]  y1[,1]  y2[,1]\n   <dbl> <dbl> <dbl>    <dbl>    <dbl>   <dbl>   <dbl>\n 1 0.477     0     2   0.542    0.307   0.958  -0.0552\n 2 0.552     1     1   0.0251  -0.459   0.888  -0.675 \n 3 0.5       1     1  -0.254   -0.164  -1.38   -0.596 \n 4 0.517     1     0   0.252   -0.522   1.72   -0.171 \n 5 0.517     1     1  -0.197   -0.183   0.343  -1.36  \n 6 0.534     1     1   0.838    0.526   1.03    1.16  \n 7 0.483     0     0   0.383   -0.524  -0.974  -0.727 \n 8 0.5       2     1  -0.291   -0.291   0.0133 -0.884 \n 9 0.534     1     1   0.310   -0.627   0.362  -0.852 \n10 0.494     0     0   0.576    0.0136  1.06   -1.01  \n# … with 39,990 more rows\n\n\nTest for variance QTL\n\nfam <- sim_sibs(af=0.5, nfam=40000, beta1=0.1, beta2=0.2, h2=0.01, c2=0.01)\nf1 <- subset(fam, g1==g2)\ntest_mz(f1$g1, f1$y1, f1$y2)\n\n# A tibble: 1 × 5\n  Estimate `Std. Error` `t value`   `Pr(>|t|)` method\n     <dbl>        <dbl>     <dbl>        <dbl> <chr> \n1   0.0438      0.00798      5.50 0.0000000394 mzdiff\n\n\nPower sims\n\nparam <- expand.grid(\n  beta1 = 0,\n  beta2 = seq(0, 0.5, by=0.01),\n  c2 = c(0),\n  h2 = c(0.1, 0.5),\n  af = c(0.5)\n)\ndim(param)\n\n[1] 102   5\n\nres4 <- lapply(1:nrow(param), function(i)\n  {\n  a1 <- do.call(sim_sibs, param[i,] %>% mutate(nfam=22000))\n  a2 <- do.call(sim_pop, param[i,] %>% select(-c(c2)) %>% mutate(n=500000))\n  if(any(is.na(a1$y1)) | any(is.na(a1$y2)) | any(is.na(a2$y)))\n  {\n    return(NULL)\n  }\n  bind_rows(\n    a1 %>% filter(g1==g2) %>% select(g=g1, y1, y2) %>% do.call(test_mz, .),\n    do.call(test_drm, a2)\n  ) %>%\n    bind_cols(., param[i,])\n}) %>%\n  bind_rows()\n\n\nres4 %>% mutate(sharing=c2+h2) %>% filter(beta1==0) %>%\n  ggplot(., aes(x=beta2, y=-log10(`Pr(>|t|)`))) +\n  geom_line(aes(colour=method, groups=af)) +\n  facet_grid(. ~ sharing)\n\nWarning in geom_line(aes(colour = method, groups = af)): Ignoring unknown\naesthetics: groups\n\n\n\n\n\nSimilar to MZ method - quite a substantial benefit from leveraging large sample sizes of unrelateds compared to using siblings."
  },
  {
    "objectID": "posts/2023-02-28-mz-vqtl/index.html#svlm",
    "href": "posts/2023-02-28-mz-vqtl/index.html#svlm",
    "title": "Variance QTL using MZs",
    "section": "SVLM",
    "text": "SVLM\nAdjust the phenotype for covariates + genotype, square residuals and then test in linear model against genotype.\n\ntest_svlm <- function(g, y)\n{\n  yres <- residuals(lm(y ~ g))^2\n  summary(lm(yres ~ g))$coefficients %>%\n    as_tibble() %>%\n    slice(n=2) %>%\n    mutate(method=\"svlm\")\n}\n\n\nn <- 10000\ng1 <- rbinom(n, 2, 0.4)\ng2 <- rbinom(n, 2, 0.4)\ny <- g1 + g2 + g1 * g2\nbind_rows(\n  test_drm(g1, y),\n  test_svlm(g1, y)\n)\n\n# A tibble: 2 × 5\n  Estimate `Std. Error` `t value` `Pr(>|t|)` method\n     <dbl>        <dbl>     <dbl>      <dbl> <chr> \n1    0.524       0.0139      37.7  2.06e-291 drm   \n2    1.85        0.0333      55.7  0         svlm  \n\n\n\ny <- g1 + g2 + rnorm(n)\nbind_rows(\n  test_drm(g1, y),\n  test_svlm(g1, y)\n)\n\n# A tibble: 2 × 5\n  Estimate `Std. Error` `t value` `Pr(>|t|)` method\n     <dbl>        <dbl>     <dbl>      <dbl> <chr> \n1 0.000577       0.0104    0.0557      0.956 drm   \n2 0.00402        0.0286    0.141       0.888 svlm  \n\n\n\ny <- g1 + g2 + g1 * g2\nyres <- residuals(lm(y ~ g1 + g2))^2\nsummary(lm(yres ~ g1))\n\n\nCall:\nlm(formula = yres ~ g1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2912 -0.2516 -0.1973  0.2402  1.7333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.155064   0.006387   24.28   <2e-16 ***\ng1          0.097849   0.006041   16.20   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4188 on 9998 degrees of freedom\nMultiple R-squared:  0.02557,   Adjusted R-squared:  0.02547 \nF-statistic: 262.4 on 1 and 9998 DF,  p-value: < 2.2e-16\n\n\n\nyres <- residuals(lm(y ~ g1))^2\nsummary(lm(yres ~ g1))\n\n\nCall:\nlm(formula = yres ~ g1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6900 -2.0229  0.2948  0.3860  8.8235 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.32939    0.03516   9.368   <2e-16 ***\ng1           1.85174    0.03325  55.686   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.305 on 9998 degrees of freedom\nMultiple R-squared:  0.2367,    Adjusted R-squared:  0.2367 \nF-statistic:  3101 on 1 and 9998 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/2023-02-28-mz-vqtl/index.html#inflation-due-to-ld",
    "href": "posts/2023-02-28-mz-vqtl/index.html#inflation-due-to-ld",
    "title": "Variance QTL using MZs",
    "section": "Inflation due to LD",
    "text": "Inflation due to LD\n\nres <- sapply(1:1000, function(i){\n  g <- correlated_binomial(n, 0.5, 0.5, 0.8)\n  g2 <- rbinom(n, 2, 0.5)\n  y <- g[,1] + rnorm(n)\n  summary(lm(y ~ g[,2]*g2))$coef[4,4] \n})\nhist(res)\n\n\n\n\n\ngendatp <- function(n, p1, p2, p3, r1)\n{\n    # dat <- simulateGP:::simulate_geno(n, r1, p1, p2) %>% as_tibble\n    dat <- correlated_binomial(n, p1, p2, r1) %>% as_tibble()\n    names(dat) <- c(\"y1\", \"y2\")\n    dat$y3 <- rbinom(n, 1, p3)\n    return(dat)\n}\n\nrun_simp <- function(param, i)\n{\n    set.seed(i*10)\n    dat <- gendatp(param$n[i], param$p1[i], param$p2[i], param$p3[i], param$r1[i])\n    x <- dat$y1 + rnorm(nrow(dat), sd=sd(dat$y1)/2)\n    mod1 <- lm(x ~ y2 + y3, dat)\n    mod2 <- lm(x ~ y2 + y3 + y2*y3, dat)\n    amod <- anova(mod1, mod2)\n    param$F[i] <- amod$F[2]\n    o1 <- test_drm(dat$y1, x)\n    o2 <- test_drm(dat$y2, x)\n    o3 <- test_drm(dat$y3, x)\n    param$drm1[i] <- o1$`Pr(>|t|)`\n    param$drm2[i] <- o2$`Pr(>|t|)`\n    param$drm3[i] <- o3$`Pr(>|t|)`\n    return(param[i,])\n}\nparam <- expand.grid(\n    p1=0.1,\n    p2=0.1,\n    p3=0.5,\n    p4=0.1,\n    n=1000,\n    r1=seq(0, 1, by=0.2),\n    sim=1:100,\n    r2=NA,\n    F=NA,\n    drm1=NA,\n    drm2=NA,\n    drm3=NA\n)\nresp <- lapply(1:nrow(param), function(x) run_simp(param, x)) %>% bind_rows()\nstr(resp)\n\n'data.frame':   600 obs. of  12 variables:\n $ p1  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p2  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p3  : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n $ p4  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ n   : num  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ...\n $ r1  : num  0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 ...\n $ sim : int  1 1 1 1 1 1 2 2 2 2 ...\n $ r2  : logi  NA NA NA NA NA NA ...\n $ F   : num  0.548 2.737 2.562 0.187 2.62 ...\n $ drm1: num  0.288 0.376 0.16 0.215 0.998 ...\n $ drm2: num  3.24e-02 1.39e-15 8.30e-18 1.03e-20 2.35e-14 ...\n $ drm3: num  0.9173 0.1256 0.0146 0.1303 0.6663 ...\n - attr(*, \"out.attrs\")=List of 2\n  ..$ dim     : Named int [1:12] 1 1 1 1 1 6 100 1 1 1 ...\n  .. ..- attr(*, \"names\")= chr [1:12] \"p1\" \"p2\" \"p3\" \"p4\" ...\n  ..$ dimnames:List of 12\n  .. ..$ p1  : chr \"p1=0.1\"\n  .. ..$ p2  : chr \"p2=0.1\"\n  .. ..$ p3  : chr \"p3=0.5\"\n  .. ..$ p4  : chr \"p4=0.1\"\n  .. ..$ n   : chr \"n=1000\"\n  .. ..$ r1  : chr [1:6] \"r1=0.0\" \"r1=0.2\" \"r1=0.4\" \"r1=0.6\" ...\n  .. ..$ sim : chr [1:100] \"sim=  1\" \"sim=  2\" \"sim=  3\" \"sim=  4\" ...\n  .. ..$ r2  : chr \"r2=NA\"\n  .. ..$ F   : chr \"F=NA\"\n  .. ..$ drm1: chr \"drm1=NA\"\n  .. ..$ drm2: chr \"drm2=NA\"\n  .. ..$ drm3: chr \"drm3=NA\"\n\n\nResult using the actual causal variant\n\nggplot(resp, aes(x=r1, y=-log10(drm1))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"Measurement precision of causal additive variant\", fill=\"LD between tagging\\nvariant and causal variant\")\n\n\n\n\nResult using\n\nggplot(resp, aes(x=r1, y=-log10(drm2))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"Measurement precision of causal additive variant\", fill=\"LD between tagging\\nvariant and causal variant\")\n\n\n\n\n\nggplot(resp, aes(x=r1, y=-log10(drm3))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"Measurement precision of causal additive variant\", fill=\"LD between tagging\\nvariant and causal variant\")\n\n\n\n\nLD issue persists with MZ analysis?\n\nsim_mz2 <- function(g, beta1, beta2, h2)\n{\n  n <- length(g)\n  prs <- g * beta1\n  vg <- rnorm(n, 0, h2)\n  v1 <- rnorm(n, 0, beta2 * g)\n  ve1 <- rnorm(n, 0, sqrt(1 - var(vg) - var(v1) - var(prs)))\n  y1 <- prs + v1 + vg + ve1\n  v2 <- rnorm(n, 0, beta2 * g)\n  ve2 <- rnorm(n, 0, sqrt(1 - var(vg) - var(v2) - var(prs)))\n  y2 <- prs + v2 + vg + ve2\n  return(tibble(\n    g, y1, y2\n  ))\n}\n\ntest_mz <- function(g, y1, y2)\n{\n  yd1 <- abs(y1-y2)\n  r1 <- summary(lm(yd1 ~ g))$coef %>%\n    as_tibble() %>%\n    slice(2) %>%\n    mutate(method=\"mzdiff\")\n  r1\n}\n\ngendatp <- function(n, p1, p2, p3, r1)\n{\n    # dat <- simulateGP:::simulate_geno(n, r1, p1, p2) %>% as_tibble\n    dat <- correlated_binomial(n, p1, p2, r1) %>% as_tibble()\n    names(dat) <- c(\"g1\", \"g2\")\n    dat$g3 <- rbinom(n, 1, p3)\n    return(dat)\n}\n\nrun_simp_mz <- function(param, i)\n{\n    set.seed(i*10)\n    dat <- gendatp(param$n[i], param$p1[i], param$p2[i], param$p3[i], param$r1[i])\n    mzdat <- sim_mz2(dat$g1, param$beta1[i], param$beta2[i], param$h2)\n    #x <- dat$y1 + rnorm(nrow(dat), sd=sd(dat$y1)/2)\n    o1 <- test_drm(dat$g1, mzdat$y1)\n    o2 <- test_drm(dat$g2, mzdat$y1)\n    o3 <- test_drm(dat$g3, mzdat$y1)\n    m1 <- test_mz(dat$g1, mzdat$y1, mzdat$y2)\n    m2 <- test_mz(dat$g2, mzdat$y1, mzdat$y2)\n    m3 <- test_mz(dat$g3, mzdat$y1, mzdat$y2)\n    param$drm1[i] <- o1$`Pr(>|t|)`\n    param$drm2[i] <- o2$`Pr(>|t|)`\n    param$drm3[i] <- o3$`Pr(>|t|)`\n    param$mz1[i] <- m1$`Pr(>|t|)`\n    param$mz2[i] <- m2$`Pr(>|t|)`\n    param$mz3[i] <- m3$`Pr(>|t|)`\n    return(param[i,])\n}\n\nparam <- expand.grid(\n    p1=0.1,\n    p2=0.1,\n    p3=0.5,\n    p4=0.1,\n    n=1000,\n    r1=seq(0, 1, by=0.2),\n    beta1=1,\n    beta2=0,\n    h2=0.5,\n    sim=1:500,\n    r2=NA,\n    F=NA,\n    drm1=NA,\n    drm2=NA,\n    drm3=NA,\n    mz1=NA,\n    mz2=NA,\n    mz3=NA\n)\n\nresmz <- lapply(1:nrow(param), function(i) run_simp_mz(param, i)) %>% bind_rows()\nresmz %>% str\n\n'data.frame':   3000 obs. of  18 variables:\n $ p1   : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p2   : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p3   : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n $ p4   : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ n    : num  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ...\n $ r1   : num  0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 ...\n $ beta1: num  1 1 1 1 1 1 1 1 1 1 ...\n $ beta2: num  0 0 0 0 0 0 0 0 0 0 ...\n $ h2   : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n $ sim  : int  1 1 1 1 1 1 2 2 2 2 ...\n $ r2   : logi  NA NA NA NA NA NA ...\n $ F    : logi  NA NA NA NA NA NA ...\n $ drm1 : num  0.717 0.117 0.951 0.239 0.899 ...\n $ drm2 : num  0.381 0.134 0.584 0.55 0.456 ...\n $ drm3 : num  0.587 0.158 0.228 0.775 0.928 ...\n $ mz1  : num  0.3104 0.0677 0.9304 0.1766 0.1589 ...\n $ mz2  : num  0.5393 0.2663 0.1479 0.0585 0.7089 ...\n $ mz3  : num  0.483 0.977 0.943 0.169 0.369 ...\n - attr(*, \"out.attrs\")=List of 2\n  ..$ dim     : Named int [1:18] 1 1 1 1 1 6 1 1 1 500 ...\n  .. ..- attr(*, \"names\")= chr [1:18] \"p1\" \"p2\" \"p3\" \"p4\" ...\n  ..$ dimnames:List of 18\n  .. ..$ p1   : chr \"p1=0.1\"\n  .. ..$ p2   : chr \"p2=0.1\"\n  .. ..$ p3   : chr \"p3=0.5\"\n  .. ..$ p4   : chr \"p4=0.1\"\n  .. ..$ n    : chr \"n=1000\"\n  .. ..$ r1   : chr [1:6] \"r1=0.0\" \"r1=0.2\" \"r1=0.4\" \"r1=0.6\" ...\n  .. ..$ beta1: chr \"beta1=1\"\n  .. ..$ beta2: chr \"beta2=0\"\n  .. ..$ h2   : chr \"h2=0.5\"\n  .. ..$ sim  : chr [1:500] \"sim=  1\" \"sim=  2\" \"sim=  3\" \"sim=  4\" ...\n  .. ..$ r2   : chr \"r2=NA\"\n  .. ..$ F    : chr \"F=NA\"\n  .. ..$ drm1 : chr \"drm1=NA\"\n  .. ..$ drm2 : chr \"drm2=NA\"\n  .. ..$ drm3 : chr \"drm3=NA\"\n  .. ..$ mz1  : chr \"mz1=NA\"\n  .. ..$ mz2  : chr \"mz2=NA\"\n  .. ..$ mz3  : chr \"mz3=NA\"\n\n\n\nggplot(resmz, aes(x=r1, y=-log10(drm2))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"LD\")\n\n\n\n\n\nresmz %>% \n  dplyr::select(r1, MZ=mz2, pop=drm2) %>% gather(., \"key\", \"value\", MZ, pop) %>%\n  ggplot(., aes(x=r1, y=-log10(value))) +\n  geom_boxplot(aes(fill=as.factor(r1))) +\n  scale_fill_brewer(type=\"seq\") +\n  facet_grid(. ~ key) +\n  labs(y=\"MZ dispersion -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"LD\")"
  },
  {
    "objectID": "posts/2023-02-28-mz-vqtl/index.html#summary",
    "href": "posts/2023-02-28-mz-vqtl/index.html#summary",
    "title": "Variance QTL using MZs",
    "section": "Summary",
    "text": "Summary\n\nUsing MZs is better powered than populations given equal sample sizes especially when h2 is large, but population sample sizes can be much larger which outweighs the power advantage of MZs\nMZs restricted to GxE or true variance heterogeneity (no GxG)\nMZs not liable to the LD leakage issue\nUsing DZs at loci where IBD=1 is possible which could boost power\nCould expand DZ model to all loci but would need to generate model appropriately.\n\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] simulateGP_0.1.2 tidyr_1.2.1      ggplot2_3.4.0    dplyr_1.0.10    \n\nloaded via a namespace (and not attached):\n [1] RColorBrewer_1.1-3 pillar_1.8.1       compiler_4.2.1     tools_4.2.1       \n [5] digest_0.6.31      jsonlite_1.8.4     evaluate_0.19      lifecycle_1.0.3   \n [9] tibble_3.1.8       gtable_0.3.1       pkgconfig_2.0.3    rlang_1.0.6       \n[13] DBI_1.1.3          cli_3.5.0          yaml_2.3.6         xfun_0.36         \n[17] fastmap_1.1.0      withr_2.5.0        stringr_1.5.0      knitr_1.41        \n[21] generics_0.1.3     vctrs_0.5.1        htmlwidgets_1.5.4  grid_4.2.1        \n[25] tidyselect_1.2.0   glue_1.6.2         R6_2.5.1           fansi_1.0.3       \n[29] rmarkdown_2.16     farver_2.1.1       purrr_1.0.0        magrittr_2.0.3    \n[33] ellipsis_0.3.2     scales_1.2.1       htmltools_0.5.4    assertthat_0.2.1  \n[37] colorspace_2.0-3   labeling_0.4.2     utf8_1.2.2         stringi_1.7.8     \n[41] munsell_0.5.0"
  },
  {
    "objectID": "posts/2023-01-26-metabolite-power-calculation/index.html",
    "href": "posts/2023-01-26-metabolite-power-calculation/index.html",
    "title": "Metabolite power calculation",
    "section": "",
    "text": "Sample of individuals who have had heart surgery, followed up and some number go on to have kidney disease. What is the predictive rsq of a metabolite on kidney disease outcome, that has 80% power to be detected after multiple testing correction?"
  },
  {
    "objectID": "posts/2023-01-26-metabolite-power-calculation/index.html#simulation",
    "href": "posts/2023-01-26-metabolite-power-calculation/index.html#simulation",
    "title": "Metabolite power calculation",
    "section": "Simulation",
    "text": "Simulation\nCall libraries\n\nlibrary(fmsb)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nDefine model\n\nsim <- function(ncase, ncontrol, b)\n{\n  met <- rnorm(ncase+ncontrol)\n  y <- rbinom(ncase+ncontrol, 1, plogis(log(ncase/(ncase+ncontrol)) + met*b + rnorm(ncase+ncontrol)))\n  table(y)\n  mod <- glm(y ~ met)\n  rsq <- NagelkerkeR2(mod)$R2\n  pval <- summary(mod)$coef[2,4]\n  return(tibble(rsq, pval, ncase, ncontrol, b))\n}\n\nSet parameters\n\n# Parameters\nntest <- 1500\nncase <- 70\nncontrol <- 100\n\nRun\n\nparam <- expand.grid(\n  b = seq(0, 1.5, by=0.01),\n  sim = 1:100\n)\nres <- lapply(1:nrow(param), function(i) {\n  sim(ncase=ncase, ncontrol=ncontrol, b=param$b[i])\n}) %>% bind_rows()\n\nVisualise\n\nres %>% group_by(b) %>%\n  summarise(rsq=mean(rsq), psig = sum(pval < (0.05/ntest))/n()) %>%\n  ggplot(., aes(x=rsq, y=psig)) +\n  geom_point() +\n  geom_hline(yintercept=0.8) +\n  labs(x=\"Negelkerke R2\", y=\"Power\", title=paste0(ncase, \" cases, \", ncontrol, \" controls, \", ntest, \" independent tests\"))\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.0 dplyr_1.0.10  fmsb_0.7.5   \n\nloaded via a namespace (and not attached):\n [1] knitr_1.41        magrittr_2.0.3    munsell_0.5.0     tidyselect_1.2.0 \n [5] colorspace_2.0-3  R6_2.5.1          rlang_1.0.6       fastmap_1.1.0    \n [9] fansi_1.0.3       stringr_1.5.0     tools_4.2.1       grid_4.2.1       \n[13] gtable_0.3.1      xfun_0.36         utf8_1.2.2        DBI_1.1.3        \n[17] cli_3.5.0         withr_2.5.0       htmltools_0.5.4   assertthat_0.2.1 \n[21] yaml_2.3.6        digest_0.6.31     tibble_3.1.8      lifecycle_1.0.3  \n[25] farver_2.1.1      htmlwidgets_1.5.4 vctrs_0.5.1       glue_1.6.2       \n[29] evaluate_0.19     rmarkdown_2.16    labeling_0.4.2    stringi_1.7.8    \n[33] compiler_4.2.1    pillar_1.8.1      scales_1.2.1      generics_0.1.3   \n[37] jsonlite_1.8.4    pkgconfig_2.0.3"
  },
  {
    "objectID": "posts/2022-05-08-prs-vs-ivw/index.html",
    "href": "posts/2022-05-08-prs-vs-ivw/index.html",
    "title": "PRS vs IVW",
    "section": "",
    "text": "How does PRS compare to IVW fixed effects analysis"
  },
  {
    "objectID": "posts/2022-05-08-prs-vs-ivw/index.html#simulation-study",
    "href": "posts/2022-05-08-prs-vs-ivw/index.html#simulation-study",
    "title": "PRS vs IVW",
    "section": "Simulation study",
    "text": "Simulation study\n\nlibrary(simulateGP)\ngeno1 <- make_geno(10000, 500, 0.5)\nb <- choose_effects(500, 0.3)\nx1 <- make_phen(b, geno1)\ny1 <- make_phen(0.4, x1)\n\ngeno2 <- make_geno(1000, 500, 0.5)\nx2 <- make_phen(b, geno2)\ny2 <- make_phen(0.4, x2)\n\nbhat <- gwas(x1, geno1)\nb_unweighted <- sign(b)\n\n\nStandard unweighted PRS analysis\n\nprs_unweighted <- geno2 %*% b_unweighted\nsummary(lm(x2 ~ prs_unweighted))\n\n\nCall:\nlm(formula = x2 ~ prs_unweighted)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2356 -0.5547 -0.0330  0.6087  3.1932 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -0.321168   0.035444  -9.061   <2e-16 ***\nprs_unweighted  0.027292   0.001791  15.239   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9011 on 998 degrees of freedom\nMultiple R-squared:  0.1888,    Adjusted R-squared:  0.1879 \nF-statistic: 232.2 on 1 and 998 DF,  p-value: < 2.2e-16\n\n\n\n\nMeta analysing per-SNP PRS scores\n\nlibrary(meta)\n\nLoading 'meta' package (version 6.0-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\no <- sapply(1:ncol(geno2), function(i)\n{\n  prs_unweighted <- geno2[,i] * b_unweighted[i]\n  summary(lm(x2 ~ prs_unweighted))$coef[2,1:2]\n})\nmetafor::rma(yi=o[1,], sei=o[2,], method=\"EE\")\n\n\nEqual-Effects Model (k = 500)\n\nI^2 (total heterogeneity / total variability):   17.58%\nH^2 (total variability / sampling variability):  1.21\n\nTest for Heterogeneity:\nQ(df = 499) = 605.4622, p-val = 0.0007\n\nModel Results:\n\nestimate      se     zval    pval   ci.lb   ci.ub      \n  0.0277  0.0020  13.8615  <.0001  0.0238  0.0316  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nStandard errors with number of SNPs\n\nses_unweighted <- sapply(1:ncol(geno2), function(i)\n{\n  prs_unweighted <- geno2[,1:i, drop=FALSE] %*% b_unweighted[1:i]\n  summary(lm(x2 ~ prs_unweighted))$coef[2,2]\n})\nplot(ses_unweighted)"
  },
  {
    "objectID": "posts/2022-10-25-regression-non-iid/index.html",
    "href": "posts/2022-10-25-regression-non-iid/index.html",
    "title": "Regression with non i.i.d. samples",
    "section": "",
    "text": "If the individuals in my dataset are correlated with known correlation structure, how can I perform regression whilst accounting for that correlation structure?\n\\[\n\\beta = (X^T \\rho^{-1} X)^{-1} X^T\\rho^{-1}Y\n\\]\nThe variance of the estimate will be\n\\[\n\\begin{aligned}\nVar(\\beta) &= \\sigma^2(X^T \\rho^{-1} X) \\\\\n&= \\frac{(\\hat{e}^T \\rho^{-1} \\hat{e})(X^T \\rho^{-1} X)}{n-r}\n\\end{aligned}\n\\]\nAdding to the complication, in this case it is the h2 estimates of a set of traits being correlated against number of GWAS hits for a set of traits, where the traits are in some way correlated. So the estimates of e.g. h2 are estimated with sampling error, so to account for that perhaps need to parametric bootstrap.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(mvtnorm)\n\n#' Regression with samples that are not independent\n#'\n#' For our analysis we're regressing estimates against estimates,\n#' and so it's a bit more complicated because the estimates each have an SE.\n#' For now let's just ignore that but I think if we don't account for it we\n#' will get a bit of regression dilution bias.\n#'\n#' @param x Vector of x values\n#' @param y Vector of y values\n#' @param rho correlation matrix\n#' @param se_x SE of x evalues (Ignore)\n#' @param se_y SE of y values (Ignore)\n#' @param nboot Number of bootstraps to get standard error (Ignore)\n#'\n#' @return\n#' @export\nreg_nonind <- function(x, y, rho, se_x=NULL, se_y=NULL, nboot=NULL)\n{\n  X <- cbind(rep(1, length(x)), x)\n  rho_inv <- solve(rho)\n  #beta <- solve(t(X) %*% rho_inv %*% X) %*% t(X) %*% rho_inv %*% y\n  beta <- ginv(t(X)%*%rho_inv %*%X)%*%t(X)%*%rho_inv %*%y\n  yhat <- X %*% beta\n  yres <- as.numeric(y - yhat)\n  se <- as.numeric((t(yres) %*% rho_inv %*% yres) / (length(x)-qr(X)$rank)) * solve(t(X) %*% rho_inv %*% X)\n  se <- sqrt(diag(se))\n  return(tibble(\n    param=c(\"intercept\", \"slope\"), beta=beta, se=se, pval=pnorm(abs(beta)/se, lower.tail=FALSE)\n  ))\n  \n  # get standard error via parametric bootstrap\n  # betaboot <- matrix(0, nboot, 2)\n  # for(i in 1:nboot)\n  # {\n  #   X[,2] <- rnorm(length(x), mean=x, sd=se_x)\n  #   Y <- rnorm(length(y), mean=y, sd=se_y)\n  #   betaboot[i,] <- solve(t(X) %*% rho_inv %*% X) %*% t(X) %*% rho_inv %*% Y %>% as.numeric()\n  # }\n  # se_boot <- apply(betaboot, 2, sd)\n}\n\nx <- runif(300)\ny <- runif(300)\nrho <- diag(300)\nrho[lower.tri(rho)] <- rnorm(sum(lower.tri(rho)), sd=0.01)\nrho[upper.tri(rho)] <- t(rho)[upper.tri(rho)]\n\nreg_nonind(x, y, rho, 10)\n\n# A tibble: 2 × 4\n  param     beta[,1]     se pval[,1]\n  <chr>        <dbl>  <dbl>    <dbl>\n1 intercept   0.507  0.0332 8.42e-53\n2 slope      -0.0522 0.0579 1.84e- 1\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50453 -0.23397 -0.01499  0.22869  0.51654 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.51135    0.03319  15.407   <2e-16 ***\nx           -0.05653    0.05748  -0.983    0.326    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2824 on 298 degrees of freedom\nMultiple R-squared:  0.003235,  Adjusted R-squared:  -0.0001103 \nF-statistic: 0.967 on 1 and 298 DF,  p-value: 0.3262"
  },
  {
    "objectID": "posts/2023-02-28-vqtl/index.html",
    "href": "posts/2023-02-28-vqtl/index.html",
    "title": "Spurious vQTL simulation",
    "section": "",
    "text": "How sensitive is DRM to incomplete LD in more realistic scenarios? Take a region of 1000 genomes data and choose a SNP at random to have an additive effect of some determined magnitude. We’d now determine how many SNPs in the region (i.e. with incomplete LD with the causal SNP) have evidence of vQTL using DRM or SVLM. How does genetic effect size and sample size relate to minimum p-value and number of positions with FDR < 0.05.\n\nlibrary(glue)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\ntest_drm <- function(g, y)\n{\n  y.i <- tapply(y, g, median, na.rm=T)\n  z.ij <- abs(y - y.i[g+1])\n  summary(lm(z.ij ~ g))$coef %>%\n    as_tibble() %>%\n    slice(2) %>%\n    mutate(method=\"drm\")\n}\n\ntest_svlm <- function(g, y)\n{\n  yres <- residuals(lm(y ~ g))^2\n  summary(lm(yres ~ g))$coefficients %>%\n    as_tibble() %>%\n    slice(n=2) %>%\n    mutate(method=\"svlm\")\n}\n\nGet some plink data - e.g. just 1-2k SNPs from 1000 genomes European samples\n\n/Users/gh13047/Downloads/plink_mac_20230116/plink \\\n    --bfile /Users/gh13047/repo/opengwas-api-internal/opengwas-api/app/ld_files/EUR \\\n    --chr 22 \\\n    --recode A \\\n    --out eur22 \\\n    --from-bp 0 --to-bp 17000000\n\nPLINK v1.90b7 64-bit (16 Jan 2023)             www.cog-genomics.org/plink/1.9/\n(C) 2005-2023 Shaun Purcell, Christopher Chang   GNU General Public License v3\nLogging to eur22.log.\nOptions in effect:\n  --bfile /Users/gh13047/repo/opengwas-api-internal/opengwas-api/app/ld_files/EUR\n  --chr 22\n  --from-bp 0\n  --out eur22\n  --recode A\n  --to-bp 17000000\n\n16384 MB RAM detected; reserving 8192 MB for main workspace.\n1734 out of 8550156 variants loaded from .bim file.\n503 people (0 males, 0 females, 503 ambiguous) loaded from .fam.\nAmbiguous sex IDs written to eur22.nosex .\nUsing 1 thread (no multithreaded calculations invoked).\nBefore main variant filters, 503 founders and 0 nonfounders present.\nCalculating allele frequencies... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\b\b done.\nTotal genotyping rate is in [0.9999995, 1).\n1734 variants and 503 people pass filters and QC.\nNote: No phenotypes present.\n--recode A to eur22.raw ... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\bdone.\n\n\nRead in genotype data\n\ngeno <- fread(\"eur22.raw\")\nfam <- geno[,1:6]\ngeno <- as.matrix(geno[,-c(1:6)])\ngeno[1:10,1:10]\n\n      rs587616822_G rs62224609_C rs4965031_A rs587646183_C rs139918843_C\n [1,]             0            0           1             0             0\n [2,]             0            1           0             0             0\n [3,]             0            1           0             0             0\n [4,]             0            0           0             0             0\n [5,]             0            0           0             0             0\n [6,]             0            1           0             0             0\n [7,]             0            0           1             0             0\n [8,]             0            0           0             0             0\n [9,]             0            1           0             0             0\n[10,]             0            0           1             0             0\n      rs376238049_T rs200777521_A rs587701155_A rs80167676_T rs915675_A\n [1,]             0             0             0            0          1\n [2,]             1             0             0            0          1\n [3,]             1             0             0            0          1\n [4,]             0             0             0            0          0\n [5,]             0             0             0            0          0\n [6,]             1             0             0            0          1\n [7,]             0             0             1            0          1\n [8,]             0             0             0            0          1\n [9,]             0             1             0            1          0\n[10,]             0             0             0            0          0\n\ndim(geno)\n\n[1]  503 1734\n\ncormat <- cor(geno)\ndim(cormat)\n\n[1] 1734 1734\n\n\n\nfast_assoc <- function(y, x)\n{\n    index <- is.finite(y) & is.finite(x)\n    n <- sum(index)\n    y <- y[index]\n    x <- x[index]\n    vx <- var(x)\n    bhat <- stats::cov(y, x)/vx\n    ahat <- mean(y) - bhat * mean(x)\n    rsq <- (bhat * vx)^2/(vx * var(y))\n    fval <- rsq * (n - 2)/(1 - rsq)\n    tval <- sqrt(fval)\n    se <- abs(bhat/tval)\n    p <- stats::pf(fval, 1, n - 2, lower.tail = FALSE)\n    return(list(ahat = ahat, bhat = bhat, se = se, fval = fval, pval = p, n = n))\n}\n\ntest_drm <- function(g, y)\n{\n  y.i <- tapply(y, g, median, na.rm=T)\n  z.ij <- abs(y - y.i[g+1])\n  fast_assoc(z.ij, g) %>% as_tibble()\n}\n\nsim_full <- function(rsq, geno, n, cormat)\n{\n  i <- sample(1:ncol(geno), 1)\n  y <- as.numeric(scale(geno[,i])) * sqrt(rsq) + rnorm(nrow(geno), 0, sqrt(1-rsq))\n  res <- lapply(1:ncol(geno), function(i)\n  {\n      test_drm(geno[1:n,i], y[1:n])\n  }) %>% bind_rows() %>% mutate(snp=1:n(), ldrsq=cormat[i,]^2)\n  return(res)\n  res %>% \n    mutate(fdr = p.adjust(pval, \"fdr\")) %>%\n    summarise(\n        vqtl = which.min(pval),\n        minp = min(pval, na.rm=T),\n        nfdr = sum(fdr < 0.05, na.rm=T),\n        vqtl_ldrsq = ldrsq[vqtl]\n        ) %>%\n    mutate(rsq = rsq, n = n, qtl=i, af=sum(geno[,i])/(2*nrow(geno))) %>%\n    ungroup()\n}\nset.seed(1234)\no <- sim_full(0.5, geno, 500, cormat)\no %>% ggplot(., aes(x=ldrsq, y = -log10(pval))) +\ngeom_point() +\ngeom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'"
  },
  {
    "objectID": "posts/2023-02-28-vqtl/index.html#determining-spurious-effects",
    "href": "posts/2023-02-28-vqtl/index.html#determining-spurious-effects",
    "title": "Spurious vQTL simulation",
    "section": "Determining spurious effects",
    "text": "Determining spurious effects\nTest DRM on each SNP in region for varying sample sizes and additive variance explained in the region\nSimulation here: https://github.com/explodecomputer/mz-gwas/blob/main/scripts/drm_sims.r\n\nload(url(\"https://github.com/explodecomputer/mz-gwas/raw/main/scripts/drm_sims.rdata\"))\n\nggplot(res %>% filter(minp < 5e-8), aes(x=as.factor(rsq), y=nfdr)) +\ngeom_boxplot(aes(fill=as.factor(n)))\n\n\n\n\n\nggplot(res %>% filter(minp < 5e-8), aes(x=as.factor(rsq), y=-log10(minp))) +\ngeom_boxplot(aes(fill=as.factor(n)))\n\n\n\n\n\nggplot(res %>% filter(minp < 5e-10), aes(x=af, y=vqtl_ldrsq)) +\ngeom_point()"
  },
  {
    "objectID": "posts/2023-02-28-vqtl/index.html#summary",
    "href": "posts/2023-02-28-vqtl/index.html#summary",
    "title": "Spurious vQTL simulation",
    "section": "Summary",
    "text": "Summary\n\nSVLM and DGLM suffer equally from the issue\nThe problem is likely a function of F statistic and LD - i.e. as sample sizes get larger, smaller effects additive become more relevant\nWith n=500 additive effect of rsq = 0.1 is enough to generate a spurious significant vQTL\n\n\nunlink(\"eur22*\")\n\n\n\nsessionInfo()\n\nR version 4.2.3 Patched (2023-03-15 r84020)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.0     dplyr_1.0.10      data.table_1.14.2 glue_1.6.2       \n\nloaded via a namespace (and not attached):\n [1] pillar_1.8.1      compiler_4.2.3    tools_4.2.3       digest_0.6.31    \n [5] lattice_0.20-45   nlme_3.1-162      jsonlite_1.8.4    evaluate_0.19    \n [9] lifecycle_1.0.3   tibble_3.1.8      gtable_0.3.1      mgcv_1.8-42      \n[13] pkgconfig_2.0.3   rlang_1.0.6       Matrix_1.5-3      cli_3.5.0        \n[17] DBI_1.1.3         yaml_2.3.6        xfun_0.36         fastmap_1.1.0    \n[21] withr_2.5.0       stringr_1.5.0     knitr_1.41        generics_0.1.3   \n[25] vctrs_0.5.1       htmlwidgets_1.5.4 grid_4.2.3        tidyselect_1.2.0 \n[29] R6_2.5.1          fansi_1.0.3       rmarkdown_2.16    farver_2.1.1     \n[33] magrittr_2.0.3    scales_1.2.1      htmltools_0.5.4   splines_4.2.3    \n[37] assertthat_0.2.1  colorspace_2.0-3  labeling_0.4.2    utf8_1.2.2       \n[41] stringi_1.7.8     munsell_0.5.0"
  },
  {
    "objectID": "posts/2023-07-24-summary-stat-storage/index.html",
    "href": "posts/2023-07-24-summary-stat-storage/index.html",
    "title": "Summary stat storage",
    "section": "",
    "text": "Summary statistics typically have a lot of redundant information due to LD. Can we improve storage space by converting to a sparse format using an external LD reference panel?\n\n\nTwo types of compression - lossless and lossy. Lossless preserves the inforation perfectly. Lossy allows some loss of information to achieve computational / storage improvement.\n\nLossless compression:\n\n1mb regions with suggestive association e.g. p-value < 1e-5\ncis regions of molecular traits\n\nLossy compression\n\nall other regions\n\n\n\n\n\n\nUse external LD reference panel, chunk up genome into areas of distinct LD, decompose summary statistics into eigenvectors and retain only the first X eigenvectors that explain sufficient variation\nRetain only the sign of the effect estimate\nRemove entirely and assume beta = 0\n\n\n\n\n\ndata storage saving\ntime cost\nloss of precision\nbias?\nsensitivity to ld reference panel\n\nImpact on different types of analyses e.g.\n\ncolocalisation\nmr\nld score regression\nbias of effects"
  },
  {
    "objectID": "posts/2023-07-24-summary-stat-storage/index.html#example-of-lossy-ld-compression-of-betas",
    "href": "posts/2023-07-24-summary-stat-storage/index.html#example-of-lossy-ld-compression-of-betas",
    "title": "Summary stat storage",
    "section": "Example of lossy LD compression of betas",
    "text": "Example of lossy LD compression of betas\n\nDownload some LD reference data e.g. from here: http://fileserve.mrcieu.ac.uk/ld/1kg.v3.tgz\nDownload some GWAS summary statistics: https://gwas.mrcieu.ac.uk/files/ukb-b-19953/ukb-b-19953.vcf.gz\nHave bcftools on path\nHave plink on path\n\n# Download example summary statistics (UKBB GWAS of BMI)\nwget https://gwas.mrcieu.ac.uk/files/ukb-b-19953/ukb-b-19953.vcf.gz\nwget https://gwas.mrcieu.ac.uk/files/ukb-b-19953/ukb-b-19953.vcf.gz.tbi\n\n# Convert vcf to txt file, just keep chr 22\nbcftools query \\\n-r 22 \\\n-e 'ID == \".\"' \\\n-f '%ID\\t[%LP]\\t%CHROM\\t%POS\\t%ALT\\t%REF\\t%AF\\t[%ES\\t%SE]\\n' \\\nukb-b-19953.vcf.gz | \\\nawk 'BEGIN {print \"variant_id\\tp_value\\tchromosome\\tbase_pair_location\\teffect_allele\\tother_allele\\teffect_allele_frequency\\tbeta\\tstandard_error\"}; {OFS=\"\\t\"; if ($2==0) $2=1; else if ($2==999) $2=0; else $2=10^-$2; print}' > gwas.tsv\n\n# Download and extract the LD reference panel - 1000 genomes\nwget http://fileserve.mrcieu.ac.uk/ld/1kg.v3.tgz\ntar xvf 1kg.v3.tgz\n\n# Get allele frequencies\nplink --bfile EUR --freq --out EUR --chr 22\n/Users/gh13047/Downloads/plink_mac_20230116/plink --bfile /Users/gh13047/repo/opengwas-api-internal/opengwas-api/app/ld_files/EUR --freq --out EUR --chr 22\nRead in GWAS sum stats\n\nlibrary(ieugwasr)\n\nAPI: public: http://gwas-api.mrcieu.ac.uk/\n\nlibrary(data.table)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(glue)\ngwas <- fread(\"gwas.tsv\")\n\nJust keep 1 Mb and get LD matrix\n\ngwas <- subset(gwas, base_pair_location < (min(base_pair_location)+1000000))\nld <- ld_matrix(gwas$variant_id, bfile=\"/Users/gh13047/repo/opengwas-api-internal/opengwas-api/app/ld_files/EUR\", plink_bin=\"/Users/gh13047/Downloads/plink_mac_20230116/plink\")\ndim(ld)\n\n[1] 275 275\n\n\nHarmonise gwas and ld\n\nstandardise <- function(d, ea=\"ea\", oa=\"oa\", beta=\"beta\", chr=\"chr\", pos=\"pos\") {\n    toflip <- d[[ea]] > d[[oa]]\n    d[[beta]][toflip] <- d[[beta]][toflip] * -1\n    temp <- d[[oa]][toflip]\n    d[[oa]][toflip] <- d[[ea]][toflip]\n    d[[ea]][toflip] <- temp\n    d[[\"snpid\"]] <- paste0(d[[chr]], \":\", d[[pos]], \"_\", toupper(d[[ea]]), \"_\", toupper(d[[oa]]))\n    d\n}\n\ngreedy_remove <- function(r, maxr=0.99) {\n    diag(r) <- 0\n    flag <- 1\n    rem <- c()\n    nom <- colnames(r)\n    while(flag == 1)\n    {\n        message(\"iteration\")\n        count <- apply(r, 2, function(x) sum(x >= maxr))\n        if(any(count > 0))\n        {\n            worst <- which.max(count)[1]\n            rem <- c(rem, names(worst))\n            r <- r[-worst,-worst]\n        } else {\n            flag <- 0\n        }\n    }\n    return(which(nom %in% rem))\n}\n\nmap <- gwas %>% dplyr::select(rsid=variant_id, chr=chromosome, pos=base_pair_location) %>% filter(!duplicated(rsid))\nldmap <- tibble(vid=rownames(ld), beta=1) %>%\n    tidyr::separate(vid, sep=\"_\", into=c(\"rsid\", \"ea\", \"oa\"), remove=FALSE) %>%\n    left_join(., map, by=\"rsid\") %>%\n    standardise()\ngwas <- subset(gwas, variant_id %in% ldmap$rsid) %>%\n    standardise(ea=\"effect_allele\", oa=\"other_allele\", chr=\"chromosome\", pos=\"base_pair_location\")\ngwas <- subset(gwas, snpid %in% ldmap$snpid)\nldmap <- subset(ldmap, snpid %in% gwas$snpid)\nstopifnot(all(gwas$snpid == ldmap$snpid))\nstopifnot(all(ldmap$vid == rownames(ld)))\n\n# Flip LD based on harmonisation with gwas\nm <- ldmap$beta %*% t(ldmap$beta)\nldh <- ld * m\n\nGet allele frequency, need the standard deviation of each SNP (xvar)\n\nfrq <- fread(\"EUR.frq\") %>%\n    inner_join(., map, by=c(\"SNP\"=\"rsid\")) %>%\n    mutate(beta=1) %>%\n    standardise(., ea=\"A1\", oa=\"A2\")\nstopifnot(all(frq$snpid == gwas$snpid))\nxvar <- sqrt(2 * frq$MAF * (1-frq$MAF))\n\n\nCompression\nInverting LD matrices is sometimes difficult because of singularity issues. If at least one of the variants is a linear combination of the other variants in the region then the matrix will be singular. If sample sizes are very large then this is less likely to happen but matrices will still be near singular with occasional hugely inflated values.\nTo avoid these issues we could just estimate PCs on the LD matrix, and then perform a lossy compression by selecting the PCs that explain the top x% variation, and then only storing the \\(\\beta \\Lambda\\) matrix.\n\n# Get principal components of the LD matrix\nldpc <- princomp(ldh)\n# This is the sd of each PC\nplot(ldpc$sdev)\n\n\n\n\nMost variation explained by rather few SNPs - how many needed to explain 80% of LD info?\n\nldpc <- princomp(ldh)\ni <- which(cumsum(ldpc$sdev) / sum(ldpc$sdev) >= 0.8)[1]\ni\n\nComp.13 \n     13 \n\n\nSo 5% of the original data size required to capture 80% of the variation?\n\n# Compress using only 80% of PC variation\ncomp <- (gwas$beta) %*% ldpc$loadings[,1:i]\n# Uncompress back to betas\nuncomp <- comp %*% t(ldpc$loadings[,1:i])\n\nPlot compressed betas against original betas\n\nplot(uncomp, gwas$beta)\n\n\n\n\nHow much info is lost?\n\ncor(drop(uncomp), gwas$beta)\n\n[1] 0.7708481\n\n\nIs there bias? e.g. is coefficient different from 1?\n\nsummary(lm(gwas$beta ~ drop(uncomp)))\n\n\nCall:\nlm(formula = gwas$beta ~ drop(uncomp))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0130259 -0.0009433  0.0001189  0.0010631  0.0110847 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.928e-05  1.465e-04  -0.473    0.637    \ndrop(uncomp)  9.973e-01  4.988e-02  19.994   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.002414 on 273 degrees of freedom\nMultiple R-squared:  0.5942,    Adjusted R-squared:  0.5927 \nF-statistic: 399.8 on 1 and 273 DF,  p-value: < 2.2e-16\n\n\nDoesn’t seem too bad. How consistent is the sign in the compressed vs original?\n\ntable(sign(gwas$beta) == sign(uncomp))\n\n\nFALSE  TRUE \n   40   235 \n\n\nOverall 5% of the data seems to capture a reasonable amount of information.\n\n\nStandard errors\nThe above does it for betas, but standard errors are a bit more complicated. Could just use the approx marginal \\(se = 1/2pq\\), but this will ignore correlation structure"
  },
  {
    "objectID": "posts/2023-07-24-summary-stat-storage/index.html#unfinished-ignore",
    "href": "posts/2023-07-24-summary-stat-storage/index.html#unfinished-ignore",
    "title": "Summary stat storage",
    "section": "Unfinished (ignore)",
    "text": "Unfinished (ignore)\nSE for each SNP will be\n\\[\ns \\rho s\n\\]\nwhere s is diagonal matrix of standard errors. Represent \\(\\rho = QA^{-1}Q^T\\) where A is diagonal matrix of eigenvalues and Q is matrix of eigenvectors / loadings\n\ntemp <- as.matrix(ldpc$loadings) %*% diag(1/ldpc$sdev) %*% t(as.matrix(ldpc$loadings))\ntemp[1:10,1:10]\ncor(temp, ldh)\n\n\nx <- prcomp(ldh)\nnames(x)\nstr(x)\nstr(ldpc)\n\ntemp <- x$vectors %*% diag(1/x$values) %*% t(x$vectors)\nfor(i in 1:nrow(temp)) { temp[,i] <- temp[,i] / temp[i,i]}\ntemp[1:10,1:10]\nplot(c(ldh), c(temp))\n\ngwas stats = gwas ld matrix = ldh\nhttps://explodecomputer.github.io/simulateGP/articles/gwas_summary_data_ld.html\nTry to solve ld\n\ntry(solve(ldh))\n\nThis doesn’t work - the matrix is singular. How to avoid? e.g. remove SNPs in high LD\n\ng <- greedy_remove(ldh, 0.99)\ngwas <- gwas[-g,]\nldh <- ldh[-g, -g]\nldmap <- ldmap[-g,]\nxvar <- xvar[-g]\nstopifnot(all(gwas$snpid == ldmap$snpid))\ntry(solve(ldh))\n\nOk this is a problem. How to select SNPs to include that will involve a non-singular matrix?\n\nMake sparse\n\n\nconv <- function(b, se, ld, xvar) {\n    # make sparse\n    bs <- (b %*% diag(xvar) %*% solve(ld) %*% diag(1/xvar)) %>% drop()\n    # make dense again\n    bhat <- (diag(1/xvar) %*% ld %*% diag(xvar) %*% bs) %>% drop()\n    # create sparse version of bs\n    tibble(b, bs, bhat)\n}\n\n#o <- conv(gwas$beta, gwas$standard_error, ldh, xvar)\n\n\nMake dense again\nCompare sparse and dense\n\nSimulations\n\nlibrary(mvtnorm)\nlibrary(simulateGP)\n\n# Provide matrix of SNPs, phenotype y, true effects of SNPs on y\ncalcs <- function(x, y, b) {\n    xpx <- t(x) %*% x\n    D <- matrix(0, ncol(x), ncol(x))\n    diag(D) <- diag(xpx)\n    # Estimate effects (these will have LD influence)\n    betahat <- gwas(y, x)$bhat\n    # Convert back to marginal effects - this is approx, doesn't use AF\n    bhat <- drop(solve(xpx) %*% D %*% betahat)\n    # Determine betas with LD\n    betahatc <- b %*% xpx %*% solve(D) %>% drop\n    rho <- cor(x)\n    xvar <- apply(x, 2, sd)\n    # Another way to determine betas with LD using just sum stats\n    betahatrho <- (diag(1/xvar) %*% rho %*% diag(xvar) %*% b) %>% drop\n    # Go back to true betas\n    betaback <- (betahatrho %*% diag(xvar) %*% solve(rho) %*% diag(1/xvar)) %>% drop()\n    tibble(b, bhat, betahat, betahatc, betahatrho, betaback)\n}\n\nn <- 10000\nnsnp <- 20\nsigma <- matrix(0.7, nsnp, nsnp)\ndiag(sigma) <- 1\nx <- rmvnorm(n, rep(0, nsnp), sigma)\n\nb <- rnorm(nsnp) * 100\ny <- x %*% b + rnorm(n)\nres <- calcs(x, y, b)\nres\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.8\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] glue_1.6.2        tidyr_1.3.0       dplyr_1.1.2       data.table_1.14.8\n[5] ieugwasr_0.1.5   \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       cli_3.6.1         knitr_1.43        rlang_1.1.1      \n [5] xfun_0.39         purrr_1.0.1       generics_0.1.3    jsonlite_1.8.5   \n [9] htmltools_0.5.5   fansi_1.0.4       rmarkdown_2.22    evaluate_0.21    \n[13] tibble_3.2.1      fastmap_1.1.1     yaml_2.3.7        lifecycle_1.0.3  \n[17] compiler_4.3.0    htmlwidgets_1.6.2 pkgconfig_2.0.3   rstudioapi_0.14  \n[21] digest_0.6.31     R6_2.5.1          tidyselect_1.2.0  utf8_1.2.3       \n[25] pillar_1.9.0      magrittr_2.0.3    withr_2.5.0       tools_4.3.0"
  },
  {
    "objectID": "posts/2023-08-25-gwas-catalog/index.html",
    "href": "posts/2023-08-25-gwas-catalog/index.html",
    "title": "GWAS catalog traits",
    "section": "",
    "text": "Identify GWAS catalog traits to import to OpenGWAS\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nlibrary(ieugwasr)\n\nAPI: public: http://gwas-api.mrcieu.ac.uk/\n\nlibrary(ggplot2)\n\nGet list of harmonised studies in GWAS catalog\n\ndownload.file(\"ftp://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/harmonised_list.txt\", \"harmonised_list.txt\")\na <- scan(\"harmonised_list.txt\", what=character())\nhead(a)\n\n[1] \"./GCST90086001-GCST90087000/GCST90086737/harmonised/35078996-GCST90086737-EFO_0007937.h.tsv.gz\"\n[2] \"./GCST90086001-GCST90087000/GCST90086591/harmonised/35078996-GCST90086591-EFO_0007937.h.tsv.gz\"\n[3] \"./GCST90086001-GCST90087000/GCST90086659/harmonised/35078996-GCST90086659-EFO_0007937.h.tsv.gz\"\n[4] \"./GCST90086001-GCST90087000/GCST90086510/harmonised/35078996-GCST90086510-EFO_0007937.h.tsv.gz\"\n[5] \"./GCST90086001-GCST90087000/GCST90086758/harmonised/35078996-GCST90086758-EFO_0007937.h.tsv.gz\"\n[6] \"./GCST90086001-GCST90087000/GCST90086480/harmonised/35078996-GCST90086480-EFO_0007937.h.tsv.gz\"\n\n\nGet EBI IDs already in OpenGWAS\n\nao <- ieugwasr::gwasinfo()\ngotebi <- grep(\"ebi-a\", ao$id, value=TRUE) %>% gsub(\"ebi-a-\", \"\", .)\n\nIdentify datasets not available\n\na <- tibble(path=a) %>%\n    tidyr::separate(path, sep=\"/\", into=c(\"dot\", \"range\", \"id\", \"harmonised\", \"fn\"), remove=FALSE) %>% \n    as_tibble %>%\n    tidyr::separate(fn, sep=\"-\", into=c(\"pmid\", \"id2\", \"efo\"), remove=FALSE) %>%\n    filter(!is.na(id2)) %>%\n    filter(!id %in% gotebi) %>%\n    dplyr::select(-c(id2, dot)) %>%\n    mutate(path=gsub(\"^./\", \"\", path))\n\nWarning: Expected 3 pieces. Missing pieces filled with `NA` in 4427 rows [4005, 4006,\n4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019,\n4020, 4021, 4022, 4023, 4024, ...].\n\na\n\n# A tibble: 22,932 × 7\n   path                                 range id    harmonised fn    pmid  efo  \n   <chr>                                <chr> <chr> <chr>      <chr> <chr> <chr>\n 1 GCST90086001-GCST90087000/GCST90086… GCST… GCST… harmonised 3507… 3507… EFO_…\n 2 GCST90086001-GCST90087000/GCST90086… GCST… GCST… harmonised 3507… 3507… EFO_…\n 3 GCST90086001-GCST90087000/GCST90086… GCST… GCST… harmonised 3507… 3507… EFO_…\n 4 GCST90086001-GCST90087000/GCST90086… GCST… GCST… harmonised 3507… 3507… EFO_…\n 5 GCST90086001-GCST90087000/GCST90086… GCST… GCST… harmonised 3507… 3507… EFO_…\n 6 GCST90086001-GCST90087000/GCST90086… GCST… GCST… harmonised 3507… 3507… EFO_…\n 7 GCST90086001-GCST90087000/GCST90086… GCST… GCST… harmonised 3507… 3507… EFO_…\n 8 GCST90086001-GCST90087000/GCST90086… GCST… GCST… harmonised 3507… 3507… EFO_…\n 9 GCST90086001-GCST90087000/GCST90086… GCST… GCST… harmonised 3507… 3507… EFO_…\n10 GCST90086001-GCST90087000/GCST90086… GCST… GCST… harmonised 3507… 3507… EFO_…\n# ℹ 22,922 more rows\n\n\nThere are 4427 that have no pmid, remove for now.\nSummarise how many studies by pmid:\n\nas <- a %>% group_by(pmid) %>% summarise(n=n()) %>% arrange(desc(n))\nas %>% head(n=20)\n\n# A tibble: 20 × 2\n   pmid         n\n   <chr>    <int>\n 1 34662886  7972\n 2 35078996  4753\n 3 34737426  2939\n 4 35668104  1863\n 5 33983923  1021\n 6 34503513   922\n 7 35115689   469\n 8 34594039   429\n 9 35115690   412\n10 33437055   337\n11 35213538   249\n12 34017140   161\n13 33414548   134\n14 33959723   115\n15 35264221   110\n16 33328453   106\n17 34226706    53\n18 29892013    46\n19 33623009    45\n20 33283231    43\n\n\nHow many traits per study?\n\nqplot(y=as$n, x=1:nrow(as)) +\nscale_y_log10() +\nlabs(y=\"Number of traits in study\", x=\"Study\")\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nHow many traits to import if we only keep studies with fewer than 1000 traits\n\nsubset(as, n <= 1000) %>% {sum(.$n)}\n\n[1] 4384\n\n\nHow many traits to import if we only keep studies with fewer than 200 traits\n\nsubset(as, n <= 200) %>% {sum(.$n)}\n\n[1] 1566\n\n\nStudies to ignore (for now at least)\n\npmid_ignore <- c(\n    34662886, # rare variant aggregate enrichment of ukb exome data\n    35078996, # pqtl\n    34737426, # ukb analysis (duplication of existing work)\n    35668104, # lipids superseded by others\n    33983923  # facial variation - 1k traits\n)\n\n\na_keep <- subset(a, !pmid %in% pmid_ignore)\nwrite.table(a_keep$path, file=\"gwascat_keeplist.txt\", row=F, col=F, qu=F)\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.8\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.2     ieugwasr_0.1.5    data.table_1.14.8 tidyr_1.3.0      \n[5] dplyr_1.1.2      \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.3      jsonlite_1.8.7    compiler_4.3.0    tidyselect_1.2.0 \n [5] scales_1.2.1      yaml_2.3.7        fastmap_1.1.1     R6_2.5.1         \n [9] labeling_0.4.2    generics_0.1.3    curl_5.0.2        knitr_1.43       \n[13] htmlwidgets_1.6.2 tibble_3.2.1      munsell_0.5.0     pillar_1.9.0     \n[17] rlang_1.1.1       utf8_1.2.3        xfun_0.39         cli_3.6.1        \n[21] withr_2.5.0       magrittr_2.0.3    digest_0.6.31     grid_4.3.0       \n[25] rstudioapi_0.14   lifecycle_1.0.3   vctrs_0.6.3       evaluate_0.21    \n[29] glue_1.6.2        farver_2.1.1      fansi_1.0.4       colorspace_2.1-0 \n[33] rmarkdown_2.22    purrr_1.0.2       httr_1.4.7        tools_4.3.0      \n[37] pkgconfig_2.0.3   htmltools_0.5.5"
  },
  {
    "objectID": "posts/2023-07-13-mr-clumping/index.html",
    "href": "posts/2023-07-13-mr-clumping/index.html",
    "title": "MR and imperfect clumping",
    "section": "",
    "text": "4000 instruments for educational attainment using clumping r2 = 0.1, and se doubles when using r2 = 0.001.\nThat smaller standard error is either due to the R2 in the exposure being higher or the non-independence of effects artificially increasing precision, or a mixture of both.\nSo the question is the impact of the latter – if we have some true correlation structure with realistic F stats at a specific locus, and then we try to clump at r2 = 0.001 vs 0.1, how many instruments do we retain (it should be 1) and if more than 1, what is that impact on the standard error\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(simulateGP)\nlibrary(TwoSampleMR)\n\nTwoSampleMR version 0.5.7 \n[>] New: Option to use non-European LD reference panels for clumping etc\n[>] Some studies temporarily quarantined to verify effect allele\n[>] See news(package='TwoSampleMR') and https://gwas.mrcieu.ac.uk for further details\n\n\n\nAttaching package: 'TwoSampleMR'\n\n\nThe following objects are masked from 'package:simulateGP':\n\n    allele_frequency, contingency, get_population_allele_frequency\n\nlibrary(purrr)\nlibrary(ggplot2)\n\nSimulate causal snp g + another that is correlated with it\n\nn <- 100000\ng <- correlated_binomial(n, p1=0.5, p2=0.5, rho=sqrt(0.1))\n# x caused by just snp 1\nx <- g[,1] * 0.5 + rnorm(n)\ny <- x * 0.5 + rnorm(n)\n\n# MR using both SNPs, treating as if they are independent\nget_effs(x, y, g) %>% mr(method=\"mr_ivw\") %>% str\n\nAnalysing 'X' on 'Y'\n\n\n'data.frame':   1 obs. of  9 variables:\n $ id.exposure: chr \"X\"\n $ id.outcome : chr \"Y\"\n $ outcome    : chr \"Y\"\n $ exposure   : chr \"X\"\n $ method     : chr \"Inverse variance weighted\"\n $ nsnp       : int 2\n $ b          : num 0.497\n $ se         : num 0.00956\n $ pval       : num 0\n\n\n\n# MR using just the causal SNP\nget_effs(x, y, g[,1, drop=F]) %>% mr(method=c(\"mr_ivw\", \"mr_wald_ratio\")) %>% str\n\nAnalysing 'X' on 'Y'\n\n\n'data.frame':   1 obs. of  9 variables:\n $ id.exposure: chr \"X\"\n $ id.outcome : chr \"Y\"\n $ outcome    : chr \"Y\"\n $ exposure   : chr \"X\"\n $ method     : chr \"Wald ratio\"\n $ nsnp       : num 1\n $ b          : num 0.496\n $ se         : num 0.0101\n $ pval       : num 0\n\n\nThere’s hardly any difference in the SE here. Try over a range of scenarios\n\nparam <- expand.grid(\n    r2=seq(0, 1, by=0.02),\n    bgx=seq(0,1, by=0.2),\n    bxy=seq(0,1, by=0.2),\n    n=100000\n)\nparam$sim <- 1:nrow(param)\ndim(param)\n\n[1] 1836    5\n\nres <- map(1:nrow(param), \\(i){\n    g <- correlated_binomial(param$n[i], p1=0.5, p2=0.5, rho=sqrt(param$r2[i]))\n    # x caused by just snp 1\n    x <- g[,1] * param$bgx[i] + rnorm(n)\n    y <- x * param$bxy[i] + rnorm(n)\n\n    bind_rows(\n        get_effs(x, y, g) %>% {suppressMessages(mr(., method=\"mr_ivw\"))},\n        get_effs(x, y, g[,1, drop=F]) %>% {suppressMessages(mr(., method=\"mr_wald_ratio\"))}\n    ) %>% mutate(sim=param$sim[i]) %>% return()\n}) %>% bind_rows %>% inner_join(param, ., by=\"sim\")\n\nWarning in summary.lm(stats::lm(b_out ~ -1 + b_exp, weights = 1/se_out^2)):\nessentially perfect fit: summary may be unreliable\n\n\nStandard errors across all scenarios\n\nggplot(res, aes(x=r2, y=se)) +\ngeom_point(aes(colour=as.factor(nsnp))) +\ngeom_line(aes(colour=as.factor(nsnp))) +\nfacet_grid(bgx ~ bxy, labeller=label_both, scale=\"free_y\")\n\n\n\n\nBias across all scenarios:\n\nggplot(res, aes(x=r2, y=b)) +\ngeom_point(aes(colour=as.factor(nsnp))) +\ngeom_line(aes(colour=as.factor(nsnp))) +\nfacet_grid(bgx ~ bxy, labeller=label_both)\n\n\n\n\nLook at just one\n\nggplot(res %>% filter(bgx == 0.2, bxy == 0.2), aes(x=r2, y=se)) +\ngeom_point(aes(colour=as.factor(nsnp))) +\ngeom_line(aes(colour=as.factor(nsnp))) +\nfacet_grid(bgx ~ bxy, labeller=label_both, scale=\"free_y\")"
  },
  {
    "objectID": "posts/2023-07-13-mr-clumping/index.html#summary",
    "href": "posts/2023-07-13-mr-clumping/index.html#summary",
    "title": "MR and imperfect clumping",
    "section": "Summary",
    "text": "Summary\n\nRelaxed r2 e.g. from 0 to 0.1 doesn’t seem to have a huge impact on standard errors\nIn the one SNP situation relaxed r2 has no impact on bias, and could only plausibly change things under substantial heterogeneity which correlates with overrepresentation.\nMore realistic simulations would look at whether this changes when the p-value at the second locus is very large, and would also look at the probability of erroneously keeping multiple loci for a single causal variant\nSome instability in SEs when correlated SNPs used\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.4.2     purrr_1.0.1       TwoSampleMR_0.5.7 simulateGP_0.1.2 \n[5] dplyr_1.1.2      \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       cli_3.6.1         knitr_1.43        rlang_1.1.1      \n [5] xfun_0.39         generics_0.1.3    jsonlite_1.8.5    labeling_0.4.2   \n [9] glue_1.6.2        colorspace_2.1-0  plyr_1.8.8        htmltools_0.5.5  \n[13] scales_1.2.1      fansi_1.0.4       rmarkdown_2.22    grid_4.3.0       \n[17] munsell_0.5.0     evaluate_0.21     tibble_3.2.1      fastmap_1.1.1    \n[21] yaml_2.3.7        lifecycle_1.0.3   compiler_4.3.0    Rcpp_1.0.10      \n[25] htmlwidgets_1.6.2 pkgconfig_2.0.3   rstudioapi_0.14   farver_2.1.1     \n[29] digest_0.6.31     R6_2.5.1          tidyselect_1.2.0  utf8_1.2.3       \n[33] pillar_1.9.0      magrittr_2.0.3    withr_2.5.0       gtable_0.3.3     \n[37] tools_4.3.0"
  },
  {
    "objectID": "posts/2023-06-06-pca-projection/index.html",
    "href": "posts/2023-06-06-pca-projection/index.html",
    "title": "Checking PCA projection",
    "section": "",
    "text": "Need to be able to calculate genetic principal components in unrelateds and then project into relateds.\nSimulate\n\nlibrary(MASS)\nm <- 20\nA <- matrix(runif(m^2)*2-1, ncol=m)\nsigma <- t(A) %*% A\ngen <- mvrnorm(1000, rep(0, m), sigma)\n\nGenerate in full sample\n\npcafull <- princomp(gen)\nnames(pcafull)\n\n[1] \"sdev\"     \"loadings\" \"center\"   \"scale\"    \"n.obs\"    \"scores\"   \"call\"    \n\n\nGenerate in 90%\n\npca90 <- princomp(gen[1:900,])\n\nProject into remaining 10%\n\npca10 <- gen[901:1000,] %*% pca90$loadings\n\ncorrelate full with projected\n\ndiag(cor(pca10, pcafull$scores[901:1000,]))\n\n   Comp.1    Comp.2    Comp.3    Comp.4    Comp.5    Comp.6    Comp.7    Comp.8 \n0.9979686 0.9869467 0.9829454 0.9843170 0.9927274 0.9905525 0.9942163 0.9960611 \n   Comp.9   Comp.10   Comp.11   Comp.12   Comp.13   Comp.14   Comp.15   Comp.16 \n0.9994964 0.9965984 0.9981390 0.9818166 0.9838703 0.9984871 0.9972718 0.9968159 \n  Comp.17   Comp.18   Comp.19   Comp.20 \n0.9982290 0.9993551 0.9991197 0.9989832 \n\n\nThis works fine. Looks like GCTA does something similar here: https://yanglab.westlake.edu.cn/software/gcta/#PCloadingandprojection\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] MASS_7.3-58.4\n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.0    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.3.0       htmltools_0.5.5   rstudioapi_0.14   yaml_2.3.7       \n [9] rmarkdown_2.22    knitr_1.43        xfun_0.39         digest_0.6.31    \n[13] jsonlite_1.8.4    rlang_1.1.1       evaluate_0.21"
  },
  {
    "objectID": "posts/2023-07-20-iv-confounding-variance/index.html",
    "href": "posts/2023-07-20-iv-confounding-variance/index.html",
    "title": "IV confounding with changing variances",
    "section": "",
    "text": "When instruments arise through U the bias is buy/bux, whereas ols bias is buy*bux. If buy is larger than bux then the iv bias will be smaller than ols bias, but otherwise likely to be larger than ols bias.\nWhat happens if you just rescale the values of x and y?\nbias_ols = buy * bux\nWhen the SNP goes through U bias_iv = buy/bux\nbuy = 0.1 bux = 0.1\nbias_iv = 1 bias_ols = 0.01\n\nchange sd of y and x to be from 1 to 10\nbuy = 1 bux = 1\nbias_iv = 1 bias_ols = 1\n\nbuy = 0.1 bux = 1\nbias_iv = 0.1 bias_ols = 0.1\ny = buy * u + e x = bux * u + e\nb_ols = cov(buyu, buxu) = buy * bux * var(u)\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(TwoSampleMR)\n\nTwoSampleMR version 0.5.7 \n[>] New: Option to use non-European LD reference panels for clumping etc\n[>] Some studies temporarily quarantined to verify effect allele\n[>] See news(package='TwoSampleMR') and https://gwas.mrcieu.ac.uk for further details\n\nlibrary(simulateGP)\n\n\nAttaching package: 'simulateGP'\n\n\nThe following objects are masked from 'package:TwoSampleMR':\n\n    allele_frequency, contingency, get_population_allele_frequency\n\niv_bias <- function(g, x, y) {\n    bgx = cov(g, x) / var(g)\n    bgy = cov(g, y) / var(g)\n    return(bgy/bgx)\n}\n\nols_bias <- function(x, y) {\n    return(cov(x,y)/var(x))\n}\n\nn <- 100000\ng <- rbinom(n, 2, 0.4)\nu <- g + rnorm(n, 0, sqrt(1 - var(g)))\n\nbux <- 0.5\nbuy <- 0.5\ny <- u * buy + rnorm(n, sd=sqrt(1-buy^2))\nx <- u * bux + rnorm(n, sd=sqrt(1-bux^2))\n\n\niv_bias(g, x, y)\n\n[1] 0.9936364\n\nols_bias(x,y)\n\n[1] 0.2451663\n\n\n\nparam1 <- expand.grid(\n    bux=seq(-1, 1, by=0.1),\n    buy=seq(-1, 1, by=0.1)\n)\n\nfor(i in 1:nrow(param1)) {\n    y <- u * param1$buy[i] + rnorm(n, sd=sqrt(1-param1$buy[i]^2))\n    x <- u * param1$bux[i] + rnorm(n, sd=sqrt(1-param1$bux[i]^2))\n    param1$ols_bias[i] <- ols_bias(x, y)\n    param1$iv_bias[i] <- iv_bias(g, x, y)\n    param1$ols_bias_e[i] <- param1$buy[i] * param1$bux[i]\n    param1$iv_bias_e[i] <- param1$buy[i] / param1$bux[i]\n}\n\nparam1 %>% filter(bux != 0) %>%\ngather(key=\"key\", value=\"value\", c(ols_bias, iv_bias)) %>% \nggplot(., aes(x=bux, y=value)) +\ngeom_point(aes(colour=key)) +\nfacet_wrap(~ buy)\n\n\n\n\nIs expected bias correct\n\nplot(ols_bias ~ ols_bias_e, param1)\n\n\n\n\n\nplot(iv_bias ~ iv_bias_e, param1 %>% filter(bux!=0))\n\n\n\n\nnow change the variances\n\nparam2 <- expand.grid(\n    bux=seq(-1, 1, by=0.1),\n    buy=0.5,\n    varx=c(1, 0.1, 10),\n    vary=c(1, 0.1, 10)\n)\n\nfor(i in 1:nrow(param2)) {\n    y <- u * param2$buy[i] + rnorm(n, sd=sqrt(1-param2$buy[i]^2))\n    x <- u * param2$bux[i] + rnorm(n, sd=sqrt(1-param2$bux[i]^2))\n    y <- y * sqrt(param2$vary[i])\n    x <- x * sqrt(param2$varx[i])\n    param2$ols_bias[i] <- ols_bias(x, y)\n    param2$iv_bias[i] <- iv_bias(g, x, y)\n    param2$ols_bias_e[i] <- param2$buy[i] * param2$bux[i]\n    param2$iv_bias_e[i] <- param2$buy[i] / param2$bux[i]\n}\n\nparam2 <- param2 %>% filter(bux != 0)\nparam2 <- bind_rows(\n    param2 %>% select(-c(ols_bias_e, iv_bias_e)) %>% gather(key=\"key\", value=\"value\", c(ols_bias, iv_bias)) %>% mutate(what=\"obs\"),\n    param2 %>% select(-c(ols_bias, iv_bias)) %>% gather(key=\"key\", value=\"value\", c(ols_bias_e, iv_bias_e)) %>% mutate(what=\"exp\") \n)\nggplot(param2, aes(x=bux, y=value)) +\ngeom_point(aes(colour=key)) +\nfacet_grid(varx ~ vary, labeller=label_both, scale=\"free_y\")\n\n\n\n\nchange the variance only, see if it changes test statistic\n\nparam3 <- expand.grid(\n    bux=seq(-1, 1, by=0.25),\n    buy=seq(-1, 1, by=0.25),\n    varx=c(0.1, 1, 10),\n    vary=c(0.1, 1, 10)\n) %>% mutate(sim=1:n())\n\nparam3 <- lapply(1:nrow(param3), function(i) {\n    y <- u * param2$buy[i] + rnorm(n, sd=sqrt(1-param2$buy[i]^2))\n    x <- u * param2$bux[i] + rnorm(n, sd=sqrt(1-param2$bux[i]^2))\n    y <- y * sqrt(param2$vary[i])\n    x <- x * sqrt(param2$varx[i])\n    get_effs(x, y, as.matrix(g)) %>% mr %>% suppressMessages %>% mutate(sim=i)\n}) %>% bind_rows() %>% inner_join(., param3, by=\"sim\")\n\nWarning in rnorm(n, sd = sqrt(1 - param2$buy[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$bux[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$buy[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$bux[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$buy[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$bux[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$buy[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$bux[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$buy[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$bux[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$buy[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$bux[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$buy[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$bux[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$buy[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$bux[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$buy[i]^2)): NAs produced\n\n\nWarning in rnorm(n, sd = sqrt(1 - param2$bux[i]^2)): NAs produced\n\nparam3$tval <- param3$b/param3$se\n\nparam3 %>% group_by(bux, buy) %>%\n    summarise(m=mean(tval), s=sd(tval))\n\n`summarise()` has grouped output by 'bux'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 81 × 4\n# Groups:   bux [9]\n     bux   buy      m       s\n   <dbl> <dbl>  <dbl>   <dbl>\n 1 -1    -1    -118.    1.11 \n 2 -1    -0.75   91.3  78.5  \n 3 -1    -0.5   -65.1 104.   \n 4 -1    -0.25   39.0 118.   \n 5 -1     0     -13.5 124.   \n 6 -1     0.25  -12.8 124.   \n 7 -1     0.5    39.1 117.   \n 8 -1     0.75  -65.3 103.   \n 9 -1     1     118.    1.11 \n10 -0.75 -1    -118.    0.826\n# ℹ 71 more rows\n\n\n\nparam3 %>% ggplot(., aes(x=as.factor(bux), y=abs(tval))) +\ngeom_point(aes(colour=as.factor(varx), shape=as.factor(vary))) +\nfacet_wrap(~ buy)\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.8\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] simulateGP_0.1.2  TwoSampleMR_0.5.7 dplyr_1.1.2       tidyr_1.3.0      \n[5] ggplot2_3.4.2    \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       cli_3.6.1         knitr_1.43        rlang_1.1.1      \n [5] xfun_0.39         purrr_1.0.1       generics_0.1.3    jsonlite_1.8.5   \n [9] labeling_0.4.2    glue_1.6.2        colorspace_2.1-0  plyr_1.8.8       \n[13] htmltools_0.5.5   scales_1.2.1      fansi_1.0.4       rmarkdown_2.22   \n[17] grid_4.3.0        evaluate_0.21     munsell_0.5.0     tibble_3.2.1     \n[21] fastmap_1.1.1     yaml_2.3.7        lifecycle_1.0.3   compiler_4.3.0   \n[25] Rcpp_1.0.10       htmlwidgets_1.6.2 pkgconfig_2.0.3   rstudioapi_0.14  \n[29] farver_2.1.1      digest_0.6.31     R6_2.5.1          tidyselect_1.2.0 \n[33] utf8_1.2.3        pillar_1.9.0      magrittr_2.0.3    withr_2.5.0      \n[37] tools_4.3.0       gtable_0.3.3"
  },
  {
    "objectID": "posts/2023-07-05-interactions-and-scale/index.html",
    "href": "posts/2023-07-05-interactions-and-scale/index.html",
    "title": "Interactions and scale",
    "section": "",
    "text": "Absolute and relative risks will change across populations if the baseline risks are different.\nFor example, suppose BMI increases CHD by 20% per SD. BMI is distributed differently in two populations\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nn <- 10000000\n\ndat <- tibble(\n    bmi_1 = rnorm(n, 25, sd=5),\n    bmi_2 = rnorm(n, 20, sd=5),\n    chdr_1 = 0.2,\n    chdr_2 = 0.2\n)\n\n\n\ndat$chdr_1[dat$bmi_1 > 27] <- 0.2*1.5\ndat$chdr_2[dat$bmi_2 > 27] <- 0.2*1.5\n\ndat$chd_1 <- rbinom(n, 1, dat$chdr_1)\ndat$chd_2 <- rbinom(n, 1, dat$chdr_2)\n\ndat\n\n# A tibble: 10,000,000 × 6\n   bmi_1 bmi_2 chdr_1 chdr_2 chd_1 chd_2\n   <dbl> <dbl>  <dbl>  <dbl> <int> <int>\n 1  17.9  22.4    0.2    0.2     0     0\n 2  31.9  20.3    0.3    0.2     0     0\n 3  24.1  23.3    0.2    0.2     0     0\n 4  21.2  21.6    0.2    0.2     0     1\n 5  28.5  13.7    0.3    0.2     1     1\n 6  21.6  30.7    0.2    0.3     1     0\n 7  26.6  19.3    0.2    0.2     0     0\n 8  18.8  25.9    0.2    0.2     0     0\n 9  21.0  26.6    0.2    0.2     0     1\n10  25.1  24.1    0.2    0.2     1     0\n# ℹ 9,999,990 more rows\n\nmean(dat$chdr_1)\n\n[1] 0.2344512\n\nmean(dat$chdr_2)\n\n[1] 0.2080624\n\nmean(dat$chd_1)\n\n[1] 0.2345312\n\nmean(dat$chd_2)\n\n[1] 0.2079281\n\nrr1 <- mean(dat$chd_1[dat$bmi_1 > 27]) / mean(dat$chd_1)\nrr2 <- mean(dat$chd_2[dat$bmi_2 > 27]) / mean(dat$chd_2)\nrr1\n\n[1] 1.279043\n\nrr2\n\n[1] 1.440206\n\n\n\nglm(chd_1 ~ bmi_1, dat, family=\"binomial\")$coef[2]\n\n     bmi_1 \n0.04133235 \n\nglm(chd_2 ~ bmi_2, dat, family=\"binomial\")$coef[2]\n\n     bmi_2 \n0.01829569 \n\nglm(chd_1 ~ bmi_1>27, dat, family=\"binomial\")$coef[2]\n\nbmi_1 > 27TRUE \n     0.5380365 \n\nglm(chd_2 ~ bmi_2>27, dat, family=\"binomial\")$coef[2]\n\nbmi_2 > 27TRUE \n     0.5370367 \n\n\n\nlm(chd_1 ~ bmi_1>27, dat)$coef[2]\n\nbmi_1 > 27TRUE \n    0.09984052 \n\nlm(chd_2 ~ bmi_2>27, dat)$coef[2]\n\nbmi_2 > 27TRUE \n    0.09955787 \n\nlm(chd_1 ~ bmi_1>27, dat)$coef[2]\n\nbmi_1 > 27TRUE \n    0.09984052 \n\nlm(chd_2 ~ bmi_2>27, dat)$coef[2]\n\nbmi_2 > 27TRUE \n    0.09955787 \n\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dplyr_1.1.2\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.31     utf8_1.2.3        R6_2.5.1          fastmap_1.1.1    \n [5] tidyselect_1.2.0  xfun_0.39         magrittr_2.0.3    glue_1.6.2       \n [9] tibble_3.2.1      knitr_1.43        pkgconfig_2.0.3   htmltools_0.5.5  \n[13] generics_0.1.3    rmarkdown_2.22    lifecycle_1.0.3   cli_3.6.1        \n[17] fansi_1.0.4       vctrs_0.6.2       compiler_4.3.0    rstudioapi_0.14  \n[21] tools_4.3.0       pillar_1.9.0      evaluate_0.21     yaml_2.3.7       \n[25] rlang_1.1.1       jsonlite_1.8.5    htmlwidgets_1.6.2"
  },
  {
    "objectID": "posts/2023-08-07-logistic-residuals/index.html",
    "href": "posts/2023-08-07-logistic-residuals/index.html",
    "title": "Logistic regression residuals",
    "section": "",
    "text": "Using REML to adjust for pedigree and then taking residuals to estimate PRS association. However the original trait is binary and the residuals are continuous, how to interpret the effect size?"
  },
  {
    "objectID": "posts/2023-08-07-logistic-residuals/index.html#analysis",
    "href": "posts/2023-08-07-logistic-residuals/index.html#analysis",
    "title": "Logistic regression residuals",
    "section": "Analysis",
    "text": "Analysis\nSimulate data - a confounder influences x and y, x influences y, and y is binary.\n\nset.seed(1234)\nn <- 10000\nu <- rnorm(n)\nx <- rnorm(n) + u\na <- rnorm(n) + x + u - 1\ny <- rbinom(n, 1, plogis(a))\ntable(y)\n\ny\n   0    1 \n6331 3669 \n\n\nEstimate using glm\n\nsummary(glm(y ~ x + u, family=\"binomial\"))$coef\n\n              Estimate Std. Error   z value      Pr(>|z|)\n(Intercept) -0.8456496 0.02749282 -30.75892 9.290785e-208\nx            0.7864091 0.02827687  27.81104 3.189961e-170\nu            0.8236558 0.03882063  21.21696 6.659380e-100\n\n\nTransformation term\n\nmu <- sum(y) / length(y)\ntr <- mu * (1-mu)\n\nGet residuals (raw)\n\nyres_raw <- residuals(glm(y ~ u, family=\"binomial\"), type=\"response\")\nsummary(lm(yres_raw ~ x))$coef\n\n                Estimate  Std. Error     t value     Pr(>|t|)\n(Intercept) 9.859144e-05 0.004038968  0.02441006 9.805260e-01\nx           6.044366e-02 0.002850011 21.20821889 1.114094e-97\n\n\nAfter transformation\n\nlm(yres_raw ~ x)$coef[2] / tr\n\n       x \n0.260214 \n\n\nGet residuals (deviance)\n\nyres_dev <- residuals(glm(y ~ u, family=\"binomial\"))\nsummary(lm(yres_dev ~ x))$coef\n\n               Estimate  Std. Error   t value      Pr(>|t|)\n(Intercept) -0.06563045 0.009654231 -6.798102  1.120585e-11\nx            0.20379274 0.006812301 29.915404 2.104826e-188\n\n\nAfter transformation\n\nlm(yres_dev ~ x)$coef[2] / tr\n\n        x \n0.8773415 \n\n\nRange\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nparam <- expand.grid(\n    b=seq(-1, 1, by=0.1),\n    int=seq(-2,0, by=0.2)\n)\n\no <- lapply(1:nrow(param), \\(i) {\n    u <- rnorm(n)\n    x <- rnorm(n) + u\n    a <- rnorm(n) + x * param$b[i] + u + param$int[i]\n    y <- rbinom(n, 1, plogis(a))\n    mu <- sum(y) / length(y)\n    tr <- mu * (1-mu)\n    bhat <- glm(y ~ x + u, family=\"binomial\")$coef[2]\n    yres_raw <- residuals(glm(y ~ u, family=\"binomial\"), type=\"response\")\n    bhat_raw <- lm(yres_raw ~ x)$coef[2]\n    yres_dev <- residuals(glm(y ~ u, family=\"binomial\"))\n    bhat_dev <- lm(yres_dev ~ x)$coef[2]\n    tibble(bhat, bhat_raw, bhat_dev, bhat_raw_tr = bhat_raw / tr, bhat_dev_tr=bhat_dev / tr, tr=tr)\n}) %>% bind_rows() %>% bind_cols(param, .)\n\nplot\n\npairs(o %>% select(-c(int, tr)))\n\n\n\ndim(o)\n\n[1] 231   8\n\n\nLooks like the deviance residuals are on the same scale after transformation, but will need a bit of work to translate between deviance and raw\nhttps://data.library.virginia.edu/understanding-deviance-residuals/\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.8\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dplyr_1.1.2\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.31     utf8_1.2.3        R6_2.5.1          fastmap_1.1.1    \n [5] tidyselect_1.2.0  xfun_0.39         magrittr_2.0.3    glue_1.6.2       \n [9] tibble_3.2.1      knitr_1.43        pkgconfig_2.0.3   htmltools_0.5.5  \n[13] generics_0.1.3    rmarkdown_2.22    lifecycle_1.0.3   cli_3.6.1        \n[17] fansi_1.0.4       vctrs_0.6.2       withr_2.5.0       compiler_4.3.0   \n[21] rstudioapi_0.14   tools_4.3.0       pillar_1.9.0      evaluate_0.21    \n[25] yaml_2.3.7        rlang_1.1.1       jsonlite_1.8.5    htmlwidgets_1.6.2"
  },
  {
    "objectID": "posts/2023-01-26-convert-lbf-to-summary-stats/index.html",
    "href": "posts/2023-01-26-convert-lbf-to-summary-stats/index.html",
    "title": "Convert Bayes factors to beta and standard error",
    "section": "",
    "text": "Is it possible to convert BF to beta and standard error? According to Giambartolomei et al 2014 -\n\\[\nABF = \\sqrt{1-r} \\times exp(rZ^2/2)\n\\]\nso\n\\[\n|Z| = \\sqrt{\\frac{2 * log(ABF) - log(\\sqrt{1-r})}{r}}\n\\]\nhere \\(r = W / V\\) where V is the variance of the SNP effect estimate\n\\[\nV \\approx \\frac{1}{2np(1-p)}\n\\]\nwhere n is sample size and p is allele frequency (assumes small amount of variance explained in trait and sd of trait is 1).\nRun simulation\n\nUse regional LD matrix to generate summary statistics with a single causal variant\nUse SuSiE to perform fine mapping\nConvert SuSiE Bayes Factors into Z scores, betas, standard errors\nCompare converted Z, beta, se against original simulated Z, beta, SE"
  },
  {
    "objectID": "posts/2023-01-26-convert-lbf-to-summary-stats/index.html#simulation",
    "href": "posts/2023-01-26-convert-lbf-to-summary-stats/index.html#simulation",
    "title": "Convert Bayes factors to beta and standard error",
    "section": "Simulation",
    "text": "Simulation\nLibraries\n\nlibrary(simulateGP)\nlibrary(susieR)\nlibrary(here)\n\nhere() starts at /Users/gh13047/repo/lab-book\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nConversion function for logBF to z, beta, se\n\n#' Convert log Bayes Factor to summary stats\n#'\n#' @param lbf p-vector of log Bayes Factors for each SNP\n#' @param n Overall sample size\n#' @param af p-vector of allele frequencies for each SNP\n#' @param prior_v Variance of prior distribution. SuSiE uses 50\n#'\n#' @return tibble with lbf, af, beta, se, z\nlbf_to_z_cont <- function(lbf, n, af, prior_v=50)\n{\n  se = sqrt(1 / (2 * n * af * (1-af)))\n  r = prior_v / (prior_v + se^2)\n  z = sqrt((2 * lbf - log(sqrt(1-r)))/r)\n  beta <- z * se\n  return(tibble(lbf, af, z, beta, se))\n}\n\nRead in example LD matrix from simulateGP repository\n\nmap <- readRDS(url(\"https://github.com/explodecomputer/simulateGP/raw/master/data/ldobj_5_141345062_141478055.rds\", \"rb\"))\nglimpse(map)\n\nList of 3\n $ ld  : num [1:501, 1:501] 1 0.565 0.566 0.565 0.565 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:501] \"V2000\" \"V2001\" \"V2002\" \"V2003\" ...\n $ map : tibble [501 × 6] (S3: tbl_df/tbl/data.frame)\n  ..$ chr: int [1:501] 5 5 5 5 5 5 5 5 5 5 ...\n  ..$ snp: chr [1:501] \"rs252141\" \"rs252140\" \"rs252139\" \"rs187544\" ...\n  ..$ pos: int [1:501] 141345062 141345192 141345218 141345361 141345678 141345805 141346830 141347360 141347465 141347931 ...\n  ..$ alt: chr [1:501] \"T\" \"T\" \"C\" \"G\" ...\n  ..$ ref: chr [1:501] \"C\" \"C\" \"T\" \"T\" ...\n  ..$ af : num [1:501] 0.627 0.831 0.83 0.831 0.831 ...\n  ..- attr(*, \".internal.selfref\")=<externalptr> \n $ nref: num 503\n\n\nGenerate summary statistics for a single causal variant and\n\nset.seed(1234)\nss <- map$map %>%\n    generate_gwas_params(h2=0.003, Pi=1/nrow(.)) %>%\n    generate_gwas_ss(50000, ld=map$ld)\ntable(ss$beta == 0)\n\n\nFALSE  TRUE \n    1   500 \n\n\n\nplot(-log10(pval) ~ pos, ss)\n\n\n\n\nRun SuSiE\n\nsout <- susie_rss(ss$bhat / ss$se, R = map$ld, n = 50000, bhat = ss$bhat, var_y=1)\n\nWARNING: XtX is not symmetric; forcing XtX to be symmetric by replacing XtX with (XtX + t(XtX))/2\n\nsummary(sout)\n\n\nVariables in credible sets:\n\n variable variable_prob cs\n      286     0.1604616  1\n      306     0.1604616  1\n      291     0.1604616  1\n      300     0.1604616  1\n      274     0.1604616  1\n      284     0.1604616  1\n\nCredible sets summary:\n\n cs cs_log10bf cs_avg_r2 cs_min_r2                variable\n  1   30.37357         1         1 274,284,286,291,300,306\n\n\n\nglimpse(sout)\n\nList of 18\n $ alpha                 : num [1:10, 1:501] 6.9e-35 2.0e-03 2.0e-03 2.0e-03 2.0e-03 ...\n $ mu                    : num [1:10, 1:501] 0.000761 0 0 0 0 ...\n $ mu2                   : num [1:10, 1:501] 2.04e-05 0.00 0.00 0.00 0.00 ...\n $ KL                    : num [1:10] 6.75 -1.24e-14 -1.24e-14 -1.24e-14 -1.24e-14 ...\n $ lbf                   : num [1:10] 6.99e+01 1.24e-14 1.24e-14 1.24e-14 1.24e-14 ...\n $ lbf_variable          : num [1:10, 1:501] -2.51 0 0 0 0 ...\n $ sigma2                : num 1\n $ V                     : num [1:10] 0.00307 0 0 0 0 ...\n $ pi                    : num [1:501] 0.002 0.002 0.002 0.002 0.002 ...\n $ null_index            : num 0\n $ XtXr                  : num [1:501, 1] -0.328 70.558 72.085 70.558 70.558 ...\n $ converged             : logi TRUE\n $ elbo                  : num [1:2] -70876 -70876\n $ niter                 : int 2\n $ X_column_scale_factors: num [1:501] 1 1 1 1 1 1 1 1 1 1 ...\n $ intercept             : num NA\n $ sets                  :List of 5\n  ..$ cs                :List of 1\n  .. ..$ L1: int [1:6] 274 284 286 291 300 306\n  ..$ purity            :'data.frame':  1 obs. of  3 variables:\n  .. ..$ min.abs.corr   : num 1\n  .. ..$ mean.abs.corr  : num 1\n  .. ..$ median.abs.corr: num 1\n  ..$ cs_index          : int 1\n  ..$ coverage          : num 0.963\n  ..$ requested_coverage: num 0.95\n $ pip                   : num [1:501] 0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"class\")= chr \"susie\"\n\n\nGet Z scores from lbf\n\na <- lbf_to_z_cont(sout$lbf_variable[1,], 50000, ss$af, prior_v = 50)\na\n\n# A tibble: 501 × 5\n     lbf     af     z    beta      se\n   <dbl>  <dbl> <dbl>   <dbl>   <dbl>\n 1 -2.51 0.373   1.41 0.00919 0.00654\n 2 -2.43 0.169   1.37 0.0115  0.00844\n 3 -2.37 0.17    1.42 0.0119  0.00842\n 4 -2.43 0.169   1.37 0.0115  0.00844\n 5 -2.43 0.169   1.37 0.0115  0.00844\n 6 -2.52 0.191   1.32 0.0106  0.00805\n 7 -2.43 0.169   1.37 0.0115  0.00844\n 8 -2.44 0.17    1.36 0.0115  0.00842\n 9  2.23 0.0139  3.17 0.0855  0.0270 \n10 -2.43 0.169   1.37 0.0115  0.00844\n# … with 491 more rows\n\n\nRelationship between lbf and re-estimated z\n\nplot(z ~ lbf, a)\n\n\n\n\nNew Z vs original Z\n\nplot(a$z^2 ~ ss$fval)\n\n\n\n\n\nlm(a$z^2 ~ ss$fval)\n\n\nCall:\nlm(formula = a$z^2 ~ ss$fval)\n\nCoefficients:\n(Intercept)      ss$fval  \n     1.5141       0.9834  \n\n\nNew beta vs original beta\n\nplot(a$beta ~ ss$bhat)\n\n\n\n\nTwo causal variants\nSet two causal variants at either end of the region\n\nset.seed(12)\nparam <- map$map\nparam$beta <- 0\nparam$beta[c(10, 490)] <- 0.3\nss <- generate_gwas_ss(param, 50000, ld=map$ld)\nplot(-log10(pval) ~ pos, ss)\n\n\n\n\nFirst variant\n\nsout <- susie_rss(ss$bhat / ss$se, R = map$ld, n = 50000, bhat = ss$bhat, var_y=1)\n\nWARNING: XtX is not symmetric; forcing XtX to be symmetric by replacing XtX with (XtX + t(XtX))/2\n\na1 <- lbf_to_z_cont(sout$lbf_variable[1,], 50000, ss$af, prior_v = 50)\n\nWarning in sqrt((2 * lbf - log(sqrt(1 - r)))/r): NaNs produced\n\nplot(a1$beta ~ ss$bhat)\n\n\n\n\n\na2 <- lbf_to_z_cont(sout$lbf_variable[2,], 50000, ss$af, prior_v = 50)\nplot(a2$beta ~ ss$bhat)\n\n\n\n\nThis looks good - it’s setting different values to 0 in the two lbf vectors that correspond to two causal variants\n\n\nsessionInfo()\n\nR version 4.2.1 Patched (2022-09-06 r82817)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dplyr_1.0.10     here_1.0.1       susieR_0.12.27   simulateGP_0.1.2\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9         plyr_1.8.7         compiler_4.2.1     pillar_1.8.1      \n [5] tools_4.2.1        digest_0.6.31      jsonlite_1.8.4     evaluate_0.19     \n [9] lifecycle_1.0.3    tibble_3.1.8       gtable_0.3.1       lattice_0.20-45   \n[13] pkgconfig_2.0.3    rlang_1.0.6        Matrix_1.4-1       DBI_1.1.3         \n[17] cli_3.5.0          yaml_2.3.6         xfun_0.36          fastmap_1.1.0     \n[21] stringr_1.5.0      knitr_1.41         generics_0.1.3     vctrs_0.5.1       \n[25] htmlwidgets_1.5.4  rprojroot_2.0.3    tidyselect_1.2.0   grid_4.2.1        \n[29] reshape_0.8.9      glue_1.6.2         R6_2.5.1           fansi_1.0.3       \n[33] rmarkdown_2.16     mixsqp_0.3-48      irlba_2.3.5.1      ggplot2_3.4.0     \n[37] magrittr_2.0.3     MASS_7.3-58.1      matrixStats_0.63.0 scales_1.2.1      \n[41] htmltools_0.5.4    assertthat_0.2.1   colorspace_2.0-3   utf8_1.2.2        \n[45] stringi_1.7.8      munsell_0.5.0      crayon_1.5.2"
  },
  {
    "objectID": "posts/2022-12-16-vqtl/index.html",
    "href": "posts/2022-12-16-vqtl/index.html",
    "title": "Inflation of vQTLs",
    "section": "",
    "text": "This paper describes how incomplete linkage disequilibrium can lead to inflated test statistics for interactions - https://www.nature.com/articles/s41586-021-03765-z. Because interaction terms contribute to variance heterogeneity across genotype classes, this could also inflate vQTL detection methods.\nExample model\nSuppose a system with three variants and one trait. The trait $x$ is influenced by a single additive causal variant $y_1$. But there is another variant in LD with this causal variant $y_2$. Finally, a third variant is independent of all other variables (think of that as a trans SNP). So\n\\[\nx_i = y_{1,i} + e_i\n\\]\nBut we test for an interaction between y_2 and y_3.\nRun some simulations…\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nset.seed(12345)\n\ntest_drm <- function(g, y)\n{\n  y.i <- tapply(y, g, median, na.rm=T)  \n  z.ij <- abs(y - y.i[g+1])\n  summary(lm(z.ij ~ g))$coef %>%\n    as_tibble() %>%\n    slice(2) %>%\n    mutate(method=\"drm\")\n}\n\ncorrelated_binomial <- function (nid, p1, p2, rho, n = 2, round = TRUE, print = FALSE) \n{\n    p <- p1\n    q <- p2\n    a <- function(rho, p, q) {\n        rho * sqrt(p * q * (1 - p) * (1 - q)) + (1 - p) * (1 - \n            q)\n    }\n    a.0 <- a(rho, p, q)\n    prob <- c(`(0,0)` = a.0, `(1,0)` = 1 - q - a.0, `(0,1)` = 1 - \n        p - a.0, `(1,1)` = a.0 + p + q - 1)\n    if (min(prob) < 0) {\n        print(prob)\n        stop(\"Error: a probability is negative.\")\n    }\n    n.sim <- nid\n    u <- sample.int(4, n.sim * n, replace = TRUE, prob = prob)\n    y <- floor((u - 1)/2)\n    x <- 1 - u%%2\n    x <- colSums(matrix(x, nrow = n))\n    y <- colSums(matrix(y, nrow = n))\n    if (round) {\n        x <- round(x)\n        y <- round(y)\n    }\n    if (print) {\n        print(table(x, y))\n        print(stats::cor(x, y))\n    }\n    return(cbind(x, y))\n}\n\ngendatp <- function(n, p1, p2, p3, r1)\n{\n    dat <- correlated_binomial(n, p1, p2, r1) %>% as_tibble()\n    names(dat) <- c(\"y1\", \"y2\")\n    dat$y3 <- rbinom(n, 1, p3)\n    return(dat)\n}\n\nrun_simp <- function(param, i)\n{\n    set.seed(i*10)\n    dat <- gendatp(param$n[i], param$p1[i], param$p2[i], param$p3[i], param$r1[i])\n    x <- dat$y1 + rnorm(nrow(dat), sd=sd(dat$y1)/4)\n    mod1 <- lm(x ~ y2 + y3, dat)\n    mod2 <- lm(x ~ y2 + y3 + y2*y3, dat)\n    amod <- anova(mod1, mod2)\n    param$F[i] <- amod$P[2]\n    o1 <- test_drm(dat$y1, x)\n    o2 <- test_drm(dat$y2, x)\n    o3 <- test_drm(dat$y3, x)\n    param$drm1[i] <- o1$`Pr(>|t|)`\n    param$drm2[i] <- o2$`Pr(>|t|)`\n    param$drm3[i] <- o3$`Pr(>|t|)`\n    return(param[i,])\n}\n\nparam <- expand.grid(\n    p1=0.1,\n    p2=0.1,\n    p3=0.5,\n    p4=0.1,\n    n=1000,\n    r1=seq(0, 1, by=0.2),\n    sim=1:500,\n    r2=NA,\n    F=NA,\n    drm1=NA,\n    drm2=NA,\n    drm3=NA\n)\n\nresp <- lapply(1:nrow(param), function(x) run_simp(param, x)) %>% bind_rows()\nstr(resp)\n\n'data.frame':   3000 obs. of  12 variables:\n $ p1  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p2  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ p3  : num  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ...\n $ p4  : num  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n $ n   : num  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ...\n $ r1  : num  0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 ...\n $ sim : int  1 1 1 1 1 1 2 2 2 2 ...\n $ r2  : logi  NA NA NA NA NA NA ...\n $ F   : num  0.4136 0.0371 0.0567 0.5604 0.0364 ...\n $ drm1: num  0.288 0.376 0.16 0.215 0.998 ...\n $ drm2: num  1.64e-02 3.88e-17 1.58e-25 2.64e-26 9.41e-19 ...\n $ drm3: num  0.9178 0.1475 0.0181 0.1416 0.7348 ...\n - attr(*, \"out.attrs\")=List of 2\n  ..$ dim     : Named int [1:12] 1 1 1 1 1 6 500 1 1 1 ...\n  .. ..- attr(*, \"names\")= chr [1:12] \"p1\" \"p2\" \"p3\" \"p4\" ...\n  ..$ dimnames:List of 12\n  .. ..$ p1  : chr \"p1=0.1\"\n  .. ..$ p2  : chr \"p2=0.1\"\n  .. ..$ p3  : chr \"p3=0.5\"\n  .. ..$ p4  : chr \"p4=0.1\"\n  .. ..$ n   : chr \"n=1000\"\n  .. ..$ r1  : chr [1:6] \"r1=0.0\" \"r1=0.2\" \"r1=0.4\" \"r1=0.6\" ...\n  .. ..$ sim : chr [1:500] \"sim=  1\" \"sim=  2\" \"sim=  3\" \"sim=  4\" ...\n  .. ..$ r2  : chr \"r2=NA\"\n  .. ..$ F   : chr \"F=NA\"\n  .. ..$ drm1: chr \"drm1=NA\"\n  .. ..$ drm2: chr \"drm2=NA\"\n  .. ..$ drm3: chr \"drm3=NA\""
  },
  {
    "objectID": "posts/2022-12-16-vqtl/index.html#type-1-error-of-vqtls",
    "href": "posts/2022-12-16-vqtl/index.html#type-1-error-of-vqtls",
    "title": "Inflation of vQTLs",
    "section": "Type 1 error of vQTLs",
    "text": "Type 1 error of vQTLs\nThis is what happens to the genetic interaction between y_2 and y_3 - remember that neither of these have a causal effect, and there is no interaction term, however y_2 is correlated with the causal variant y_1\n\nggplot(resp, aes(x=as.factor(r1), y=-log10(F))) +\ngeom_boxplot() +\ngeom_hline(yintercept=-log10(0.05/nrow(resp))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"Interaction -log10 p for y2xy3\", x=\"LD between tagging\\nvariant and causal variant\")\n\n\n\n\nSo you get some false positives even after bonferroni correction. However now look at what happens to the variance QTL estimate for y_2 (the SNP that has no interaction but is in incomplete LD with the additive SNP y_1). Here we’ll use the DRM method to test for vQTL effects at y_2\n\nggplot(resp, aes(x=r1, y=-log10(drm2))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\")\n\n\n\n\nThis is really extreme type 1 sensitivity to incomplete LD. There’s no problem at the actual causal locus (y_1)\n\nggplot(resp, aes(x=r1, y=-log10(drm1))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\")\n\n\n\n\nOr at the unlinked locus y_3\n\nggplot(resp, aes(x=r1, y=-log10(drm3))) +\ngeom_boxplot(aes(fill=as.factor(r1))) +\nscale_fill_brewer(type=\"seq\") +\nlabs(y=\"DRM -log10 p\", x=\"LD between tagging\\nvariant and causal variant\", fill=\"\")\n\n\n\n\nImplications - performing an exhaustive search is going to give quite problematic results if the main effects aren’t controlled. So you’d really have to know what all the main effects are before performing the vQTL tests in order to control for them. Note that incomplete control of the main effects is inevitable and we should be anticipating elevated type 1 error rates for any SNPs that are in the region of any large main effects."
  },
  {
    "objectID": "posts/2022-12-16-vqtl/index.html#power-issues-when-controlling-for-main-effects",
    "href": "posts/2022-12-16-vqtl/index.html#power-issues-when-controlling-for-main-effects",
    "title": "Inflation of vQTLs",
    "section": "Power issues when controlling for main effects",
    "text": "Power issues when controlling for main effects\nThe other problem is actually controlling for main effects. Suppose that a probe has two distal SNPs that interact e.g.\n\nn <- 10000\ng1 <- rbinom(n, 2, 0.4)\ng2 <- rbinom(n, 2, 0.4)\ny <- g1 + g2 + g1 * g2 + rnorm(n, sd=1.5)\ntest_drm(g1, y) %>% str\n\ntibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n $ Estimate  : num 0.362\n $ Std. Error: num 0.0173\n $ t value   : num 20.9\n $ Pr(>|t|)  : num 3.16e-95\n $ method    : chr \"drm\"\n\n\nThe DRM method finds a big vQTL effect here because of the GxG interaction - so it’s detecting that g1 might be interacting with something.\nIf we adjust for the main effects of g1 and g2 now look at DRM\n\nyres <- residuals(lm(y ~ g1 + g2))\ntest_drm(g1, yres) %>% str\n\ntibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n $ Estimate  : num 0.0194\n $ Std. Error: num 0.0138\n $ t value   : num 1.4\n $ Pr(>|t|)  : num 0.161\n $ method    : chr \"drm\"\n\n\nThe test statistic for the interaction test has massively attenuated.\nWhere does this leave us?\n\nIf a SNP is a known additive causal variant then it is relatively safe from type 1 error\nIf a SNP is not a known additive causal variant, then it is susceptible to type 1 error due to incomplete LD with actual additive causal variants\nIf we adjust the probe for additive causal variants before testing the SNP, we risk drastically reducing the vQTL effect that arise due to GxG interactions\nNote that this applies to GxE for when adjusting for other covariates too - e.g. if we adjust probes for smoking, age, sex, cell type etc and we are trying to find interactions with those based on vQTLs then the power to identify those vQTL effects drastically reduces"
  },
  {
    "objectID": "posts/2022-12-16-vqtl/index.html#power-of-vqtl-vs-interaction",
    "href": "posts/2022-12-16-vqtl/index.html#power-of-vqtl-vs-interaction",
    "title": "Inflation of vQTLs",
    "section": "Power of vQTL vs interaction",
    "text": "Power of vQTL vs interaction\nSuppose we simulate a GxE interaction. We can try to detect it either using a vQTL method (e.g. DRM) or using a direct interaction test.\n\nsim_gxe <- function(n, p, bi)\n{\n  params <- environment() %>% as.list() %>% as_tibble()\n  g <- rbinom(n, 2, p)\n  e <- rnorm(n)\n  y <- g + bi * g*e + e + rnorm(n)\n  \n  bind_rows(\n    test_drm(g, y),\n    summary(lm(y ~ g*e))$coef %>%\n      as_tibble() %>%\n      slice(n=4) %>%\n      mutate(method=\"interaction\")\n  ) %>%\n    bind_cols(., params)\n}\n\nparam <- expand.grid(\n  n=1000,\n  bi=seq(0,1,by=0.01),\n  nsim=10,\n  p=0.5\n) %>% select(-nsim)\nres <- lapply(1:nrow(param), function(i) do.call(sim_gxe, param[i,])) %>% bind_rows()\nres %>%\n  ggplot(., aes(x=bi, y=-log10(`Pr(>|t|)`))) +\n  geom_point(aes(colour=method))\n\n\n\n\nThe direct interaction test seems much better powered to detect these associations."
  },
  {
    "objectID": "posts/2023-01-30-conditional-f-winners-curse/index.html",
    "href": "posts/2023-01-30-conditional-f-winners-curse/index.html",
    "title": "Can winner’s curse generate a high conditional F statistic",
    "section": "",
    "text": "Multivariable MR requires that exposure effects are heterogeneous, indicated by the conditional F-statistic. If the sample overlap between two exposures is 0, how much can winner’s curse drive apparent heterogeneity?\n\nlibrary(simulateGP)\nlibrary(dplyr)\nlibrary(MVMR)\nlibrary(ggplot2)\n\n# 1. Simulate architecture for nsnps\n# 2. Simualate two independent GWAS summary datasets each with sample size nid\n# 3. Identify instruments significant in each dataset\n# 4. Calculate conditional Fstat for these independent instruments\n# 5. Repeat for a random selection of instruments (no selection)\nsim <- function(nsnp, nid)\n{\n    map <- tibble(snp=1:nsnp, af=runif(nsnp, 0.01, 0.99)) %>%\n        generate_gwas_params(h2=0.4, S=-0.1, Pi=1)\n    map\n\n    ss1 <- generate_gwas_ss(map, nid)\n    ss2 <- generate_gwas_ss(map, nid)\n\n    table(ss1$pval < 5e-8, ss2$pval < 5e-8)\n\n    inst1 <- subset(ss1, pval < 5e-8)$snp\n    inst2 <- subset(ss2, pval < 5e-8)$snp\n    insts <- unique(c(inst1, inst2))\n\n    mvmrdat <- format_mvmr(\n        BXGs = cbind(ss1$bhat[insts], ss2$bhat[insts]), \n        BYG = runif(length(insts)), \n        seBXGs = cbind(ss1$se[insts], ss2$se[insts]),\n        seBYG = rep(0.1, length(insts)),\n        RSID = insts\n    )\n    selected <- strength_mvmr(mvmrdat, 0)\n\n    insts_random <- sample(map$snp, length(insts), replace=FALSE)\n    mvmrdat_random <- format_mvmr(\n        BXGs = cbind(ss1$bhat[insts_random], ss2$bhat[insts_random]), \n        BYG = runif(length(insts_random)), \n        seBXGs = cbind(ss1$se[insts_random], ss2$se[insts_random]),\n        seBYG = rep(0.1, length(insts_random)),\n        RSID = insts\n    )    \n    random <- strength_mvmr(mvmrdat_random, 0)\n    return(list(selected=selected, random=random, ninst=length(insts)))\n}\n\n# Simulation parameters\nparam <- expand.grid(\n    nsnp=seq(5000, 100000, by=5000), \n    nid=240000,\n    nsim=1:5\n)\n\n# Run simulations\no <- lapply(1:nrow(param), function(i)\n{\n    x <- param[i,]\n    o <- sim(x$nsnp, x$nid)\n    bind_cols(\n        x, \n        tibble(\n            what=c(\"selected\", \"random\"), \n            Fstat=c(o$selected$exposure1[1], o$random$exposure1[1]), \n            ninst=o$ninst\n        )\n    )\n}) %>% bind_rows()\n\n\n# Plot\nggplot(o, aes(x=as.factor(nsnp), y=Fstat)) +\ngeom_boxplot(aes(fill=what)) +\nlabs(x=\"Number of causal variants\", y=\"Conditional F-statistic\", fill=\"\") +\ntheme(axis.text.x=element_text(angle=90))\n\n\n\n\nSo, it looks like winner’s curse alone can generate quite large conditional F stat ~4 under some circumstances - that the number of causal variants is very high like 60k with n=240000 in each GWAS (which essentially means that most of the GWAS hits are hovering around the significance threshold, which is where winner’s curse is maximised). The empirical analysis in ukbb will be useful to get a more concrete answer on how much it’s realistically contributing\n\n\nsessionInfo()"
  },
  {
    "objectID": "posts/2023-07-07-mr-network-confounding/index.html",
    "href": "posts/2023-07-07-mr-network-confounding/index.html",
    "title": "Confounder instruments via networks",
    "section": "",
    "text": "In a previous post looked at the rate of bias as GWAS sample size increases. It was predicated on smaller effects more likely acting via confounders.\nWhat is the justification for smaller effects acting via confounders? After all, all effects likely act via mediators. Some of those mediators will confound a X-Y relationship while some will not. Is there any reason to believe that as effects get smaller they’re more likely to act via a confounder?\nImagine a large DAG representing the genotype-phenotype map. All nodes that have no parents are genetic variants. All other nodes are traits. If you choose any two traits, one as the exposure and one as the outcome, what is the nature of the instruments for the exposure? Hypothesis: the instruments that are mediated by confounders are likely to be more distal from the exposure, compared to those that act via non-confounding mediators.\nThese simulations aim to test that.\n\nlibrary(dagitty)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(furrr)\n\nLoading required package: future\n\nlibrary(ggplot2)\nlibrary(tictoc)\n\nsimulategraphconf <- function(n, p) {\n    # Generate graph\n    g <- dagitty::randomDAG(n, p)\n\n    # All ancestors\n    anc <- lapply(names(g), \\(x) ancestors(g, x))\n    names(anc) <- names(g)\n\n    # Identify genetic factors (no ancestors)\n    temp <- lapply(anc, length)\n    temp <- tibble(node=names(temp), nanc=unlist(temp))\n\n    gen <- subset(temp, nanc==1)$node\n    message(\"Number of genetic variants: \", length(gen))\n\n    traits <- subset(temp, nanc != 1)$node\n    message(\"Number of traits: \", length(traits))\n    # Find distance of all genetic variants to \n\n    # Find all trait pairs\n    tp <- lapply(traits, \\(tr) {\n        temp <- ancestors(g, tr)\n        tibble(x=temp, y=tr) %>%\n            filter(! x %in% c(gen, tr))\n    }) %>% bind_rows\n    # tp <- expand.grid(x=traits, y=traits) %>%\n    #   as_tibble() %>%\n    #   filter(x != y)\n    message(\"Number of trait pairs: \", nrow(tp))\n\n    res <- furrr::future_map(1:min(nrow(tp), 500), \\(i)\n    {\n        x_ancestors <- ancestors(g, tp$x[i])\n        x_ancestors <- x_ancestors[x_ancestors %in% gen]\n        y_ancestors <- ancestors(g, tp$y[i])\n        y_ancestors <- y_ancestors[y_ancestors %in% gen]\n\n        conf <- c()\n        nonconf <- c()\n        for(a in y_ancestors)\n        {\n            x <- paths(g, a, tp$y[i], dir=T)$paths\n            if(any(!grepl(paste0(\" \", tp$x[i], \" \"), x)))\n            {\n                conf <- c(conf, a)\n            } else {\n                nonconf <- c(nonconf, a)\n            }\n        }\n        bind_rows(\n            lapply(conf, function(co)\n            {\n                pa <- paths(g, co, tp$x[i], directed=TRUE)$paths\n                if(length(pa) != 0)\n                {\n                    tibble(\n                        x=tp$x[i],\n                        y=tp$y[i],\n                        gen=co,\n                        type=\"confounder\",\n                        w=pa %>%\n                            sapply(., function(x)\n                            {\n                                stringr::str_count(x, \"->\") %>%\n                                unlist() %>%\n                                {0.2^.}\n                            }) %>% sum\n                    )\n                } else {\n                    NULL\n                }\n            }) %>% bind_rows(),\n            lapply(nonconf, function(co)\n            {\n                pa <- paths(g, co, tp$x[i], directed=TRUE)$paths\n                if(length(pa) != 0)\n                {\n                    tibble(\n                        x=tp$x[i],\n                        y=tp$y[i],\n                        gen=co,\n                        type=\"direct\",\n                        w=pa %>%\n                            sapply(., function(x)\n                            {\n                                stringr::str_count(x, \"->\") %>%\n                                unlist() %>%\n                                {0.2^.}\n                            }) %>% sum\n                    )\n                } else {\n                    NULL\n                }\n            }) %>% bind_rows()\n        )\n    }) %>% bind_rows()\n    # res$causal <- paste(res$x, res$y) %in% paste(tpc$x, tpc$y)\n    return(res)\n}\n\nset.seed(1234) # note this doesn't work for dagitty - for seed should use pcalg\nplan(multisession, workers=1)\ntic()\nres <- simulategraphconf(50, 0.1)\n\nNumber of genetic variants: 13\n\n\nNumber of traits: 37\n\n\nNumber of trait pairs: 191\n\ntoc()\n\n16.505 sec elapsed\n\nres %>% group_by(type) %>% summarise(\n    n=n(),\n    w=mean(w)\n)\n\n# A tibble: 2 × 3\n  type           n      w\n  <chr>      <int>  <dbl>\n1 confounder   310 0.0882\n2 direct       190 0.120 \n\n\nRun the simulations (note, ran this externally on epifranklin)\n\nparam <- expand.grid(\n    n=seq(75, 150, by=25),\n    p=seq(0.01, 0.1, by=0.01),\n    rep=c(1:10)\n)\n\nres <- lapply(1:nrow(param), \\(i) {\n    message(i)\n    res <- simulategraphconf(param$n[i], param$p[i])\n    res %>% group_by(type) %>% summarise(\n        ntype=n(),\n        w=mean(w)\n    ) %>% mutate(\n        n=param$n[i],\n        p=param$p[i],\n        rep=param$rep[i]\n    )\n}) %>% bind_rows(res)\n\n\nres <- readRDS(\"mrnetworkconf.rds\")\nres %>%\nggplot(., aes(x=p, y=w)) +\ngeom_jitter(aes(colour=type, size=n), alpha=0.4, width=0.001) +\ngeom_smooth(aes(colour=type), se=F) +\nlabs(size=\"Graph size\", x=\"Graph density\", y=\"Mean effect size (arbitrary units)\", colour=\"Mediating traits\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 594 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 594 rows containing missing values (`geom_point()`).\n\n\n\n\n\nSo the effect sizes for instruments acting via non-confounders tend to be substantially larger than those acting via confounders.\n\nggsave(\"mrnetworkconf.pdf\")\n\nSaving 7 x 5 in image\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 594 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 594 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/2023-07-07-mr-network-confounding/index.html#alternative-graph-generating-method",
    "href": "posts/2023-07-07-mr-network-confounding/index.html#alternative-graph-generating-method",
    "title": "Confounder instruments via networks",
    "section": "Alternative graph generating method",
    "text": "Alternative graph generating method\nPhenomic layers. But it’s not clear how to generate this realistically. Not implemented in sims yet.\n\nsim_graph <- function(ng, ne, np, nm, nd, nge, nep, npm, nmd) {\n    links <- bind_rows(\n        tibble(\n            x=sample(paste0(\"g\", 1:ng), nge, replace=TRUE),\n            y=sample(paste0(\"e\", 1:ne), nge, replace=TRUE)\n        ),\n        tibble(\n            x=sample(paste0(\"e\", 1:ne), nep, replace=TRUE),\n            y=sample(paste0(\"p\", 1:np), nep, replace=TRUE)\n        ),\n        tibble(\n            x=sample(paste0(\"p\", 1:np), npm, replace=TRUE),\n            y=sample(paste0(\"m\", 1:nm), npm, replace=TRUE)\n        ),\n        tibble(\n            x=sample(paste0(\"m\", 1:nm), nmd, replace=TRUE),\n            y=sample(paste0(\"d\", 1:nd), nmd, replace=TRUE)\n        )\n    ) %>% mutate(\n        rel=paste0(x, \" -> \", y)\n    )\n    g <- dagitty(paste(\"dag{\",\n        paste(links$rel, collapse=\"\\n\"),\n    \"}\"))\n    return(g)\n}\n\ng <- sim_graph(400, 200, 300, 400, 20, 600, 200, 200, 300)\nplot(g)\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\nWarning in arrows(ax1[directed], -ay1[directed], ax2[directed], -ay2[directed],\n: zero-length arrow is of indeterminate angle and so skipped\n\n\n\n\nancestors(g, \"d1\") %>% {grep(\"g\", ., value=TRUE)}\n\n[1] \"g79\"  \"g341\" \"g262\" \"g199\" \"g186\" \"g93\"  \"g170\"\n\npaths(g, \"g79\", \"d1\", dir=T)\n\n$paths\n[1] \"g79 -> e43 -> p236 -> m215 -> d1\"\n\n$open\n[1] TRUE\n\npaths(g, \"g79\", \"d3\", dir=T)\n\n$paths\nlist()\n\n$open\nlist()"
  },
  {
    "objectID": "posts/2023-07-07-mr-network-confounding/index.html#meta-regression-using-exposure-effect-size",
    "href": "posts/2023-07-07-mr-network-confounding/index.html#meta-regression-using-exposure-effect-size",
    "title": "Confounder instruments via networks",
    "section": "Meta regression using exposure effect size",
    "text": "Meta regression using exposure effect size\n\nSimulate individual level data in system in which u causes x and y, and both x and u have independent instruments\nIdentify instruments (should include many gx and some gu)\nEstimate heterogeneity contribution from each instrument\nMeta regression of instrument strength against heterogeneity\n\n\nlibrary(simulateGP)\nlibrary(TwoSampleMR)\n\nTwoSampleMR version 0.5.7 \n[>] New: Option to use non-European LD reference panels for clumping etc\n[>] Some studies temporarily quarantined to verify effect allele\n[>] See news(package='TwoSampleMR') and https://gwas.mrcieu.ac.uk for further details\n\n\n\nAttaching package: 'TwoSampleMR'\n\n\nThe following objects are masked from 'package:simulateGP':\n\n    allele_frequency, contingency, get_population_allele_frequency\n\n# 1. simulate system\nnid <- 100000\nnsnp <- 1000\ngx <- make_geno(nid, nsnp, 0.4)\ngu <- make_geno(nid, nsnp, 0.4)\nbx <- choose_effects(nsnp, sqrt(0.4))\nbu <- choose_effects(nsnp, sqrt(0.4))\nu <- make_phen(bu, gu)\nx <- make_phen(c(bx, 0.4), cbind(gx, u))\ny <- make_phen(c(0.4, 0.4), cbind(x, u))\n\n\n# 2. Identify instruments\ndat <- get_effs(x, y, cbind(gx, gu))\ndats <- subset(dat, pval.exposure < (0.05/2000))\nstr(dat)\n\ntibble [2,000 × 18] (S3: tbl_df/tbl/data.frame)\n $ SNP                : int [1:2000] 1 2 3 4 5 6 7 8 9 10 ...\n $ exposure           : chr [1:2000] \"X\" \"X\" \"X\" \"X\" ...\n $ id.exposure        : chr [1:2000] \"X\" \"X\" \"X\" \"X\" ...\n $ outcome            : chr [1:2000] \"Y\" \"Y\" \"Y\" \"Y\" ...\n $ id.outcome         : chr [1:2000] \"Y\" \"Y\" \"Y\" \"Y\" ...\n $ beta.exposure      : num [1:2000] -0.0368 0.0252 0.0335 -0.0272 0.0328 ...\n $ beta.outcome       : num [1:2000] -0.0185 0.00598 0.01851 -0.00805 0.00889 ...\n $ se.exposure        : num [1:2000] 0.00456 0.00457 0.00456 0.00457 0.00455 ...\n $ se.outcome         : num [1:2000] 0.00456 0.00457 0.00456 0.00457 0.00455 ...\n $ pval.exposure      : num [1:2000] 6.98e-16 3.51e-08 2.07e-13 2.48e-09 5.95e-13 ...\n $ pval.outcome       : num [1:2000] 5.03e-05 1.91e-01 5.01e-05 7.81e-02 5.09e-02 ...\n $ samplesize.exposure: num [1:2000] 1e+05 1e+05 1e+05 1e+05 1e+05 1e+05 1e+05 1e+05 1e+05 1e+05 ...\n $ samplesize.outcome : num [1:2000] 1e+05 1e+05 1e+05 1e+05 1e+05 1e+05 1e+05 1e+05 1e+05 1e+05 ...\n $ units.exposure     : chr [1:2000] \"SD\" \"SD\" \"SD\" \"SD\" ...\n $ units.outcome      : chr [1:2000] \"SD\" \"SD\" \"SD\" \"SD\" ...\n $ rsq.exposure       : num [1:2000] 0.000651 0.000304 0.000539 0.000355 0.000519 ...\n $ rsq.outcome        : num [1:2000] 1.64e-04 1.71e-05 1.64e-04 3.10e-05 3.81e-05 ...\n $ mr_keep            : logi [1:2000] TRUE TRUE TRUE TRUE TRUE TRUE ...\n\n\n\ntable(dats$SNP > 1000)\n\n\nFALSE  TRUE \n  592   203 \n\n\n\ndats$strength <- cut(-log10(dats$pval.exposure), breaks=quantile(-log10(dats$pval.exposure), probs=seq(0, 1, 0.1)))\ntable(dats$strength)\n\n\n(4.63,5.55] (5.55,6.55] (6.55,7.89] (7.89,9.76] (9.76,11.9] (11.9,14.9] \n         79          79          80          79          80          79 \n(14.9,20.8] (20.8,28.6] (28.6,44.3]  (44.3,283] \n         79          80          79          80 \n\n\n\n# Overall MR estimate\nres <- mr(dats, method=\"mr_ivw\")\n\nAnalysing 'X' on 'Y'\n\nres\n\n  id.exposure id.outcome outcome exposure                    method nsnp\n1           X          Y       Y        X Inverse variance weighted  795\n          b        se pval\n1 0.4757469 0.0102651    0\n\n\nStratify by instrument strength\n\ndats %>% group_by(strength) %>% do({mr(., method=\"mr_ivw\")}) %>% \n    ggplot(., aes(x=strength, y=b)) +\n    geom_point() +\n    geom_errorbar(aes(ymin=b-se*1.96, ymax=b+se*1.96), width=0) +\n    geom_hline(yintercept=0.4) +\n    labs(x=\"Instrument strength\", y=\"MR effect estimate\")\n\nAnalysing 'X' on 'Y'\nAnalysing 'X' on 'Y'\nAnalysing 'X' on 'Y'\nAnalysing 'X' on 'Y'\nAnalysing 'X' on 'Y'\nAnalysing 'X' on 'Y'\nAnalysing 'X' on 'Y'\nAnalysing 'X' on 'Y'\nAnalysing 'X' on 'Y'\nAnalysing 'X' on 'Y'\nAnalysing 'X' on 'Y'\n\n\n\n\n\n\n# 3. Estimate heterogeneity\nss <- mr_singlesnp(dats) %>% filter(!grepl(\"All\", SNP)) %>% mutate(SNP = as.numeric(SNP))\nss$qj <- (1/ss$se^2) * (res$b - ss$b)^2\nss$str <- (dats$beta.exposure/dats$se.exposure)^2\nss$str2 <- -log10(dats$pval.exposure)\nmr_heterogeneity(dats, method=\"mr_ivw\")$Q == sum(ss$qj)\n\n[1] TRUE\n\n\n\nMeta regression\n\n\n# All SNPs\nsummary(lm(qj ~ str, data=ss))\n\n\nCall:\nlm(formula = qj ~ str, data = ss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -8.171  -7.145  -5.373   1.080 129.726 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.416813   0.626477  13.435  < 2e-16 ***\nstr         -0.013380   0.004787  -2.795  0.00531 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.21 on 793 degrees of freedom\nMultiple R-squared:  0.009757,  Adjusted R-squared:  0.008508 \nF-statistic: 7.813 on 1 and 793 DF,  p-value: 0.005312\n\n\nHigher strength means lower heterogeneity\n\n# Only x snps\nsummary(lm(qj ~ str, data=subset(ss, SNP <= 1000)))\n\n\nCall:\nlm(formula = qj ~ str, data = subset(ss, SNP <= 1000))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8033 -0.8923 -0.4404  0.6002 13.0460 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.4515093  0.1019608   4.428 1.13e-05 ***\nstr         0.0109704  0.0006811  16.106  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.784 on 590 degrees of freedom\nMultiple R-squared:  0.3054,    Adjusted R-squared:  0.3042 \nF-statistic: 259.4 on 1 and 590 DF,  p-value: < 2.2e-16\n\n\nWouldn’t expect this - weaker SNPs should contribute more heterogeneity due to weak instrument bias\n\nsummary\n\nfunction (object, ...) \nUseMethod(\"summary\")\n<bytecode: 0x12d4ab510>\n<environment: namespace:base>\n\n\n\n# Only u snps\nsummary(lm(qj ~ str, data=subset(ss, SNP > 1000)))\n\n\nCall:\nlm(formula = qj ~ str, data = subset(ss, SNP > 1000))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.201  -8.024  -0.785   7.060  44.492 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.99448    1.89762   0.524    0.601    \nstr          0.62044    0.04559  13.609   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.75 on 201 degrees of freedom\nMultiple R-squared:  0.4796,    Adjusted R-squared:  0.477 \nF-statistic: 185.2 on 1 and 201 DF,  p-value: < 2.2e-16\n\n\n\nplot(qj ~ str, ss)\n\n\n\nplot(qj ~ str, ss %>% filter(SNP > 1000))\n\n\n\nplot(qj ~ str, ss %>% filter(SNP <= 1000))\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] TwoSampleMR_0.5.7 simulateGP_0.1.2  purrr_1.0.1       tictoc_1.2       \n[5] ggplot2_3.4.2     furrr_0.3.1       future_1.33.0     dplyr_1.1.2      \n[9] dagitty_0.3-1    \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.3        generics_0.1.3    stringi_1.7.12    lattice_0.21-8   \n [5] listenv_0.9.0     digest_0.6.31     magrittr_2.0.3    evaluate_0.21    \n [9] grid_4.3.0        fastmap_1.1.1     plyr_1.8.8        Matrix_1.5-4     \n[13] jsonlite_1.8.5    mgcv_1.8-42       fansi_1.0.4       scales_1.2.1     \n[17] textshaping_0.3.6 codetools_0.2-19  cli_3.6.1         rlang_1.1.1      \n[21] parallelly_1.36.0 munsell_0.5.0     splines_4.3.0     withr_2.5.0      \n[25] yaml_2.3.7        tools_4.3.0       parallel_4.3.0    colorspace_2.1-0 \n[29] boot_1.3-28.1     globals_0.16.2    curl_5.0.0        vctrs_0.6.2      \n[33] R6_2.5.1          lifecycle_1.0.3   stringr_1.5.0     V8_4.3.2         \n[37] htmlwidgets_1.6.2 MASS_7.3-58.4     ragg_1.2.5        pkgconfig_2.0.3  \n[41] pillar_1.9.0      gtable_0.3.3      glue_1.6.2        Rcpp_1.0.10      \n[45] systemfonts_1.0.4 xfun_0.39         tibble_3.2.1      tidyselect_1.2.0 \n[49] rstudioapi_0.14   knitr_1.43        farver_2.1.1      htmltools_0.5.5  \n[53] nlme_3.1-162      labeling_0.4.2    rmarkdown_2.22    compiler_4.3.0"
  },
  {
    "objectID": "posts/2023-07-06-2sls-se/index.html",
    "href": "posts/2023-07-06-2sls-se/index.html",
    "title": "Standard error in two-stage least squares",
    "section": "",
    "text": "How to calculate the 2sls SE that is returned by e.g. ivreg\n\nlibrary(ivreg)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nset.seed(1234)\n\n# Simulate\nn <- 1000\nz <- matrix(rnorm(n*2), n)\nu <- rnorm(n)\nx <-  z %*% c(0.1, 0.2) + rnorm(n) + u\ny <- u + x * 0.3 + rnorm(n)\n\nd <- tibble(z[,1], z[,2], x, y)\nnames(d) <- c(\"z1\", \"z2\", \"x\", \"y\")\nhead(d)\n\n# A tibble: 6 × 4\n      z1     z2    x[,1]  y[,1]\n   <dbl>  <dbl>    <dbl>  <dbl>\n1 -1.21  -1.21  -1.72    -2.45 \n2  0.277  0.301  0.502   -0.763\n3  1.08  -1.54  -0.00212 -0.306\n4 -2.35   0.635  2.92     3.99 \n5  0.429  0.703  0.121   -0.918\n6  0.506 -1.91  -1.73    -0.595\n\n\nEstimate with ivreg\n\nsummary(ivreg(y ~ x | z1 + z2, data=d))\n\n\nCall:\nivreg(formula = y ~ x | z1 + z2, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-4.77224 -0.95740 -0.02718  1.02122  4.52467 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) -0.008573   0.047024  -0.182    0.855\nx            0.223361   0.231282   0.966    0.334\n\nDiagnostic tests:\n                 df1 df2 statistic  p-value    \nWeak instruments   2 997    10.249 3.93e-05 ***\nWu-Hausman         1 997    10.298  0.00137 ** \nSargan             1  NA     0.114  0.73617    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.48 on 998 degrees of freedom\nMultiple R-Squared: 0.2276, Adjusted R-squared: 0.2268 \nWald test: 0.9327 on 1 and 998 DF,  p-value: 0.3344 \n\n\nManual estimation based on https://stats.stackexchange.com/questions/265780/calculation-of-iv-standard-errors-using-r\n\nPz <- z %*% solve(t(z) %*% z) %*% t(z)\nbhat <- solve(t(x) %*% Pz %*% x) %*% t(x) %*% Pz %*% y\nomega <- diag(n) * drop(var(y))\nv <- solve(t(x) %*% Pz %*% x) %*% t(x) %*% Pz %*% omega %*% Pz %*% x %*% solve(t(x) %*% Pz %*% x)\n\nResult\n\nbhat\n\n          [,1]\n[1,] 0.2237422\n\nsqrt(v)\n\n          [,1]\n[1,] 0.2631128\n\n\nbhat matches, standard error seems off\nAlternatively try a more standard approach - https://stats.stackexchange.com/questions/472144/how-to-manually-calculate-standard-errors-for-instrumental-variables\n\nxhat <- z %*% solve(t(z) %*% z) %*% t(z) %*% x\nbhat <- solve(t(xhat) %*% xhat) %*% t(xhat) %*% y\ne <- y - x %*% bhat\nC <- sum(e^2) / n * solve(t(xhat) %*% xhat)\n\nResult\n\nbhat\n\n          [,1]\n[1,] 0.2237422\n\nsqrt(C)\n\n          [,1]\n[1,] 0.2310808\n\n\nThis matches ivreg more closely\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dplyr_1.1.2 ivreg_0.6-2\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       cli_3.6.1         knitr_1.43        rlang_1.1.1      \n [5] xfun_0.39         Formula_1.2-5     car_3.1-2         generics_0.1.3   \n [9] jsonlite_1.8.5    glue_1.6.2        zoo_1.8-12        htmltools_0.5.5  \n[13] lmtest_0.9-40     fansi_1.0.4       rmarkdown_2.22    grid_4.3.0       \n[17] tibble_3.2.1      evaluate_0.21     carData_3.0-5     abind_1.4-5      \n[21] MASS_7.3-58.4     fastmap_1.1.1     lifecycle_1.0.3   yaml_2.3.7       \n[25] compiler_4.3.0    pkgconfig_2.0.3   htmlwidgets_1.6.2 rstudioapi_0.14  \n[29] lattice_0.21-8    digest_0.6.31     R6_2.5.1          tidyselect_1.2.0 \n[33] utf8_1.2.3        pillar_1.9.0      magrittr_2.0.3    tools_4.3.0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes",
    "section": "",
    "text": "GWAS catalog traits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic regression residuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary stat storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIV confounding with changing variances\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMR and imperfect clumping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeta regression using exposure effect size\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfounder instruments via networks\n\n\n\n\n\n\n\nconfounding\n\n\nmr\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard error in two-stage least squares\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractions and scale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferential susceptibility explained by interactors and mediators with different distributions\n\n\n\n\n\n\n\ninteractions\n\n\nmulti-ancestry\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCross group effect comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChecking PCA projection\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenetic confounding as a function of sample size\n\n\n\n\n\n\n\nMR\n\n\ngenetic confounding\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n  \n\n\n\n\nLDL and drug adjustment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGene-environment equivalence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoosting MZ vQTL power\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariance QTL using MZs\n\n\n\n\n\n\n\ninteractions\n\n\ngenetics\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpurious vQTL simulation\n\n\n\n\n\n\n\ngenetics\n\n\ninteractions\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverlapping genes\n\n\n\n\n\n\n\ngenomics\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSandwich variance estimators to control LD leakage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCell-specific effects for mQTLs from bulk tissue\n\n\n\n\n\n\n\nDNA methylation\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\nCan winner's curse generate a high conditional F statistic\n\n\n\nstatistics\nwinner's curse\nmultivariable mr\n\n\n\n\n\n\n\nJan 30, 2023\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetabolite power calculation\n\n\n\n\n\n\n\npower calculation\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nGibran Hemani and Nic Timpson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvert Bayes factors to beta and standard error\n\n\n\n\n\n\n\nstatistics\n\n\nfine mapping\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI instrument replication\n\n\n\n\n\n\n\nstatistics\n\n\ngenetics\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInflation of vQTLs\n\n\n\n\n\n\n\nstatistics\n\n\ngenetics\n\n\ninteractions\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSibling replication of Backman rare variants\n\n\n\n\n\n\n\nrare variants\n\n\ngenetics\n\n\nfamily studies\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability of a random variable being larger than all other random variables in a multivariate normal vector\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression with non i.i.d. samples\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelated SNPs\n\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower of GWAS in ascertained case control datasets\n\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPRS vs IVW\n\n\n\n\n\n\n\nstatistics\n\n\nMendelian randomization\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifference in difference models in observational data are still potentially confounded\n\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2022\n\n\nGibran Hemani\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I use this place to keep track of miscellaneous code and dynamic documents. It’s always rough work, unpolished, potentially full of errors."
  }
]